diff --git a/.gitmodules b/.gitmodules
new file mode 100644
index 0000000..1098231
--- /dev/null
+++ b/.gitmodules
@@ -0,0 +1,3 @@
+[submodule "riscv-clang"]
+	path = riscv-clang
+	url = https://github.com/riscv/riscv-clang.git
diff --git a/.travis.yml b/.travis.yml
new file mode 100644
index 0000000..e7e16b9
--- /dev/null
+++ b/.travis.yml
@@ -0,0 +1,46 @@
+language: cpp
+compiler:
+  - gcc
+sudo: false
+notifications:
+    email:
+        - colins@eecs.berkeley.edu
+cache: apt
+addons:
+  apt:
+    sources:
+      - ubuntu-toolchain-r-test
+      - kubuntu-backports
+    packages:
+    - gcc-4.9
+    - g++-4.9
+    - gperf
+    - autoconf
+    - automake
+    - autotools-dev
+    - libmpc-dev
+    - libmpfr-dev
+    - libgmp-dev
+    - gawk
+    - build-essential
+    - bison
+    - flex
+    - texinfo
+    - cmake
+env:
+  global:
+    - RISCV="/home/travis/riscv_install"
+  matrix:
+    - CONFIG="../configure --enable-optimized --enable-targets=riscv --prefix=${RISCV}" TEST="check LIT_ARGS=--filter=.*RISCV.*"
+    - CONFIG="cmake -DCMAKE_BUILD_TYPE=Release -DLLVM_TARGETS_TO_BUILD=RISCV -DCMAKE_INSTALL_PREFIX=${RISCV} ../" TEST="check-llvm-codegen-riscv check-llvm-mc-riscv"
+git:
+      submodules: true
+before_install:
+  - export CXX="g++-4.9" CC="gcc-4.9"
+  - $CXX --version
+script: 
+  - mkdir build
+  - cd build
+  - $CONFIG
+  - make -j4
+  - make $TEST
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 78fc78b..7688186 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -60,7 +60,7 @@ set(CMAKE_MODULE_PATH
 
 set(LLVM_VERSION_MAJOR 3)
 set(LLVM_VERSION_MINOR 7)
-set(LLVM_VERSION_PATCH 1)
+set(LLVM_VERSION_PATCH 0)
 set(LLVM_VERSION_SUFFIX "")
 
 if (NOT PACKAGE_VERSION)
@@ -185,6 +185,7 @@ set(LLVM_ALL_TARGETS
   MSP430
   NVPTX
   PowerPC
+  RISCV
   Sparc
   SystemZ
   X86
diff --git a/CREDITS.TXT b/CREDITS.TXT
index fd5119f..da1fb01 100644
--- a/CREDITS.TXT
+++ b/CREDITS.TXT
@@ -465,54 +465,3 @@ N: Bob Wilson
 E: bob.wilson@acm.org
 D: Advanced SIMD (NEON) support in the ARM backend.
 
-N: Alexey Bataev
-E: a.bataev@hotmail.com
-D: Clang OpenMP implementation
-
-N: Andrey Bokhanko
-E: andreybokhanko@gmail.com 
-D: Clang OpenMP implementation
-
-N: Carlo Bertolli
-E: cbertol@us.ibm.com 
-D: Clang OpenMP implementation
-
-N: Eric Stotzer
-E: estotzer@ti.com 
-D: Clang OpenMP implementation
-
-N: Kelvin Li
-E: kkwli0@gmail.com 
-D: Clang OpenMP implementation
-
-N: Samuel Antao
-E: sfantao@us.ibm.com 
-D: Clang OpenMP implementation
-
-N: Sergey Ostanevich
-E: sergos.gnu@gmail.com 
-D: Clang OpenMP implementation
-
-N: Alexandre Eichenberger
-E: alexe@us.ibm.com 
-D: Clang OpenMP implementation
-
-N: Guansong Zhang
-E: guansong.zhang@amd.com 
-D: Clang OpenMP implementation
-
-N: Sunita Chandrasekaran
-E: sunisg123@gmail.com  
-D: Clang OpenMP implementation
-
-N: Michael Wong
-E: fraggamuffin@gmail.com 
-D: Clang OpenMP implementation
-
-N: Alexander Mussman
-E: alexander.musman@intel.com 
-D: Clang OpenMP implementation
-
-N: Kevin O'Brien
-E: caomhin@us.ibm.com 
-D: Clang OpenMP implementation
\ No newline at end of file
diff --git a/README.md b/README.md
new file mode 100644
index 0000000..8fc4b3e
--- /dev/null
+++ b/README.md
@@ -0,0 +1,91 @@
+Low Level Virtual Machine (LLVM)
+======================================================
+
+This directory and its subdirectories contain source code for the Low Level
+Virtual Machine, a toolkit for the construction of highly optimized compilers,
+optimizers, and runtime environments.
+
+LLVM is open source software. You may freely distribute it under the terms of
+the license agreement found in `LICENSE.txt`.
+
+Please see the documentation provided in `docs/` for further
+assistance with LLVM, and in particular `docs/GettingStarted.rst` for getting
+started with LLVM and `docs/README.txt` for an overview of LLVM's
+documentation setup.
+
+If you're writing a package for LLVM, see `docs/Packaging.rst` for our
+suggestions.
+
+
+RISC-V LLVM Support [![Build Status](https://travis-ci.org/riscv/riscv-llvm.svg?branch=riscv-37)](https://travis-ci.org/riscv/riscv-llvm)
+--------------------------------------------------------
+
+Author  : Colin Schmidt (colins@eecs.berkeley.edu)  
+Date    : February 24, 2014  
+Version : (under version control)  
+
+
+This repository contains a new target for LLVM RISC-V. It supports the latest
+version of the ISA 2.0. This backend currently only supports assembly generation
+and riscv64-unknown-\*-gcc must be used to assemble and link the executable. The
+[RISCV](https://github.com/riscv/riscv-llvm/tree/RISCV) branch is based on LLVM 3.3 and, the 
+[riscv-trunk](https://github.com/riscv/riscv-llvm/tree/riscv-trunk) branch is following upstream LLVM master.
+
+The backend is structured similarly to most other LLVM backends and tries to use 
+the tablegen format as much as possible. The description of the instructions
+are found in `RISCVInstFormats.td`, and `RISCVInstrInfo*.td`. The registers are 
+described in `RISCVRegisterInfo.td` and the calling convention is described in
+`RISCVCallingConv.td`.
+
+The instructions are defined using the LLVM IR DAG format, and simple 
+instructions that use pre-existing LLVM IR operations should be very easy to
+add. The instructions are divided into separate files based on their extension,
+e.g. atomic operations are defined in `RISCVInstInfoA.td`. Instructions 
+implemented with these patterns are simply matched against the programs LLVM IR
+DAG for selection. More complicated instructions can use C++ to perform custom
+lowering of the LLVM IR in `RISCVISelLowering.cpp`. Combining of multiple LLVM IR
+nodes into single target instructions is also possible using C++ in
+the same file. In general `RISCVISelLowering.cpp` sets up the lowering based on
+the ISA and the specific subtargets features. 
+
+This backend does not include all features of a backend but is focused on 
+generating assembly in an extensible way such that adding new ISA extensions
+and using them should be relatively painless. As the RISC-V support develops
+the backend may provide more features.
+
+The compiler is fairly robust with similar performance to riscv64-unknown-\*-gcc, so it use
+in any and all projects is encouraged.
+
+Feedback and suggestions are welcome.
+
+Installation
+------------------------------------------------------------------
+
+The LLVM RISCV backend is built just as the normal LLVM system.
+
+	$ git clone -b RISCV https://github.com/riscv/riscv-llvm.git
+	$ git submodule update --init
+	$ mkdir build
+	$ cd build
+	$ ../configure --prefix=/opt/riscv
+	$ make
+	$ make install
+
+Now if `/opt/riscv` is on your path you should be able to use clang and LLVM with
+RISC-V support.
+
+Use
+--------------------------------------------------------------------
+
+Using the llvm-riscv is fairly simple to build a full executable however you
+need riscv-64-unknown-\*-gcc to do the assembling and linking. An example of compiling hello
+world:
+
+	$ cat hello.c
+	#include <stdio.h>
+	int main() {
+	    printf("Hello World!\n");
+	}
+	$ clang -target riscv -mriscv=RV64IAMFD -S hello.c -o hello.S
+	$ riscv-64-unknown-elf-gcc -o hello.riscv hello.S
+
diff --git a/autoconf/configure.ac b/autoconf/configure.ac
index af57712..74ebea2 100644
--- a/autoconf/configure.ac
+++ b/autoconf/configure.ac
@@ -32,11 +32,11 @@ dnl===-----------------------------------------------------------------------===
 dnl Initialize autoconf and define the package name, version number and
 dnl address for reporting bugs.
 
-AC_INIT([LLVM],[3.7.1],[http://llvm.org/bugs/])
+AC_INIT([LLVM],[3.7.0],[http://llvm.org/bugs/])
 
 LLVM_VERSION_MAJOR=3
 LLVM_VERSION_MINOR=7
-LLVM_VERSION_PATCH=1
+LLVM_VERSION_PATCH=0
 LLVM_VERSION_SUFFIX=
 
 AC_DEFINE_UNQUOTED([LLVM_VERSION_MAJOR], $LLVM_VERSION_MAJOR, [Major version of the LLVM API])
diff --git a/bindings/go/llvm/ir.go b/bindings/go/llvm/ir.go
index 76f5f06..80f7798 100644
--- a/bindings/go/llvm/ir.go
+++ b/bindings/go/llvm/ir.go
@@ -1728,7 +1728,7 @@ func (b Builder) CreatePtrDiff(lhs, rhs Value, name string) (v Value) {
 func (b Builder) CreateLandingPad(t Type, personality Value, nclauses int, name string) (l Value) {
 	cname := C.CString(name)
 	defer C.free(unsafe.Pointer(cname))
-	l.C = C.LLVMBuildLandingPad(b.C, t.C, nil, C.unsigned(nclauses), cname)
+	l.C = C.LLVMBuildLandingPad(b.C, t.C, C.unsigned(nclauses), cname)
 	return l
 }
 
diff --git a/bindings/ocaml/llvm/llvm_ocaml.c b/bindings/ocaml/llvm/llvm_ocaml.c
index 3889f92..26835d0 100644
--- a/bindings/ocaml/llvm/llvm_ocaml.c
+++ b/bindings/ocaml/llvm/llvm_ocaml.c
@@ -1745,7 +1745,7 @@ CAMLprim LLVMValueRef llvm_build_invoke_bc(value Args[], int NumArgs) {
 CAMLprim LLVMValueRef llvm_build_landingpad(LLVMTypeRef Ty, LLVMValueRef PersFn,
                                             value NumClauses,  value Name,
                                             value B) {
-    return LLVMBuildLandingPad(Builder_val(B), Ty, PersFn, Int_val(NumClauses),
+    return LLVMBuildLandingPad(Builder_val(B), Ty, Int_val(NumClauses),
                                String_val(Name));
 }
 
diff --git a/configure b/configure
index c192415..3594e3d 100755
--- a/configure
+++ b/configure
@@ -1,6 +1,6 @@
 #! /bin/sh
 # Guess values for system-dependent variables and create Makefiles.
-# Generated by GNU Autoconf 2.60 for LLVM 3.7.1.
+# Generated by GNU Autoconf 2.60 for LLVM 3.7.0.
 #
 # Report bugs to <http://llvm.org/bugs/>.
 #
@@ -561,8 +561,8 @@ SHELL=${CONFIG_SHELL-/bin/sh}
 # Identity of this package.
 PACKAGE_NAME='LLVM'
 PACKAGE_TARNAME='llvm'
-PACKAGE_VERSION='3.7.1'
-PACKAGE_STRING='LLVM 3.7.1'
+PACKAGE_VERSION='3.7.0'
+PACKAGE_STRING='LLVM 3.7.0'
 PACKAGE_BUGREPORT='http://llvm.org/bugs/'
 
 ac_unique_file="lib/IR/Module.cpp"
@@ -1333,7 +1333,7 @@ if test "$ac_init_help" = "long"; then
   # Omit some internal or obsolete options to make the list less imposing.
   # This message is too long to be a string in the A/UX 3.1 sh.
   cat <<_ACEOF
-\`configure' configures LLVM 3.7.1 to adapt to many kinds of systems.
+\`configure' configures LLVM 3.7.0 to adapt to many kinds of systems.
 
 Usage: $0 [OPTION]... [VAR=VALUE]...
 
@@ -1399,7 +1399,7 @@ fi
 
 if test -n "$ac_init_help"; then
   case $ac_init_help in
-     short | recursive ) echo "Configuration of LLVM 3.7.1:";;
+     short | recursive ) echo "Configuration of LLVM 3.7.0:";;
    esac
   cat <<\_ACEOF
 
@@ -1463,7 +1463,7 @@ Optional Features:
                           target1,target2,... Valid targets are: host, x86,
                           x86_64, sparc, powerpc, arm64, arm, aarch64, mips,
                           hexagon, xcore, msp430, nvptx, systemz, r600, bpf,
-                          wasm, and cpp (default=all)
+                          wasm, riscv, and cpp (default=all)
   --enable-experimental-targets
                           Build experimental host targets: disable or
                           target1,target2,... (default=disable)
@@ -1583,7 +1583,7 @@ fi
 test -n "$ac_init_help" && exit $ac_status
 if $ac_init_version; then
   cat <<\_ACEOF
-LLVM configure 3.7.1
+LLVM configure 3.7.0
 generated by GNU Autoconf 2.60
 
 Copyright (C) 1992, 1993, 1994, 1995, 1996, 1998, 1999, 2000, 2001,
@@ -1599,7 +1599,7 @@ cat >config.log <<_ACEOF
 This file contains any messages produced by compilers while
 running configure, to aid debugging if configure makes a mistake.
 
-It was created by LLVM $as_me 3.7.1, which was
+It was created by LLVM $as_me 3.7.0, which was
 generated by GNU Autoconf 2.60.  Invocation command line was
 
   $ $0 $@
@@ -1955,7 +1955,7 @@ ac_compiler_gnu=$ac_cv_c_compiler_gnu
 
 LLVM_VERSION_MAJOR=3
 LLVM_VERSION_MINOR=7
-LLVM_VERSION_PATCH=1
+LLVM_VERSION_PATCH=0
 LLVM_VERSION_SUFFIX=
 
 
@@ -4193,6 +4193,7 @@ else
   nvptx-*)                llvm_cv_target_arch="NVPTX" ;;
   s390x-*)                llvm_cv_target_arch="SystemZ" ;;
   wasm*-*)                llvm_cv_target_arch="WebAssembly" ;;
+  riscv-*)                llvm_cv_target_arch="RISCV" ;;
   *)                      llvm_cv_target_arch="Unknown" ;;
 esac
 fi
@@ -4229,6 +4230,7 @@ case $host in
   msp430-*)               host_arch="MSP430" ;;
   hexagon-*)              host_arch="Hexagon" ;;
   s390x-*)                host_arch="SystemZ" ;;
+  riscv-*)                host_arch="RISCV" ;;
   wasm*-*)                host_arch="WebAssembly" ;;
   *)                      host_arch="Unknown" ;;
 esac
@@ -5657,6 +5659,7 @@ case "$enableval" in
         amdgpu)   TARGETS_TO_BUILD="AMDGPU $TARGETS_TO_BUILD" ;;
         r600)     TARGETS_TO_BUILD="AMDGPU $TARGETS_TO_BUILD" ;;
         wasm)     TARGETS_TO_BUILD="WebAssembly $TARGETS_TO_BUILD" ;;
+        riscv)    TARGETS_TO_BUILD="RISCV $TARGETS_TO_BUILD" ;;
         host) case "$llvm_cv_target_arch" in
             x86)         TARGETS_TO_BUILD="X86 $TARGETS_TO_BUILD" ;;
             x86_64)      TARGETS_TO_BUILD="X86 $TARGETS_TO_BUILD" ;;
@@ -5671,6 +5674,7 @@ case "$enableval" in
             NVPTX)       TARGETS_TO_BUILD="NVPTX $TARGETS_TO_BUILD" ;;
             SystemZ)     TARGETS_TO_BUILD="SystemZ $TARGETS_TO_BUILD" ;;
             WebAssembly) TARGETS_TO_BUILD="WebAssembly $TARGETS_TO_BUILD" ;;
+            RISCV)       TARGETS_TO_BUILD="RISCV $TARGETS_TO_BUILD" ;;
             *)       { { echo "$as_me:$LINENO: error: Can not set target to build" >&5
 echo "$as_me: error: Can not set target to build" >&2;}
    { (exit 1); exit 1; }; } ;;
@@ -8643,6 +8647,87 @@ fi
 
 if test "$llvm_cv_os_type" = "MingW" ; then
 
+{ echo "$as_me:$LINENO: checking for main in -limagehlp" >&5
+echo $ECHO_N "checking for main in -limagehlp... $ECHO_C" >&6; }
+if test "${ac_cv_lib_imagehlp_main+set}" = set; then
+  echo $ECHO_N "(cached) $ECHO_C" >&6
+else
+  ac_check_lib_save_LIBS=$LIBS
+LIBS="-limagehlp  $LIBS"
+cat >conftest.$ac_ext <<_ACEOF
+/* confdefs.h.  */
+_ACEOF
+cat confdefs.h >>conftest.$ac_ext
+cat >>conftest.$ac_ext <<_ACEOF
+/* end confdefs.h.  */
+
+
+int
+main ()
+{
+return main ();
+  ;
+  return 0;
+}
+_ACEOF
+rm -f conftest.$ac_objext conftest$ac_exeext
+if { (ac_try="$ac_link"
+case "(($ac_try" in
+  *\"* | *\`* | *\\*) ac_try_echo=\$ac_try;;
+  *) ac_try_echo=$ac_try;;
+esac
+eval "echo \"\$as_me:$LINENO: $ac_try_echo\"") >&5
+  (eval "$ac_link") 2>conftest.er1
+  ac_status=$?
+  grep -v '^ *+' conftest.er1 >conftest.err
+  rm -f conftest.er1
+  cat conftest.err >&5
+  echo "$as_me:$LINENO: \$? = $ac_status" >&5
+  (exit $ac_status); } &&
+	 { ac_try='test -z "$ac_c_werror_flag" || test ! -s conftest.err'
+  { (case "(($ac_try" in
+  *\"* | *\`* | *\\*) ac_try_echo=\$ac_try;;
+  *) ac_try_echo=$ac_try;;
+esac
+eval "echo \"\$as_me:$LINENO: $ac_try_echo\"") >&5
+  (eval "$ac_try") 2>&5
+  ac_status=$?
+  echo "$as_me:$LINENO: \$? = $ac_status" >&5
+  (exit $ac_status); }; } &&
+	 { ac_try='test -s conftest$ac_exeext'
+  { (case "(($ac_try" in
+  *\"* | *\`* | *\\*) ac_try_echo=\$ac_try;;
+  *) ac_try_echo=$ac_try;;
+esac
+eval "echo \"\$as_me:$LINENO: $ac_try_echo\"") >&5
+  (eval "$ac_try") 2>&5
+  ac_status=$?
+  echo "$as_me:$LINENO: \$? = $ac_status" >&5
+  (exit $ac_status); }; }; then
+  ac_cv_lib_imagehlp_main=yes
+else
+  echo "$as_me: failed program was:" >&5
+sed 's/^/| /' conftest.$ac_ext >&5
+
+	ac_cv_lib_imagehlp_main=no
+fi
+
+rm -f core conftest.err conftest.$ac_objext \
+      conftest$ac_exeext conftest.$ac_ext
+LIBS=$ac_check_lib_save_LIBS
+fi
+{ echo "$as_me:$LINENO: result: $ac_cv_lib_imagehlp_main" >&5
+echo "${ECHO_T}$ac_cv_lib_imagehlp_main" >&6; }
+if test $ac_cv_lib_imagehlp_main = yes; then
+  cat >>confdefs.h <<_ACEOF
+#define HAVE_LIBIMAGEHLP 1
+_ACEOF
+
+  LIBS="-limagehlp $LIBS"
+
+fi
+
+
 { echo "$as_me:$LINENO: checking for main in -lole32" >&5
 echo $ECHO_N "checking for main in -lole32... $ECHO_C" >&6; }
 if test "${ac_cv_lib_ole32_main+set}" = set; then
@@ -18529,7 +18614,7 @@ exec 6>&1
 # report actual input values of CONFIG_FILES etc. instead of their
 # values after options handling.
 ac_log="
-This file was extended by LLVM $as_me 3.7.1, which was
+This file was extended by LLVM $as_me 3.7.0, which was
 generated by GNU Autoconf 2.60.  Invocation command line was
 
   CONFIG_FILES    = $CONFIG_FILES
@@ -18582,7 +18667,7 @@ Report bugs to <bug-autoconf@gnu.org>."
 _ACEOF
 cat >>$CONFIG_STATUS <<_ACEOF
 ac_cs_version="\\
-LLVM config.status 3.7.1
+LLVM config.status 3.7.0
 configured by $0, generated by GNU Autoconf 2.60,
   with options \\"`echo "$ac_configure_args" | sed 's/^ //; s/[\\""\`\$]/\\\\&/g'`\\"
 
diff --git a/docs/CMake.rst b/docs/CMake.rst
index 909fc04..b6dd838 100644
--- a/docs/CMake.rst
+++ b/docs/CMake.rst
@@ -387,10 +387,6 @@ LLVM-specific variables
   ``-DLLVM_ENABLE_DOXYGEN_QT_HELP=ON``; otherwise this has no
   effect.
 
-**LLVM_DOXYGEN_SVG**:BOOL
-  Uses .svg files instead of .png files for graphs in the Doxygen output.
-  Defaults to OFF.
-
 **LLVM_ENABLE_SPHINX**:BOOL
   If enabled CMake will search for the ``sphinx-build`` executable and will make
   the ``SPHINX_OUTPUT_HTML`` and ``SPHINX_OUTPUT_MAN`` CMake options available.
diff --git a/docs/CMakeLists.txt b/docs/CMakeLists.txt
index 2388a92..da27627 100644
--- a/docs/CMakeLists.txt
+++ b/docs/CMakeLists.txt
@@ -56,14 +56,6 @@ if (LLVM_ENABLE_DOXYGEN)
     set(llvm_doxygen_qhp_cust_filter_attrs "")
   endif()
   
-  option(LLVM_DOXYGEN_SVG
-    "Use svg instead of png files for doxygen graphs." OFF)
-  if (LLVM_DOXYGEN_SVG)
-    set(DOT_IMAGE_FORMAT "svg")
-  else()
-    set(DOT_IMAGE_FORMAT "png")
-  endif()
-
   configure_file(${CMAKE_CURRENT_SOURCE_DIR}/doxygen.cfg.in
     ${CMAKE_CURRENT_BINARY_DIR}/doxygen.cfg @ONLY)
 
@@ -81,7 +73,6 @@ if (LLVM_ENABLE_DOXYGEN)
   set(llvm_doxygen_qhelpgenerator_path)
   set(llvm_doxygen_qhp_cust_filter_name)
   set(llvm_doxygen_qhp_cust_filter_attrs)
-  set(DOT_IMAGE_FORMAT)
 
   add_custom_target(doxygen-llvm
     COMMAND ${DOXYGEN_EXECUTABLE} ${CMAKE_CURRENT_BINARY_DIR}/doxygen.cfg
diff --git a/docs/Makefile b/docs/Makefile
index da649bc..c9d2477 100644
--- a/docs/Makefile
+++ b/docs/Makefile
@@ -31,7 +31,6 @@ $(PROJ_OBJ_DIR)/doxygen.cfg: doxygen.cfg.in
 	  -e 's/@llvm_doxygen_qhp_cust_filter_name@//g' \
 	  -e 's/@llvm_doxygen_qhp_namespace@//g' \
 	  -e 's/@searchengine_url@//g' \
-	  -e 's/@DOT_IMAGE_FORMAT@/png/g' \
 	  > $@
 endif
 
diff --git a/docs/ReleaseNotes.rst b/docs/ReleaseNotes.rst
index b68f5ec..7c02902 100644
--- a/docs/ReleaseNotes.rst
+++ b/docs/ReleaseNotes.rst
@@ -5,6 +5,12 @@ LLVM 3.7 Release Notes
 .. contents::
     :local:
 
+.. warning::
+   These are in-progress notes for the upcoming LLVM 3.7 release.  You may
+   prefer the `LLVM 3.6 Release Notes <http://llvm.org/releases/3.6.0/docs
+   /ReleaseNotes.html>`_.
+
+
 Introduction
 ============
 
@@ -25,35 +31,7 @@ LLVM web page, this document applies to the *next* release, not the current
 one.  To see the release notes for a specific release, please see the `releases
 page <http://llvm.org/releases/>`_.
 
-Major changes in 3.7.1
-======================
-
-* 3.7.0 was released with an inadvertent change to the signature of the C
-  API function: LLVMBuildLandingPad, which made the C API incompatible with
-  prior releases.  This has been corrected in LLVM 3.7.1.
-
-  As a result of this change, 3.7.0 is not ABI compatible with 3.7.1.
-
-  +----------------------------------------------------------------------------+
-  | History of the LLVMBuildLandingPad() function                              |
-  +===========================+================================================+
-  | 3.6.2 and prior releases  | LLVMBuildLandingPad(LLVMBuilderRef,            |
-  |                           |                     LLVMTypeRef,               |
-  |                           |                     LLVMValueRef,              |
-  |                           |                     unsigned, const char*)     |
-  +---------------------------+------------------------------------------------+
-  | 3.7.0                     | LLVMBuildLandingPad(LLVMBuilderRef,            |
-  |                           |                     LLVMTypeRef,               |
-  |                           |                     unsigned, const char*)     |
-  +---------------------------+------------------------------------------------+
-  | 3.7.1 and future releases | LLVMBuildLandingPad(LLVMBuilderRef,            |
-  |                           |                     LLVMTypeRef,               |
-  |                           |                     LLVMValueRef,              |
-  |                           |                     unsigned, const char*)     |
-  +---------------------------+------------------------------------------------+
-
-
-Non-comprehensive list of changes in 3.7.0
+Non-comprehensive list of changes in this release
 =================================================
 
 .. NOTE
@@ -70,20 +48,12 @@ Non-comprehensive list of changes in 3.7.0
   collection of tips for frontend authors on how to generate IR which LLVM is
   able to effectively optimize.
 
-* The ``DataLayout`` is no longer optional. All the IR level optimizations expects
+* The DataLayout is no longer optional. All the IR level optimizations expects
   it to be present and the API has been changed to use a reference instead of
   a pointer to make it explicit. The Module owns the datalayout and it has to
   match the one attached to the TargetMachine for generating code.
 
-  In 3.6, a pass was inserted in the pipeline to make the ``DataLayout`` accessible:
-    ``MyPassManager->add(new DataLayoutPass(MyTargetMachine->getDataLayout()));``
-  In 3.7, you don't need a pass, you set the ``DataLayout`` on the ``Module``:
-    ``MyModule->setDataLayout(MyTargetMachine->createDataLayout());``
-
-  The LLVM C API ``LLVMGetTargetMachineData`` is deprecated to reflect the fact
-  that it won't be available anymore from ``TargetMachine`` in 3.8.
-
-* Comdats are now orthogonal to the linkage. LLVM will not create
+* Comdats are now ortogonal to the linkage. LLVM will not create
   comdats for weak linkage globals and the frontends are responsible
   for explicitly adding them.
 
@@ -100,46 +70,28 @@ Non-comprehensive list of changes in 3.7.0
   instruction set that can be dynamically loaded into the Linux kernel via the
   `bpf(2) <http://man7.org/linux/man-pages/man2/bpf.2.html>`_ syscall.
 
-  Support for BPF has been present in the kernel for some time, but starting
-  from 3.18 has been extended with such features as: 64-bit registers, 8
-  additional registers registers, conditional backwards jumps, call
-  instruction, shift instructions, map (hash table, array, etc.), 1-8 byte
-  load/store from stack, and more.
-
-  Up until now, users of BPF had to write bytecode by hand, or use
-  custom generators. This release adds a proper LLVM backend target for the BPF
-  bytecode architecture.
-
-  The BPF target is now available by default, and options exist in both Clang
-  (-target bpf) or llc (-march=bpf) to pick eBPF as a backend.
-
 * Switch-case lowering was rewritten to avoid generating unbalanced search trees
   (`PR22262 <http://llvm.org/pr22262>`_) and to exploit profile information
   when available. Some lowering strategies are now disabled when optimizations
   are turned off, to save compile time.
 
-* The debug info IR class hierarchy now inherits from ``Metadata`` and has its
-  own bitcode records and assembly syntax
-  (`documented in LangRef <LangRef.html#specialized-metadata-nodes>`_).  The debug
-  info verifier has been merged with the main verifier.
-
-* LLVM IR and APIs are in a period of transition to aid in the removal of
-  pointer types (the end goal being that pointers are typeless/opaque - void*,
-  if you will). Some APIs and IR constructs have been modified to take
-  explicit types that are currently checked to match the target type of their
-  pre-existing pointer type operands. Further changes are still needed, but the
-  more you can avoid using ``PointerType::getPointeeType``, the easier the
-  migration will be.
-
-* Argument-less ``TargetMachine::getSubtarget`` and
-  ``TargetMachine::getSubtargetImpl`` have been removed from the tree. Updating
-  out of tree ports is as simple as implementing a non-virtual version in the
-  target, but implementing full ``Function`` based ``TargetSubtargetInfo``
-  support is recommended.
-
-* This is expected to be the last major release of LLVM that supports being
-  run on Windows XP and Windows Vista.  For the next major release the minimum
-  Windows version requirement will be Windows 7.
+* ... next change ...
+
+.. NOTE
+   If you would like to document a larger change, then you can add a
+   subsection about it right here. You can copy the following boilerplate
+   and un-indent it (the indentation causes it to be inside this comment).
+
+   Special New Feature
+   -------------------
+
+   Makes programs 10x faster by doing Special New Thing.
+
+Changes to the ARM Backend
+--------------------------
+
+ During this release ...
+
 
 Changes to the MIPS Target
 --------------------------
@@ -148,8 +100,8 @@ During this release the MIPS target has:
 
 * Added support for MIPS32R3, MIPS32R5, MIPS32R3, MIPS32R5, and microMIPS32.
 
-* Added support for dynamic stack realignment. This is of particular importance
-  to MSA on 32-bit subtargets since vectors always exceed the stack alignment on
+* Added support for dynamic stack realignment. This of particular importance to
+  MSA on 32-bit subtargets since vectors always exceed the stack alignment on
   the O32 ABI.
 
 * Added support for compiler-rt including:
@@ -260,8 +212,8 @@ There are numerous improvements to the PowerPC target in this release:
 
 * Many bugs have been identified and fixed.
 
-Changes to the SystemZ Target
------------------------------
+Changes to the System/Z Target
+------------------------------
 
 * LLVM no longer attempts to automatically detect the current host CPU when
   invoked natively.
@@ -278,138 +230,11 @@ Changes to the SystemZ Target
 * Support for the z13 processor and its vector facility.
 
 
-Changes to the JIT APIs
------------------------
-
-* Added a new C++ JIT API called On Request Compilation, or ORC.
-
-  ORC is a new JIT API inspired by MCJIT but designed to be more testable, and
-  easier to extend with new features. A key new feature already in tree is lazy,
-  function-at-a-time compilation for X86. Also included is a reimplementation of
-  MCJIT's API and behavior (OrcMCJITReplacement). MCJIT itself remains in tree,
-  and continues to be the default JIT ExecutionEngine, though new users are
-  encouraged to try ORC out for their projects. (A good place to start is the
-  new ORC tutorials under llvm/examples/kaleidoscope/orc).
-
-Sub-project Status Update
-=========================
-
-In addition to the core LLVM 3.7 distribution of production-quality compiler
-infrastructure, the LLVM project includes sub-projects that use the LLVM core
-and share the same distribution license. This section provides updates on these
-sub-projects.
-
-Polly - The Polyhedral Loop Optimizer in LLVM
----------------------------------------------
-
-`Polly <http://polly.llvm.org>`_ is a polyhedral loop optimization
-infrastructure that provides data-locality optimizations to LLVM-based
-compilers. When compiled as part of clang or loaded as a module into clang,
-it can perform loop optimizations such as tiling, loop fusion or outer-loop
-vectorization. As a generic loop optimization infrastructure it allows
-developers to get a per-loop-iteration model of a loop nest on which detailed
-analysis and transformations can be performed.
-
-Changes since the last release:
-
-* isl imported into Polly distribution
-
-  `isl <http://repo.or.cz/w/isl.git>`_, the math library Polly uses, has been
-  imported into the source code repository of Polly and is now distributed as part
-  of Polly. As this was the last external library dependency of Polly, Polly can
-  now be compiled right after checking out the Polly source code without the need
-  for any additional libraries to be pre-installed.
-
-* Small integer optimization of isl
-
-  The MIT licensed imath backend using in `isl <http://repo.or.cz/w/isl.git>`_ for
-  arbitrary width integer computations has been optimized to use native integer
-  operations for the common case where the operands of a computation fit into 32
-  bit and to only fall back to large arbitrary precision integers for the
-  remaining cases. This optimization has greatly improved the compile-time
-  performance of Polly, both due to faster native operations also due to a
-  reduction in malloc traffic and pointer indirections. As a result, computations
-  that use arbitrary precision integers heavily have been speed up by almost 6x.
-  As a result, the compile-time of Polly on the Polybench test kernels in the LNT
-  suite has been reduced by 20% on average with compile time reductions between
-  9-43%.
-
-* Schedule Trees
-
-  Polly now uses internally so-called > Schedule Trees < to model the loop
-  structure it optimizes. Schedule trees are an easy to understand tree structure
-  that describes a loop nest using integer constraint sets to keep track of
-  execution constraints. It allows the developer to use per-tree-node operations
-  to modify the loop tree. Programatic analysis that work on the schedule tree
-  (e.g., as dependence analysis) also show a visible speedup as they can exploit
-  the tree structure of the schedule and need to fall back to ILP based
-  optimization problems less often. Section 6 of `Polyhedral AST generation is
-  more than scanning polyhedra
-  <http://www.grosser.es/#pub-polyhedral-AST-generation>`_ gives a detailed
-  explanation of this schedule trees.
-
-* Scalar and PHI node modeling - Polly as an analysis
-
-  Polly now requires almost no preprocessing to analyse LLVM-IR, which makes it
-  easier to use Polly as a pure analysis pass e.g. to provide more precise
-  dependence information to non-polyhedral transformation passes. Originally,
-  Polly required the input LLVM-IR to be preprocessed such that all scalar and
-  PHI-node dependences are translated to in-memory operations. Since this release,
-  Polly has full support for scalar and PHI node dependences and requires no
-  scalar-to-memory translation for such kind of dependences.
-
-* Modeling of modulo and non-affine conditions
-
-  Polly can now supports modulo operations such as A[t%2][i][j] as they appear
-  often in stencil computations and also allows data-dependent conditional
-  branches as they result e.g. from ternary conditions ala A[i] > 255 ? 255 :
-  A[i].
-
-* Delinearization
-
-  Polly now support the analysis of manually linearized multi-dimensional arrays
-  as they result form macros such as
-  "#define 2DARRAY(A,i,j) (A.data[(i) * A.size + (j)]". Similar constructs appear
-  in old C code written before C99, C++ code such as boost::ublas, LLVM exported
-  from Julia, Matlab generated code and many others. Our work titled
-  `Optimistic Delinearization of Parametrically Sized Arrays
-  <http://www.grosser.es/#pub-optimistic-delinerization>`_ gives details.
-
-* Compile time improvements
-
-  Pratik Bahtu worked on compile-time performance tuning of Polly. His work
-  together with the support for schedule trees and the small integer optimization
-  in isl notably reduced the compile time.
-
-* Increased compute timeouts
-
-  As Polly's compile time has been notabily improved, we were able to increase
-  the compile time saveguards in Polly. As a result, the default configuration
-  of Polly can now analyze larger loop nests without running into compile time
-  restrictions.
-
-* Export Debug Locations via JSCoP file
-
-  Polly's JSCoP import/export format gained support for debug locations that show
-  to the user the source code location of detected scops.
-
-* Improved windows support
-
-  The compilation of Polly on windows using cmake has been improved and several
-  visual studio build issues have been addressed.
+Changes to the OCaml bindings
+-----------------------------
 
-* Many bug fixes
-
-libunwind
----------
-
-The unwind implementation which use to reside in `libc++abi` has been moved into
-a separate repository.  This implementation can still be used for `libc++abi` by
-specifying `-DLIBCXXABI_USE_LLVM_UNWINDER=YES` and
-`-DLIBCXXABI_LIBUNWIND_PATH=<path to libunwind source>` when configuring
-`libc++abi`, which defaults to `true` when building on ARM.
+ During this release ...
 
-The new repository can also be built standalone if just `libunwind` is desired.
 
 External Open Source Projects Using LLVM 3.7
 ============================================
@@ -470,24 +295,6 @@ BPF Compiler Collection (BCC)
 networking that is using Clang rewriter + 2nd pass of Clang + BPF backend to
 generate eBPF and push it into the kernel.
 
-LLVMSharp & ClangSharp
-----------------------
-
-`LLVMSharp <http://www.llvmsharp.org>`_ and
-`ClangSharp <http://www.clangsharp.org>`_ are type-safe C# bindings for
-Microsoft.NET and Mono that Platform Invoke into the native libraries.
-ClangSharp is self-hosted and is used to generated LLVMSharp using the
-LLVM-C API.
-
-`LLVMSharp Kaleidoscope Tutorials <http://www.llvmsharp.org/Kaleidoscope/>`_
-are instructive examples of writing a compiler in C#, with certain improvements
-like using the visitor pattern to generate LLVM IR.
-
-`ClangSharp PInvoke Generator <http://www.clangsharp.org/PInvoke/>`_ is the
-self-hosting mechanism for LLVM/ClangSharp and is demonstrative of using
-LibClang to generate Platform Invoke (PInvoke) signatures for C APIs.
-
-
 Additional Information
 ======================
 
@@ -500,3 +307,4 @@ going into the ``llvm/docs/`` directory in the LLVM tree.
 
 If you have any questions or comments about LLVM, please feel free to contact
 us via the `mailing lists <http://llvm.org/docs/#maillist>`_.
+
diff --git a/docs/doxygen.cfg.in b/docs/doxygen.cfg.in
index 5c70db0..d8c4051 100644
--- a/docs/doxygen.cfg.in
+++ b/docs/doxygen.cfg.in
@@ -2205,7 +2205,7 @@ DIRECTORY_GRAPH        = YES
 # The default value is: png.
 # This tag requires that the tag HAVE_DOT is set to YES.
 
-DOT_IMAGE_FORMAT       = @DOT_IMAGE_FORMAT@
+DOT_IMAGE_FORMAT       = png
 
 # If DOT_IMAGE_FORMAT is set to svg, then this option can be set to YES to
 # enable generation of interactive SVG images that allow zooming and panning.
diff --git a/include/llvm-c/Core.h b/include/llvm-c/Core.h
index 9dbcbfe..1529007 100644
--- a/include/llvm-c/Core.h
+++ b/include/llvm-c/Core.h
@@ -2675,8 +2675,7 @@ LLVMValueRef LLVMBuildInvoke(LLVMBuilderRef, LLVMValueRef Fn,
                              LLVMBasicBlockRef Then, LLVMBasicBlockRef Catch,
                              const char *Name);
 LLVMValueRef LLVMBuildLandingPad(LLVMBuilderRef B, LLVMTypeRef Ty,
-                                 LLVMValueRef PersFn, unsigned NumClauses,
-                                 const char *Name);
+                                 unsigned NumClauses, const char *Name);
 LLVMValueRef LLVMBuildResume(LLVMBuilderRef B, LLVMValueRef Exn);
 LLVMValueRef LLVMBuildUnreachable(LLVMBuilderRef);
 
diff --git a/include/llvm-c/TargetMachine.h b/include/llvm-c/TargetMachine.h
index 8cf1f43..d4993e7 100644
--- a/include/llvm-c/TargetMachine.h
+++ b/include/llvm-c/TargetMachine.h
@@ -115,7 +115,7 @@ char *LLVMGetTargetMachineCPU(LLVMTargetMachineRef T);
   LLVMDisposeMessage. */
 char *LLVMGetTargetMachineFeatureString(LLVMTargetMachineRef T);
 
-/** Deprecated: use LLVMGetDataLayout(LLVMModuleRef M) instead. */
+/** Returns the llvm::DataLayout used for this llvm:TargetMachine. */
 LLVMTargetDataRef LLVMGetTargetMachineData(LLVMTargetMachineRef T);
 
 /** Set the target machine's ASM verbosity. */
diff --git a/include/llvm/ADT/Triple.h b/include/llvm/ADT/Triple.h
index 947812d..91b0407 100644
--- a/include/llvm/ADT/Triple.h
+++ b/include/llvm/ADT/Triple.h
@@ -63,6 +63,8 @@ public:
     ppc64le,    // PPC64LE: powerpc64le
     r600,       // R600: AMD GPUs HD2XXX - HD6XXX
     amdgcn,     // AMDGCN: AMD GCN GPUs
+    riscv,      // RISCV: riscv 32-bit
+    riscv64,    // RISCV: riscv 64-bit
     sparc,      // Sparc: sparc
     sparcv9,    // Sparcv9: Sparcv9
     sparcel,    // Sparc: (endianness = little). NB: 'Sparcle' is a CPU variant
diff --git a/include/llvm/CodeGen/CommandFlags.h b/include/llvm/CodeGen/CommandFlags.h
index bedb7d5..4b2e0b0 100644
--- a/include/llvm/CodeGen/CommandFlags.h
+++ b/include/llvm/CodeGen/CommandFlags.h
@@ -21,7 +21,7 @@
 #include "llvm/IR/Intrinsics.h"
 #include "llvm/IR/Module.h"
 #include "llvm/MC/MCTargetOptionsCommandFlags.h"
-#include "llvm/MC/SubtargetFeature.h"
+#include "llvm//MC/SubtargetFeature.h"
 #include "llvm/Support/CodeGen.h"
 #include "llvm/Support/CommandLine.h"
 #include "llvm/Support/Host.h"
diff --git a/include/llvm/Support/ELF.h b/include/llvm/Support/ELF.h
index 94a4bfb..2b4bfbc 100644
--- a/include/llvm/Support/ELF.h
+++ b/include/llvm/Support/ELF.h
@@ -309,7 +309,8 @@ enum {
   EM_COOL          = 217, // iCelero CoolEngine
   EM_NORC          = 218, // Nanoradio Optimized RISC
   EM_CSR_KALIMBA   = 219, // CSR Kalimba architecture family
-  EM_AMDGPU        = 224  // AMD GPU architecture
+  EM_AMDGPU        = 224, // AMD GPU architecture
+  EM_RISCV         = 243  // RISC-V
 };
 
 // Object file classes.
@@ -560,6 +561,11 @@ enum {
 #include "ELFRelocs/SystemZ.def"
 };
 
+// ELF Relocation types for RISCV
+enum {
+#include "ELFRelocs/RISCV.def"
+};
+
 // ELF Relocation type for Sparc.
 enum {
 #include "ELFRelocs/Sparc.def"
diff --git a/include/llvm/Support/ELFRelocs/RISCV.def b/include/llvm/Support/ELFRelocs/RISCV.def
new file mode 100644
index 0000000..a61172d
--- /dev/null
+++ b/include/llvm/Support/ELFRelocs/RISCV.def
@@ -0,0 +1,47 @@
+
+#ifndef ELF_RELOC
+#error "ELF_RELOC must be defined"
+#endif
+
+ELF_RELOC (R_RISCV_NONE,          0) /* No relocation. */
+ELF_RELOC (R_RISCV_32,            1)
+ELF_RELOC (R_RISCV_64,            2)
+ELF_RELOC (R_RISCV_RELATIVE,      3)
+ELF_RELOC (R_RISCV_COPY,          4)
+ELF_RELOC (R_RISCV_JUMP_SLOT,     5)
+ELF_RELOC (R_RISCV_TLS_DTPMOD32,  6)
+ELF_RELOC (R_RISCV_TLS_DTPMOD64,  7)
+ELF_RELOC (R_RISCV_TLS_DTPREL32,  8)
+ELF_RELOC (R_RISCV_TLS_DTPREL64,  9)
+ELF_RELOC (R_RISCV_TLS_TPREL32,   10)
+ELF_RELOC (R_RISCV_TLS_TPREL64,   11)
+ELF_RELOC (R_RISCV_BRANCH,        16)
+ELF_RELOC (R_RISCV_JAL,           17)
+ELF_RELOC (R_RISCV_CALL,          18)
+ELF_RELOC (R_RISCV_CALL_PLT,      19)
+ELF_RELOC (R_RISCV_GOT_HI20,      20)
+ELF_RELOC (R_RISCV_TLS_GOT_HI20,  21)
+ELF_RELOC (R_RISCV_TLS_GD_HI20,   22)
+ELF_RELOC (R_RISCV_PCREL_HI20,    23)
+ELF_RELOC (R_RISCV_PCREL_LO12_I,  24)
+ELF_RELOC (R_RISCV_PCREL_LO12_S,  25)
+ELF_RELOC (R_RISCV_HI20,          26)
+ELF_RELOC (R_RISCV_LO12_I,        27)
+ELF_RELOC (R_RISCV_LO12_S,        28)
+ELF_RELOC (R_RISCV_TPREL_HI20,    29)
+ELF_RELOC (R_RISCV_TPREL_LO12_I,  30)
+ELF_RELOC (R_RISCV_TPREL_LO12_S,  31)
+ELF_RELOC (R_RISCV_TPREL_ADD,     32)
+ELF_RELOC (R_RISCV_ADD8,          33)
+ELF_RELOC (R_RISCV_ADD16,         34)
+ELF_RELOC (R_RISCV_ADD32,         35)
+ELF_RELOC (R_RISCV_ADD64,         36)
+ELF_RELOC (R_RISCV_SUB8,          37)
+ELF_RELOC (R_RISCV_SUB16,         38)
+ELF_RELOC (R_RISCV_SUB32,         39)
+ELF_RELOC (R_RISCV_SUB64,         40)
+ELF_RELOC (R_RISCV_GNU_VTINHERIT, 41)
+ELF_RELOC (R_RISCV_GNU_VTENTRY,   42)
+ELF_RELOC (R_RISCV_ALIGN,         43)
+ELF_RELOC (R_RISCV_RVC_BRANCH,    44)
+ELF_RELOC (R_RISCV_RVC_JUMP,      45)
diff --git a/include/llvm/Target/TargetMachine.h b/include/llvm/Target/TargetMachine.h
index f1e9d17..06a2b13 100644
--- a/include/llvm/Target/TargetMachine.h
+++ b/include/llvm/Target/TargetMachine.h
@@ -125,15 +125,10 @@ public:
     return *static_cast<const STC*>(getSubtargetImpl(F));
   }
 
-  /// Deprecated in 3.7, will be removed in 3.8. Use createDataLayout() instead.
-  ///
   /// This method returns a pointer to the DataLayout for the target. It should
   /// be unchanging for every subtarget.
   const DataLayout *getDataLayout() const { return &DL; }
 
-  /// Create a DataLayout.
-  const DataLayout createDataLayout() const { return DL; }
-
   /// \brief Reset the target options based on the function's attributes.
   // FIXME: Remove TargetOptions that affect per-function code generation
   // from TargetMachine.
diff --git a/lib/Analysis/BasicAliasAnalysis.cpp b/lib/Analysis/BasicAliasAnalysis.cpp
index 3586354..68f766e 100644
--- a/lib/Analysis/BasicAliasAnalysis.cpp
+++ b/lib/Analysis/BasicAliasAnalysis.cpp
@@ -206,6 +206,14 @@ static Value *GetLinearExpression(Value *V, APInt &Scale, APInt &Offset,
     return V;
   }
 
+  if (ConstantInt *Const = dyn_cast<ConstantInt>(V)) {
+    // if it's a constant, just convert it to an offset
+    // and remove the variable.
+    Offset += Const->getValue();
+    assert(Scale == 0 && "Constant values don't have a scale");
+    return V;
+  }
+
   if (BinaryOperator *BOp = dyn_cast<BinaryOperator>(V)) {
     if (ConstantInt *RHSC = dyn_cast<ConstantInt>(BOp->getOperand(1))) {
       switch (BOp->getOpcode()) {
@@ -253,7 +261,10 @@ static Value *GetLinearExpression(Value *V, APInt &Scale, APInt &Offset,
     Value *Result = GetLinearExpression(CastOp, Scale, Offset, Extension, DL,
                                         Depth + 1, AC, DT);
     Scale = Scale.zext(OldWidth);
-    Offset = Offset.zext(OldWidth);
+
+    // We have to sign-extend even if Extension == EK_ZeroExt as we can't
+    // decompose a sign extension (i.e. zext(x - 1) != zext(x) - zext(-1)).
+    Offset = Offset.sext(OldWidth);
 
     return Result;
   }
@@ -1124,12 +1135,43 @@ AliasResult BasicAliasAnalysis::aliasGEP(
     }
   }
 
-  // Try to distinguish something like &A[i][1] against &A[42][0].
-  // Grab the least significant bit set in any of the scales.
   if (!GEP1VariableIndices.empty()) {
     uint64_t Modulo = 0;
-    for (unsigned i = 0, e = GEP1VariableIndices.size(); i != e; ++i)
+    bool AllPositive = true;
+    for (unsigned i = 0, e = GEP1VariableIndices.size(); i != e; ++i) {
+
+      // Try to distinguish something like &A[i][1] against &A[42][0].
+      // Grab the least significant bit set in any of the scales. We
+      // don't need std::abs here (even if the scale's negative) as we'll
+      // be ^'ing Modulo with itself later.
       Modulo |= (uint64_t) GEP1VariableIndices[i].Scale;
+
+      if (AllPositive) {
+        // If the Value could change between cycles, then any reasoning about
+        // the Value this cycle may not hold in the next cycle. We'll just
+        // give up if we can't determine conditions that hold for every cycle:
+        const Value *V = GEP1VariableIndices[i].V;
+
+        bool SignKnownZero, SignKnownOne;
+        ComputeSignBit(const_cast<Value *>(V), SignKnownZero, SignKnownOne, *DL,
+                       0, AC1, nullptr, DT);
+
+        // Zero-extension widens the variable, and so forces the sign
+        // bit to zero.
+        bool IsZExt = GEP1VariableIndices[i].Extension == EK_ZeroExt;
+        SignKnownZero |= IsZExt;
+        SignKnownOne &= !IsZExt;
+
+        // If the variable begins with a zero then we know it's
+        // positive, regardless of whether the value is signed or
+        // unsigned.
+        int64_t Scale = GEP1VariableIndices[i].Scale;
+        AllPositive =
+          (SignKnownZero && Scale >= 0) ||
+          (SignKnownOne && Scale < 0);
+      }
+    }
+
     Modulo = Modulo ^ (Modulo & (Modulo - 1));
 
     // We can compute the difference between the two addresses
@@ -1140,6 +1182,12 @@ AliasResult BasicAliasAnalysis::aliasGEP(
         V2Size != MemoryLocation::UnknownSize && ModOffset >= V2Size &&
         V1Size <= Modulo - ModOffset)
       return NoAlias;
+
+    // If we know all the variables are positive, then GEP1 >= GEP1BasePtr.
+    // If GEP1BasePtr > V2 (GEP1BaseOffset > 0) then we know the pointers
+    // don't alias if V2Size can fit in the gap between V2 and GEP1BasePtr.
+    if (AllPositive && GEP1BaseOffset > 0 && V2Size <= (uint64_t) GEP1BaseOffset)
+      return NoAlias;
   }
 
   // Statically, we can see that the base objects are the same, but the
diff --git a/lib/Analysis/InstructionSimplify.cpp b/lib/Analysis/InstructionSimplify.cpp
index a7f8f5c..fa42b48 100644
--- a/lib/Analysis/InstructionSimplify.cpp
+++ b/lib/Analysis/InstructionSimplify.cpp
@@ -3574,9 +3574,18 @@ static Value *SimplifyExtractElementInst(Value *Vec, Value *Idx, const Query &,
 
   // If extracting a specified index from the vector, see if we can recursively
   // find a previously computed scalar that was inserted into the vector.
-  if (auto *IdxC = dyn_cast<ConstantInt>(Idx))
-    if (Value *Elt = findScalarElement(Vec, IdxC->getZExtValue()))
+  if (auto *IdxC = dyn_cast<ConstantInt>(Idx)) {
+    unsigned IndexVal = IdxC->getZExtValue();
+    unsigned VectorWidth = Vec->getType()->getVectorNumElements();
+
+    // If this is extracting an invalid index, turn this into undef, to avoid
+    // crashing the code below.
+    if (IndexVal >= VectorWidth)
+      return UndefValue::get(Vec->getType()->getVectorElementType());
+
+    if (Value *Elt = findScalarElement(Vec, IndexVal))
       return Elt;
+  }
 
   return nullptr;
 }
diff --git a/lib/Analysis/VectorUtils.cpp b/lib/Analysis/VectorUtils.cpp
index 8c671ef..67f68dc 100644
--- a/lib/Analysis/VectorUtils.cpp
+++ b/lib/Analysis/VectorUtils.cpp
@@ -402,9 +402,8 @@ llvm::Value *llvm::findScalarElement(llvm::Value *V, unsigned EltNo) {
   if (match(V,
             llvm::PatternMatch::m_Add(llvm::PatternMatch::m_Value(Val),
                                       llvm::PatternMatch::m_Constant(Con)))) {
-    if (Constant *Elt = Con->getAggregateElement(EltNo))
-      if (Elt->isNullValue())
-        return findScalarElement(Val, EltNo);
+    if (Con->getAggregateElement(EltNo)->isNullValue())
+      return findScalarElement(Val, EltNo);
   }
 
   // Otherwise, we don't know.
diff --git a/lib/CodeGen/AsmPrinter/WinException.cpp b/lib/CodeGen/AsmPrinter/WinException.cpp
index a2b9316..71c7781 100644
--- a/lib/CodeGen/AsmPrinter/WinException.cpp
+++ b/lib/CodeGen/AsmPrinter/WinException.cpp
@@ -169,7 +169,7 @@ void WinException::endFunction(const MachineFunction *MF) {
     Asm->OutStreamer->PopSection();
   }
 
-  if (shouldEmitMoves || shouldEmitPersonality)
+  if (shouldEmitMoves)
     Asm->OutStreamer->EmitWinCFIEndProc();
 }
 
diff --git a/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp b/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp
index fbc8f1e..21ab072 100644
--- a/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp
+++ b/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp
@@ -439,7 +439,7 @@ ExpandUnalignedLoad(LoadSDNode *LD, SelectionDAG &DAG,
                              ISD::ANY_EXTEND, dl, VT, Result);
 
       ValResult = Result;
-      ChainResult = newLoad.getValue(1);
+      ChainResult = Chain;
       return;
     }
 
diff --git a/lib/CodeGen/SelectionDAG/LegalizeTypes.cpp b/lib/CodeGen/SelectionDAG/LegalizeTypes.cpp
index 54cfaf5..a7392fa 100644
--- a/lib/CodeGen/SelectionDAG/LegalizeTypes.cpp
+++ b/lib/CodeGen/SelectionDAG/LegalizeTypes.cpp
@@ -1010,8 +1010,6 @@ SDValue DAGTypeLegalizer::GetVectorElementPointer(SDValue VecPtr, EVT EltVT,
 
   // Calculate the element offset and add it to the pointer.
   unsigned EltSize = EltVT.getSizeInBits() / 8; // FIXME: should be ABI size.
-  assert(EltSize * 8 == EltVT.getSizeInBits() &&
-         "Converting bits to bytes lost precision");
 
   Index = DAG.getNode(ISD::MUL, dl, Index.getValueType(), Index,
                       DAG.getConstant(EltSize, dl, Index.getValueType()));
diff --git a/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp b/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp
index 51cd661..4348ab7 100644
--- a/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp
+++ b/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp
@@ -1528,25 +1528,9 @@ SDValue DAGTypeLegalizer::SplitVecOp_EXTRACT_VECTOR_ELT(SDNode *N) {
   if (CustomLowerNode(N, N->getValueType(0), true))
     return SDValue();
 
-  // Make the vector elements byte-addressable if they aren't already.
-  SDLoc dl(N);
-  EVT EltVT = VecVT.getVectorElementType();
-  if (EltVT.getSizeInBits() < 8) {
-    SmallVector<SDValue, 4> ElementOps;
-    for (unsigned i = 0; i < VecVT.getVectorNumElements(); ++i) {
-      ElementOps.push_back(DAG.getAnyExtOrTrunc(
-          DAG.getNode(ISD::EXTRACT_VECTOR_ELT, dl, EltVT, Vec,
-                      DAG.getConstant(i, dl, MVT::i8)),
-          dl, MVT::i8));
-    }
-
-    EltVT = MVT::i8;
-    VecVT = EVT::getVectorVT(*DAG.getContext(), EltVT,
-                             VecVT.getVectorNumElements());
-    Vec = DAG.getNode(ISD::BUILD_VECTOR, dl, VecVT, ElementOps);
-  }
-
   // Store the vector to the stack.
+  EVT EltVT = VecVT.getVectorElementType();
+  SDLoc dl(N);
   SDValue StackPtr = DAG.CreateStackTemporary(VecVT);
   SDValue Store = DAG.getStore(DAG.getEntryNode(), dl, Vec, StackPtr,
                                MachinePointerInfo(), false, false, 0);
diff --git a/lib/IR/AsmWriter.cpp b/lib/IR/AsmWriter.cpp
index b553f11..adc620d 100644
--- a/lib/IR/AsmWriter.cpp
+++ b/lib/IR/AsmWriter.cpp
@@ -794,10 +794,6 @@ void SlotTracker::processFunction() {
   ST_DEBUG("begin processFunction!\n");
   fNext = 0;
 
-  // Process function metadata if it wasn't hit at the module-level.
-  if (!ShouldInitializeAllMetadata)
-    processFunctionMetadata(*TheFunction);
-
   // Add all the function arguments with no names.
   for(Function::const_arg_iterator AI = TheFunction->arg_begin(),
       AE = TheFunction->arg_end(); AI != AE; ++AI)
@@ -811,6 +807,8 @@ void SlotTracker::processFunction() {
     if (!BB.hasName())
       CreateFunctionSlot(&BB);
 
+    processFunctionMetadata(*TheFunction);
+
     for (auto &I : BB) {
       if (!I.getType()->isVoidTy() && !I.hasName())
         CreateFunctionSlot(&I);
@@ -838,11 +836,11 @@ void SlotTracker::processFunction() {
 
 void SlotTracker::processFunctionMetadata(const Function &F) {
   SmallVector<std::pair<unsigned, MDNode *>, 4> MDs;
-  F.getAllMetadata(MDs);
-  for (auto &MD : MDs)
-    CreateMetadataSlot(MD.second);
-
   for (auto &BB : F) {
+    F.getAllMetadata(MDs);
+    for (auto &MD : MDs)
+      CreateMetadataSlot(MD.second);
+
     for (auto &I : BB)
       processInstructionMetadata(I);
   }
diff --git a/lib/IR/Core.cpp b/lib/IR/Core.cpp
index 0eb88a9..e0e729d 100644
--- a/lib/IR/Core.cpp
+++ b/lib/IR/Core.cpp
@@ -2257,14 +2257,7 @@ LLVMValueRef LLVMBuildInvoke(LLVMBuilderRef B, LLVMValueRef Fn,
 }
 
 LLVMValueRef LLVMBuildLandingPad(LLVMBuilderRef B, LLVMTypeRef Ty,
-                                 LLVMValueRef PersFn, unsigned NumClauses,
-                                 const char *Name) {
-  // The personality used to live on the landingpad instruction, but now it
-  // lives on the parent function. For compatibility, take the provided
-  // personality and put it on the parent function.
-  if (PersFn)
-    unwrap(B)->GetInsertBlock()->getParent()->setPersonalityFn(
-        cast<Function>(unwrap(PersFn)));
+                                 unsigned NumClauses, const char *Name) {
   return wrap(unwrap(B)->CreateLandingPad(unwrap(Ty), NumClauses, Name));
 }
 
diff --git a/lib/IR/Type.cpp b/lib/IR/Type.cpp
index a9ca800..b5c4e5d 100644
--- a/lib/IR/Type.cpp
+++ b/lib/IR/Type.cpp
@@ -613,9 +613,6 @@ bool StructType::isLayoutIdentical(StructType *Other) const {
   if (isPacked() != Other->isPacked() ||
       getNumElements() != Other->getNumElements())
     return false;
-
-  if (!getNumElements())
-    return true;
   
   return std::equal(element_begin(), element_end(), Other->element_begin());
 }
diff --git a/lib/LTO/LTOCodeGenerator.cpp b/lib/LTO/LTOCodeGenerator.cpp
index 25ae4ac..149ec6a 100644
--- a/lib/LTO/LTOCodeGenerator.cpp
+++ b/lib/LTO/LTOCodeGenerator.cpp
@@ -63,21 +63,14 @@ const char* LTOCodeGenerator::getVersionString() {
 #endif
 }
 
-static void handleLTODiagnostic(const DiagnosticInfo &DI) {
-  DiagnosticPrinterRawOStream DP(errs());
-  DI.print(DP);
-  errs() << "\n";
-}
-
 LTOCodeGenerator::LTOCodeGenerator()
-    : Context(getGlobalContext()), IRLinker(new Module("ld-temp.o", Context),
-                                            handleLTODiagnostic) {
+    : Context(getGlobalContext()), IRLinker(new Module("ld-temp.o", Context)) {
   initializeLTOPasses();
 }
 
 LTOCodeGenerator::LTOCodeGenerator(std::unique_ptr<LLVMContext> Context)
     : OwnedContext(std::move(Context)), Context(*OwnedContext),
-      IRLinker(new Module("ld-temp.o", *OwnedContext), handleLTODiagnostic) {
+      IRLinker(new Module("ld-temp.o", *OwnedContext)) {
   initializeLTOPasses();
 }
 
diff --git a/lib/MC/MCContext.cpp b/lib/MC/MCContext.cpp
index a85796c..c601c56 100644
--- a/lib/MC/MCContext.cpp
+++ b/lib/MC/MCContext.cpp
@@ -82,7 +82,6 @@ void MCContext::reset() {
 
   UsedNames.clear();
   Symbols.clear();
-  SectionSymbols.clear();
   Allocator.Reset();
   Instances.clear();
   CompilationDir.clear();
diff --git a/lib/Support/Triple.cpp b/lib/Support/Triple.cpp
index c6646fb..5a3df95 100644
--- a/lib/Support/Triple.cpp
+++ b/lib/Support/Triple.cpp
@@ -41,6 +41,8 @@ const char *Triple::getArchTypeName(ArchType Kind) {
   case sparc:       return "sparc";
   case sparcv9:     return "sparcv9";
   case sparcel:     return "sparcel";
+  case riscv:       return "riscv";
+  case riscv64:     return "riscv64";
   case systemz:     return "s390x";
   case tce:         return "tce";
   case thumb:       return "thumb";
@@ -101,6 +103,10 @@ const char *Triple::getArchTypePrefix(ArchType Kind) {
   case sparcel:
   case sparc:       return "sparc";
 
+  case riscv:
+  case riscv64:     return "riscv";
+
+
   case systemz:     return "s390";
 
   case x86:
@@ -240,6 +246,8 @@ Triple::ArchType Triple::getArchTypeForLLVMName(StringRef Name) {
     .Case("sparc", sparc)
     .Case("sparcel", sparcel)
     .Case("sparcv9", sparcv9)
+    .Case("riscv", riscv)
+    .Case("riscv64", riscv64)
     .Case("systemz", systemz)
     .Case("tce", tce)
     .Case("thumb", thumb)
@@ -352,6 +360,8 @@ static Triple::ArchType parseArch(StringRef ArchName) {
     .Case("sparc", Triple::sparc)
     .Case("sparcel", Triple::sparcel)
     .Cases("sparcv9", "sparc64", Triple::sparcv9)
+    .Case("riscv", Triple::riscv)
+    .Case("riscv64", Triple::riscv64)
     .Case("tce", Triple::tce)
     .Case("xcore", Triple::xcore)
     .Case("nvptx", Triple::nvptx)
@@ -525,6 +535,8 @@ static Triple::ObjectFormatType getDefaultFormat(const Triple &T) {
   case Triple::amdgcn:
   case Triple::sparc:
   case Triple::sparcv9:
+  case Triple::riscv:
+  case Triple::riscv64:
   case Triple::systemz:
   case Triple::xcore:
   case Triple::ppc64le:
@@ -1007,6 +1019,7 @@ static unsigned getArchPointerBitWidth(llvm::Triple::ArchType Arch) {
   case llvm::Triple::r600:
   case llvm::Triple::sparc:
   case llvm::Triple::sparcel:
+  case llvm::Triple::riscv:
   case llvm::Triple::tce:
   case llvm::Triple::thumb:
   case llvm::Triple::thumbeb:
@@ -1032,6 +1045,7 @@ static unsigned getArchPointerBitWidth(llvm::Triple::ArchType Arch) {
   case llvm::Triple::ppc64:
   case llvm::Triple::ppc64le:
   case llvm::Triple::sparcv9:
+  case llvm::Triple::riscv64:
   case llvm::Triple::systemz:
   case llvm::Triple::x86_64:
   case llvm::Triple::amdil64:
@@ -1083,6 +1097,7 @@ Triple Triple::get32BitArchVariant() const {
   case Triple::nvptx:
   case Triple::ppc:
   case Triple::r600:
+  case Triple::riscv:
   case Triple::sparc:
   case Triple::sparcel:
   case Triple::tce:
@@ -1101,6 +1116,7 @@ Triple Triple::get32BitArchVariant() const {
   case Triple::nvptx64:   T.setArch(Triple::nvptx);   break;
   case Triple::ppc64:     T.setArch(Triple::ppc);     break;
   case Triple::sparcv9:   T.setArch(Triple::sparc);   break;
+  case Triple::riscv64:   T.setArch(Triple::riscv);   break;
   case Triple::x86_64:    T.setArch(Triple::x86);     break;
   case Triple::amdil64:   T.setArch(Triple::amdil);   break;
   case Triple::hsail64:   T.setArch(Triple::hsail);   break;
@@ -1146,6 +1162,7 @@ Triple Triple::get64BitArchVariant() const {
   case Triple::sparcv9:
   case Triple::systemz:
   case Triple::x86_64:
+  case Triple::riscv64:
   case Triple::wasm64:
     // Already 64-bit.
     break;
@@ -1160,6 +1177,7 @@ Triple Triple::get64BitArchVariant() const {
   case Triple::amdil:   T.setArch(Triple::amdil64);   break;
   case Triple::hsail:   T.setArch(Triple::hsail64);   break;
   case Triple::spir:    T.setArch(Triple::spir64);    break;
+  case Triple::riscv:   T.setArch(Triple::riscv64);   break;
   case Triple::wasm32:  T.setArch(Triple::wasm64);    break;
   }
   return T;
@@ -1182,6 +1200,8 @@ Triple Triple::getBigEndianArchVariant() const {
   case Triple::nvptx64:
   case Triple::nvptx:
   case Triple::r600:
+  case Triple::riscv64:
+  case Triple::riscv:
   case Triple::shave:
   case Triple::spir64:
   case Triple::spir:
@@ -1258,6 +1278,8 @@ Triple Triple::getLittleEndianArchVariant() const {
   case Triple::nvptx:
   case Triple::ppc64le:
   case Triple::r600:
+  case Triple::riscv64:
+  case Triple::riscv:
   case Triple::shave:
   case Triple::sparcel:
   case Triple::spir64:
diff --git a/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp b/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
index 0a5309b..709d753 100644
--- a/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
+++ b/lib/Target/AMDGPU/AMDGPUAsmPrinter.cpp
@@ -264,12 +264,6 @@ void AMDGPUAsmPrinter::getSIProgramInfo(SIProgramInfo &ProgInfo,
   for (const MachineBasicBlock &MBB : MF) {
     for (const MachineInstr &MI : MBB) {
       // TODO: CodeSize should account for multiple functions.
-
-      // TODO: Should we count size of debug info?
-      if (MI.isDebugValue())
-        continue;
-
-      // FIXME: This is reporting 0 for many instructions.
       CodeSize += MI.getDesc().Size;
 
       unsigned numOperands = MI.getNumOperands();
diff --git a/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp b/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
index 57b7a73..4a65bfc 100644
--- a/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
+++ b/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp
@@ -134,17 +134,13 @@ static Value* GEPToVectorIndex(GetElementPtrInst *GEP) {
 //
 // TODO: Check isTriviallyVectorizable for calls and handle other
 // instructions.
-static bool canVectorizeInst(Instruction *Inst, User *User) {
+static bool canVectorizeInst(Instruction *Inst) {
   switch (Inst->getOpcode()) {
   case Instruction::Load:
+  case Instruction::Store:
   case Instruction::BitCast:
   case Instruction::AddrSpaceCast:
     return true;
-  case Instruction::Store: {
-    // Must be the stored pointer operand, not a stored value.
-    StoreInst *SI = cast<StoreInst>(Inst);
-    return SI->getPointerOperand() == User;
-  }
   default:
     return false;
   }
@@ -170,7 +166,7 @@ static bool tryPromoteAllocaToVector(AllocaInst *Alloca) {
   for (User *AllocaUser : Alloca->users()) {
     GetElementPtrInst *GEP = dyn_cast<GetElementPtrInst>(AllocaUser);
     if (!GEP) {
-      if (!canVectorizeInst(cast<Instruction>(AllocaUser), Alloca))
+      if (!canVectorizeInst(cast<Instruction>(AllocaUser)))
         return false;
 
       WorkList.push_back(AllocaUser);
@@ -188,7 +184,7 @@ static bool tryPromoteAllocaToVector(AllocaInst *Alloca) {
 
     GEPVectorIdx[GEP] = Index;
     for (User *GEPUser : AllocaUser->users()) {
-      if (!canVectorizeInst(cast<Instruction>(GEPUser), AllocaUser))
+      if (!canVectorizeInst(cast<Instruction>(GEPUser)))
         return false;
 
       WorkList.push_back(GEPUser);
@@ -244,12 +240,7 @@ static bool collectUsesWithPtrTypes(Value *Val, std::vector<Value*> &WorkList) {
   for (User *User : Val->users()) {
     if(std::find(WorkList.begin(), WorkList.end(), User) != WorkList.end())
       continue;
-    if (CallInst *CI = dyn_cast<CallInst>(User)) {
-      // TODO: We might be able to handle some cases where the callee is a
-      // constantexpr bitcast of a function.
-      if (!CI->getCalledFunction())
-        return false;
-
+    if (isa<CallInst>(User)) {
       WorkList.push_back(User);
       continue;
     }
@@ -259,12 +250,6 @@ static bool collectUsesWithPtrTypes(Value *Val, std::vector<Value*> &WorkList) {
     if (UseInst && UseInst->getOpcode() == Instruction::PtrToInt)
       return false;
 
-    if (StoreInst *SI = dyn_cast_or_null<StoreInst>(UseInst)) {
-      // Reject if the stored value is not the pointer operand.
-      if (SI->getPointerOperand() != Val)
-        return false;
-    }
-
     if (!User->getType()->isPointerTy())
       continue;
 
diff --git a/lib/Target/AMDGPU/AMDGPURegisterInfo.td b/lib/Target/AMDGPU/AMDGPURegisterInfo.td
index ba0490a..835a146 100644
--- a/lib/Target/AMDGPU/AMDGPURegisterInfo.td
+++ b/lib/Target/AMDGPU/AMDGPURegisterInfo.td
@@ -14,7 +14,8 @@
 let Namespace = "AMDGPU" in {
 
 foreach Index = 0-15 in {
-  def sub#Index : SubRegIndex<32, !shl(Index, 5)>;
+  // Indices are used in a variety of ways here, so don't set a size/offset.
+  def sub#Index : SubRegIndex<-1, -1>;
 }
 
 def INDIRECT_BASE_ADDR : Register <"INDIRECT_BASE_ADDR">;
diff --git a/lib/Target/AMDGPU/MCTargetDesc/AMDGPUAsmBackend.cpp b/lib/Target/AMDGPU/MCTargetDesc/AMDGPUAsmBackend.cpp
index 4434d9b..468563c 100644
--- a/lib/Target/AMDGPU/MCTargetDesc/AMDGPUAsmBackend.cpp
+++ b/lib/Target/AMDGPU/MCTargetDesc/AMDGPUAsmBackend.cpp
@@ -71,26 +71,12 @@ void AMDGPUMCObjectWriter::writeObject(MCAssembler &Asm,
   }
 }
 
-static unsigned getFixupKindNumBytes(unsigned Kind) {
-  switch (Kind) {
-  case FK_Data_1:
-    return 1;
-  case FK_Data_2:
-    return 2;
-  case FK_Data_4:
-    return 4;
-  case FK_Data_8:
-    return 8;
-  default:
-    llvm_unreachable("Unknown fixup kind!");
-  }
-}
-
 void AMDGPUAsmBackend::applyFixup(const MCFixup &Fixup, char *Data,
                                   unsigned DataSize, uint64_t Value,
                                   bool IsPCRel) const {
 
   switch ((unsigned)Fixup.getKind()) {
+    default: llvm_unreachable("Unknown fixup kind");
     case AMDGPU::fixup_si_sopp_br: {
       uint16_t *Dst = (uint16_t*)(Data + Fixup.getOffset());
       *Dst = (Value - 4) / 4;
@@ -110,24 +96,6 @@ void AMDGPUAsmBackend::applyFixup(const MCFixup &Fixup, char *Data,
       *Dst = Value + 4;
       break;
     }
-    default: {
-      // FIXME: Copied from AArch64
-      unsigned NumBytes = getFixupKindNumBytes(Fixup.getKind());
-      if (!Value)
-        return; // Doesn't change encoding.
-      MCFixupKindInfo Info = getFixupKindInfo(Fixup.getKind());
-
-      // Shift the value into position.
-      Value <<= Info.TargetOffset;
-
-      unsigned Offset = Fixup.getOffset();
-      assert(Offset + NumBytes <= DataSize && "Invalid fixup offset!");
-
-      // For each byte of the fragment that the fixup touches, mask in the
-      // bits from the fixup value.
-      for (unsigned i = 0; i != NumBytes; ++i)
-        Data[Offset + i] |= uint8_t((Value >> (i * 8)) & 0xff);
-    }
   }
 }
 
diff --git a/lib/Target/AMDGPU/SIISelLowering.cpp b/lib/Target/AMDGPU/SIISelLowering.cpp
index c2db9ff..099b0b1 100644
--- a/lib/Target/AMDGPU/SIISelLowering.cpp
+++ b/lib/Target/AMDGPU/SIISelLowering.cpp
@@ -157,7 +157,6 @@ SITargetLowering::SITargetLowering(TargetMachine &TM,
 
   setTruncStoreAction(MVT::i64, MVT::i32, Expand);
   setTruncStoreAction(MVT::v8i32, MVT::v8i16, Expand);
-  setTruncStoreAction(MVT::v16i32, MVT::v16i8, Expand);
   setTruncStoreAction(MVT::v16i32, MVT::v16i16, Expand);
 
   setOperationAction(ISD::LOAD, MVT::i1, Custom);
@@ -2253,8 +2252,10 @@ MachineSDNode *SITargetLowering::buildScratchRSRC(SelectionDAG &DAG,
                                                   SDValue Ptr) const {
   const SIInstrInfo *TII =
       static_cast<const SIInstrInfo *>(Subtarget->getInstrInfo());
+  uint64_t Rsrc = TII->getDefaultRsrcDataFormat() | AMDGPU::RSRC_TID_ENABLE |
+                  0xffffffff; // Size
 
-  return buildRSRC(DAG, DL, Ptr, 0, TII->getScratchRsrcWords23());
+  return buildRSRC(DAG, DL, Ptr, 0, Rsrc);
 }
 
 SDValue SITargetLowering::CreateLiveInRegister(SelectionDAG &DAG,
diff --git a/lib/Target/AMDGPU/SIInstrInfo.cpp b/lib/Target/AMDGPU/SIInstrInfo.cpp
index cfd2c42..1891061 100644
--- a/lib/Target/AMDGPU/SIInstrInfo.cpp
+++ b/lib/Target/AMDGPU/SIInstrInfo.cpp
@@ -2778,16 +2778,3 @@ uint64_t SIInstrInfo::getDefaultRsrcDataFormat() const {
 
   return RsrcDataFormat;
 }
-
-uint64_t SIInstrInfo::getScratchRsrcWords23() const {
-  uint64_t Rsrc23 = getDefaultRsrcDataFormat() |
-                    AMDGPU::RSRC_TID_ENABLE |
-                    0xffffffff; // Size;
-
-  // If TID_ENABLE is set, DATA_FORMAT specifies stride bits [14:17].
-  // Clear them unless we want a huge stride.
-  if (ST.getGeneration() >= AMDGPUSubtarget::VOLCANIC_ISLANDS)
-    Rsrc23 &= ~AMDGPU::RSRC_DATA_FORMAT;
-
-  return Rsrc23;
-}
diff --git a/lib/Target/AMDGPU/SIInstrInfo.h b/lib/Target/AMDGPU/SIInstrInfo.h
index 5053786..015ea12 100644
--- a/lib/Target/AMDGPU/SIInstrInfo.h
+++ b/lib/Target/AMDGPU/SIInstrInfo.h
@@ -353,7 +353,7 @@ public:
   }
 
   uint64_t getDefaultRsrcDataFormat() const;
-  uint64_t getScratchRsrcWords23() const;
+
 };
 
 namespace AMDGPU {
diff --git a/lib/Target/AMDGPU/SIInstructions.td b/lib/Target/AMDGPU/SIInstructions.td
index e0eeea9..f78ffd7 100644
--- a/lib/Target/AMDGPU/SIInstructions.td
+++ b/lib/Target/AMDGPU/SIInstructions.td
@@ -1548,12 +1548,6 @@ defm V_WRITELANE_B32 : VOP2SI_3VI_m <
 // These instructions only exist on SI and CI
 let SubtargetPredicate = isSICI in {
 
-let isCommutable = 1 in {
-defm V_MAC_LEGACY_F32 : VOP2InstSI <vop2<0x6>, "v_mac_legacy_f32",
-  VOP_F32_F32_F32
->;
-} // End isCommutable = 1
-
 defm V_MIN_LEGACY_F32 : VOP2InstSI <vop2<0xd>, "v_min_legacy_f32",
   VOP_F32_F32_F32, AMDGPUfmin_legacy
 >;
@@ -1568,6 +1562,12 @@ defm V_LSHL_B32 : VOP2InstSI <vop2<0x19>, "v_lshl_b32", VOP_I32_I32_I32>;
 } // End isCommutable = 1
 } // End let SubtargetPredicate = SICI
 
+let isCommutable = 1 in {
+defm V_MAC_LEGACY_F32 : VOP2_VI3_Inst <vop23<0x6, 0x28e>, "v_mac_legacy_f32",
+  VOP_F32_F32_F32
+>;
+} // End isCommutable = 1
+
 defm V_BFM_B32 : VOP2_VI3_Inst <vop23<0x1e, 0x293>, "v_bfm_b32",
   VOP_I32_I32_I32
 >;
diff --git a/lib/Target/AMDGPU/SIPrepareScratchRegs.cpp b/lib/Target/AMDGPU/SIPrepareScratchRegs.cpp
index 2cd600d..0a7f684 100644
--- a/lib/Target/AMDGPU/SIPrepareScratchRegs.cpp
+++ b/lib/Target/AMDGPU/SIPrepareScratchRegs.cpp
@@ -135,7 +135,8 @@ bool SIPrepareScratchRegs::runOnMachineFunction(MachineFunction &MF) {
       unsigned ScratchRsrcReg =
           RS.scavengeRegister(&AMDGPU::SReg_128RegClass, 0);
 
-      uint64_t Rsrc23 = TII->getScratchRsrcWords23();
+      uint64_t Rsrc = AMDGPU::RSRC_DATA_FORMAT | AMDGPU::RSRC_TID_ENABLE |
+                      0xffffffff; // Size
 
       unsigned Rsrc0 = TRI->getSubReg(ScratchRsrcReg, AMDGPU::sub0);
       unsigned Rsrc1 = TRI->getSubReg(ScratchRsrcReg, AMDGPU::sub1);
@@ -151,11 +152,11 @@ bool SIPrepareScratchRegs::runOnMachineFunction(MachineFunction &MF) {
               .addReg(ScratchRsrcReg, RegState::ImplicitDefine);
 
       BuildMI(MBB, I, DL, TII->get(AMDGPU::S_MOV_B32), Rsrc2)
-              .addImm(Rsrc23 & 0xffffffff)
+              .addImm(Rsrc & 0xffffffff)
               .addReg(ScratchRsrcReg, RegState::ImplicitDefine);
 
       BuildMI(MBB, I, DL, TII->get(AMDGPU::S_MOV_B32), Rsrc3)
-              .addImm(Rsrc23 >> 32)
+              .addImm(Rsrc >> 32)
               .addReg(ScratchRsrcReg, RegState::ImplicitDefine);
 
       // Scratch Offset
diff --git a/lib/Target/AMDGPU/SIRegisterInfo.cpp b/lib/Target/AMDGPU/SIRegisterInfo.cpp
index e9e8412..54c4d54 100644
--- a/lib/Target/AMDGPU/SIRegisterInfo.cpp
+++ b/lib/Target/AMDGPU/SIRegisterInfo.cpp
@@ -26,25 +26,23 @@ using namespace llvm;
 
 SIRegisterInfo::SIRegisterInfo() : AMDGPURegisterInfo() {}
 
-void SIRegisterInfo::reserveRegisterTuples(BitVector &Reserved, unsigned Reg) const {
-  MCRegAliasIterator R(Reg, this, true);
-
-  for (; R.isValid(); ++R)
-    Reserved.set(*R);
-}
-
 BitVector SIRegisterInfo::getReservedRegs(const MachineFunction &MF) const {
   BitVector Reserved(getNumRegs());
-  Reserved.set(AMDGPU::INDIRECT_BASE_ADDR);
+  Reserved.set(AMDGPU::EXEC);
 
-  // EXEC_LO and EXEC_HI could be allocated and used as regular register, but
-  // this seems likely to result in bugs, so I'm marking them as reserved.
-  reserveRegisterTuples(Reserved, AMDGPU::EXEC);
-  reserveRegisterTuples(Reserved, AMDGPU::FLAT_SCR);
+  // EXEC_LO and EXEC_HI could be allocated and used as regular register,
+  // but this seems likely to result in bugs, so I'm marking them as reserved.
+  Reserved.set(AMDGPU::EXEC_LO);
+  Reserved.set(AMDGPU::EXEC_HI);
+
+  Reserved.set(AMDGPU::INDIRECT_BASE_ADDR);
+  Reserved.set(AMDGPU::FLAT_SCR);
+  Reserved.set(AMDGPU::FLAT_SCR_LO);
+  Reserved.set(AMDGPU::FLAT_SCR_HI);
 
   // Reserve some VGPRs to use as temp registers in case we have to spill VGPRs
-  reserveRegisterTuples(Reserved, AMDGPU::VGPR254);
-  reserveRegisterTuples(Reserved, AMDGPU::VGPR255);
+  Reserved.set(AMDGPU::VGPR255);
+  Reserved.set(AMDGPU::VGPR254);
 
   // Tonga and Iceland can only allocate a fixed number of SGPRs due
   // to a hw bug.
@@ -56,7 +54,10 @@ BitVector SIRegisterInfo::getReservedRegs(const MachineFunction &MF) const {
 
     for (unsigned i = Limit; i < NumSGPRs; ++i) {
       unsigned Reg = AMDGPU::SGPR_32RegClass.getRegister(i);
-      reserveRegisterTuples(Reserved, Reg);
+      MCRegAliasIterator R = MCRegAliasIterator(Reg, this, true);
+
+      for (; R.isValid(); ++R)
+        Reserved.set(*R);
     }
   }
 
diff --git a/lib/Target/AMDGPU/SIRegisterInfo.h b/lib/Target/AMDGPU/SIRegisterInfo.h
index 7da6de2..bfdb67c 100644
--- a/lib/Target/AMDGPU/SIRegisterInfo.h
+++ b/lib/Target/AMDGPU/SIRegisterInfo.h
@@ -23,10 +23,7 @@
 namespace llvm {
 
 struct SIRegisterInfo : public AMDGPURegisterInfo {
-private:
-  void reserveRegisterTuples(BitVector &, unsigned Reg) const;
 
-public:
   SIRegisterInfo();
 
   BitVector getReservedRegs(const MachineFunction &MF) const override;
diff --git a/lib/Target/ARM/ARMISelLowering.cpp b/lib/Target/ARM/ARMISelLowering.cpp
index 8cc06df..e335784 100644
--- a/lib/Target/ARM/ARMISelLowering.cpp
+++ b/lib/Target/ARM/ARMISelLowering.cpp
@@ -4583,12 +4583,6 @@ static SDValue LowerVSETCC(SDValue Op, SelectionDAG &DAG) {
   ISD::CondCode SetCCOpcode = cast<CondCodeSDNode>(CC)->get();
   SDLoc dl(Op);
 
-  if (CmpVT.getVectorElementType() == MVT::i64)
-    // 64-bit comparisons are not legal. We've marked SETCC as non-Custom,
-    // but it's possible that our operands are 64-bit but our result is 32-bit.
-    // Bail in this case.
-    return SDValue();
-
   if (Op1.getValueType().isFloatingPoint()) {
     switch (SetCCOpcode) {
     default: llvm_unreachable("Illegal FP comparison");
diff --git a/lib/Target/ARM/AsmParser/ARMAsmParser.cpp b/lib/Target/ARM/AsmParser/ARMAsmParser.cpp
index cf6b892..f8f0eb2 100644
--- a/lib/Target/ARM/AsmParser/ARMAsmParser.cpp
+++ b/lib/Target/ARM/AsmParser/ARMAsmParser.cpp
@@ -15,7 +15,6 @@
 #include "llvm/ADT/SmallVector.h"
 #include "llvm/ADT/StringExtras.h"
 #include "llvm/ADT/StringSwitch.h"
-#include "llvm/ADT/Triple.h"
 #include "llvm/ADT/Twine.h"
 #include "llvm/MC/MCAsmInfo.h"
 #include "llvm/MC/MCAssembler.h"
@@ -9105,10 +9104,6 @@ bool ARMAsmParser::parseDirectiveArch(SMLoc L) {
     return false;
   }
 
-  Triple T;
-  STI.setDefaultFeatures(T.getARMCPUForArch(Arch));
-  setAvailableFeatures(ComputeAvailableFeatures(STI.getFeatureBits()));
-
   getTargetStreamer().emitArch(ID);
   return false;
 }
diff --git a/lib/Target/BPF/BPFISelDAGToDAG.cpp b/lib/Target/BPF/BPFISelDAGToDAG.cpp
index 9d5f1d4..d9e654c 100644
--- a/lib/Target/BPF/BPFISelDAGToDAG.cpp
+++ b/lib/Target/BPF/BPFISelDAGToDAG.cpp
@@ -50,7 +50,6 @@ private:
 
   // Complex Pattern for address selection.
   bool SelectAddr(SDValue Addr, SDValue &Base, SDValue &Offset);
-  bool SelectFIAddr(SDValue Addr, SDValue &Base, SDValue &Offset);
 };
 }
 
@@ -68,7 +67,7 @@ bool BPFDAGToDAGISel::SelectAddr(SDValue Addr, SDValue &Base, SDValue &Offset) {
       Addr.getOpcode() == ISD::TargetGlobalAddress)
     return false;
 
-  // Addresses of the form Addr+const or Addr|const
+  // Addresses of the form FI+const or FI|const
   if (CurDAG->isBaseWithConstantOffset(Addr)) {
     ConstantSDNode *CN = dyn_cast<ConstantSDNode>(Addr.getOperand(1));
     if (isInt<32>(CN->getSExtValue())) {
@@ -90,31 +89,6 @@ bool BPFDAGToDAGISel::SelectAddr(SDValue Addr, SDValue &Base, SDValue &Offset) {
   return true;
 }
 
-// ComplexPattern used on BPF FI instruction
-bool BPFDAGToDAGISel::SelectFIAddr(SDValue Addr, SDValue &Base, SDValue &Offset) {
-  SDLoc DL(Addr);
-
-  if (!CurDAG->isBaseWithConstantOffset(Addr))
-    return false;
-
-  // Addresses of the form Addr+const or Addr|const
-  ConstantSDNode *CN = dyn_cast<ConstantSDNode>(Addr.getOperand(1));
-  if (isInt<32>(CN->getSExtValue())) {
-
-    // If the first operand is a FI, get the TargetFI Node
-    if (FrameIndexSDNode *FIN =
-            dyn_cast<FrameIndexSDNode>(Addr.getOperand(0)))
-      Base = CurDAG->getTargetFrameIndex(FIN->getIndex(), MVT::i64);
-    else
-      return false;
-
-    Offset = CurDAG->getTargetConstant(CN->getSExtValue(), DL, MVT::i64);
-    return true;
-  }
-
-  return false;
-}
-
 SDNode *BPFDAGToDAGISel::Select(SDNode *Node) {
   unsigned Opcode = Node->getOpcode();
 
@@ -130,6 +104,13 @@ SDNode *BPFDAGToDAGISel::Select(SDNode *Node) {
   // tablegen selection should be handled here.
   switch (Opcode) {
   default: break;
+
+  case ISD::UNDEF: {
+    errs() << "BUG: "; Node->dump(CurDAG); errs() << '\n';
+    report_fatal_error("shouldn't see UNDEF during Select");
+    break;
+  }
+
   case ISD::INTRINSIC_W_CHAIN: {
     unsigned IntNo = cast<ConstantSDNode>(Node->getOperand(1))->getZExtValue();
     switch (IntNo) {
diff --git a/lib/Target/BPF/BPFISelLowering.cpp b/lib/Target/BPF/BPFISelLowering.cpp
index 7341828..58498a1 100644
--- a/lib/Target/BPF/BPFISelLowering.cpp
+++ b/lib/Target/BPF/BPFISelLowering.cpp
@@ -102,7 +102,6 @@ BPFTargetLowering::BPFTargetLowering(const TargetMachine &TM,
 
   setOperationAction(ISD::BR_CC, MVT::i64, Custom);
   setOperationAction(ISD::BR_JT, MVT::Other, Expand);
-  setOperationAction(ISD::BRIND, MVT::Other, Expand);
   setOperationAction(ISD::BRCOND, MVT::Other, Expand);
   setOperationAction(ISD::SETCC, MVT::i64, Expand);
   setOperationAction(ISD::SELECT, MVT::i64, Expand);
@@ -129,6 +128,9 @@ BPFTargetLowering::BPFTargetLowering(const TargetMachine &TM,
   setOperationAction(ISD::SUBC, MVT::i64, Expand);
   setOperationAction(ISD::SUBE, MVT::i64, Expand);
 
+  // no UNDEF allowed
+  setOperationAction(ISD::UNDEF, MVT::i64, Expand);
+
   setOperationAction(ISD::ROTR, MVT::i64, Expand);
   setOperationAction(ISD::ROTL, MVT::i64, Expand);
   setOperationAction(ISD::SHL_PARTS, MVT::i64, Expand);
diff --git a/lib/Target/BPF/BPFInstrInfo.td b/lib/Target/BPF/BPFInstrInfo.td
index 6b73db8..26b2cfe 100644
--- a/lib/Target/BPF/BPFInstrInfo.td
+++ b/lib/Target/BPF/BPFInstrInfo.td
@@ -54,8 +54,7 @@ def i64immSExt32 : PatLeaf<(imm),
                 [{return isInt<32>(N->getSExtValue()); }]>;
 
 // Addressing modes.
-def ADDRri : ComplexPattern<i64, 2, "SelectAddr", [], []>;
-def FIri : ComplexPattern<i64, 2, "SelectFIAddr", [add, or], []>;
+def ADDRri : ComplexPattern<i64, 2, "SelectAddr", [frameindex], []>;
 
 // Address operands
 def MEMri : Operand<i64> {
@@ -261,15 +260,6 @@ def MOV_rr : MOV_RR<"mov">;
 def MOV_ri : MOV_RI<"mov">;
 }
 
-def FI_ri
-    : InstBPF<(outs GPR:$dst), (ins MEMri:$addr),
-               "lea\t$dst, $addr",
-               [(set i64:$dst, FIri:$addr)]> {
-  // This is a tentative instruction, and will be replaced
-  // with MOV_rr and ADD_ri in PEI phase
-}
-
-
 def LD_pseudo
     : InstBPF<(outs GPR:$dst), (ins i64imm:$pseudo, u64imm:$imm),
               "ld_pseudo\t$dst, $pseudo, $imm",
diff --git a/lib/Target/BPF/BPFRegisterInfo.cpp b/lib/Target/BPF/BPFRegisterInfo.cpp
index 952615b..8f885c3 100644
--- a/lib/Target/BPF/BPFRegisterInfo.cpp
+++ b/lib/Target/BPF/BPFRegisterInfo.cpp
@@ -58,13 +58,14 @@ void BPFRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
 
   unsigned FrameReg = getFrameRegister(MF);
   int FrameIndex = MI.getOperand(i).getIndex();
-  const TargetInstrInfo &TII = *MF.getSubtarget().getInstrInfo();
-  MachineBasicBlock &MBB = *MI.getParent();
 
   if (MI.getOpcode() == BPF::MOV_rr) {
+    const TargetInstrInfo &TII = *MF.getSubtarget().getInstrInfo();
     int Offset = MF.getFrameInfo()->getObjectOffset(FrameIndex);
 
     MI.getOperand(i).ChangeToRegister(FrameReg, false);
+
+    MachineBasicBlock &MBB = *MI.getParent();
     unsigned reg = MI.getOperand(i - 1).getReg();
     BuildMI(MBB, ++II, DL, TII.get(BPF::ADD_ri), reg)
         .addReg(reg)
@@ -78,24 +79,8 @@ void BPFRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
   if (!isInt<32>(Offset))
     llvm_unreachable("bug in frame offset");
 
-  if (MI.getOpcode() == BPF::FI_ri) {
-    // architecture does not really support FI_ri, replace it with
-    //    MOV_rr <target_reg>, frame_reg
-    //    ADD_ri <target_reg>, imm
-    unsigned reg = MI.getOperand(i - 1).getReg();
-
-    BuildMI(MBB, ++II, DL, TII.get(BPF::MOV_rr), reg)
-        .addReg(FrameReg);
-    BuildMI(MBB, II, DL, TII.get(BPF::ADD_ri), reg)
-        .addReg(reg)
-        .addImm(Offset);
-
-    // Remove FI_ri instruction
-    MI.eraseFromParent();
-  } else {
-    MI.getOperand(i).ChangeToRegister(FrameReg, false);
-    MI.getOperand(i + 1).ChangeToImmediate(Offset);
-  }
+  MI.getOperand(i).ChangeToRegister(FrameReg, false);
+  MI.getOperand(i + 1).ChangeToImmediate(Offset);
 }
 
 unsigned BPFRegisterInfo::getFrameRegister(const MachineFunction &MF) const {
diff --git a/lib/Target/LLVMBuild.txt b/lib/Target/LLVMBuild.txt
index f05d7a4..8a90966 100644
--- a/lib/Target/LLVMBuild.txt
+++ b/lib/Target/LLVMBuild.txt
@@ -29,6 +29,7 @@ subdirectories =
  NVPTX
  Mips
  PowerPC
+ RISCV
  Sparc
  SystemZ
  WebAssembly
diff --git a/lib/Target/Mips/MipsISelLowering.h b/lib/Target/Mips/MipsISelLowering.h
index b3d861d..6fe8f83 100644
--- a/lib/Target/Mips/MipsISelLowering.h
+++ b/lib/Target/Mips/MipsISelLowering.h
@@ -269,14 +269,6 @@ namespace llvm {
     unsigned getRegisterByName(const char* RegName, EVT VT,
                                SelectionDAG &DAG) const override;
 
-    /// Returns true if a cast between SrcAS and DestAS is a noop.
-    bool isNoopAddrSpaceCast(unsigned SrcAS, unsigned DestAS) const override {
-      // Mips doesn't have any special address spaces so we just reserve
-      // the first 256 for software use (e.g. OpenCL) and treat casts
-      // between them as noops.
-      return SrcAS < 256 && DestAS < 256;
-    }
-
   protected:
     SDValue getGlobalReg(SelectionDAG &DAG, EVT Ty) const;
 
diff --git a/lib/Target/Mips/MipsSEISelDAGToDAG.cpp b/lib/Target/Mips/MipsSEISelDAGToDAG.cpp
index 2ebfbd1..cb46d73 100644
--- a/lib/Target/Mips/MipsSEISelDAGToDAG.cpp
+++ b/lib/Target/Mips/MipsSEISelDAGToDAG.cpp
@@ -115,11 +115,6 @@ bool MipsSEDAGToDAGISel::replaceUsesWithZeroReg(MachineRegisterInfo *MRI,
     if (MI->isPHI() || MI->isRegTiedToDefOperand(OpNo) || MI->isPseudo())
       continue;
 
-    // Also, we have to check that the register class of the operand
-    // contains the zero register.
-    if (!MRI->getRegClass(MO.getReg())->contains(ZeroReg))
-      continue;
-
     MO.setReg(ZeroReg);
   }
 
diff --git a/lib/Target/PowerPC/PPCAsmPrinter.cpp b/lib/Target/PowerPC/PPCAsmPrinter.cpp
index 8e118ec..4444466 100644
--- a/lib/Target/PowerPC/PPCAsmPrinter.cpp
+++ b/lib/Target/PowerPC/PPCAsmPrinter.cpp
@@ -947,11 +947,11 @@ void PPCAsmPrinter::EmitInstruction(const MachineInstr *MI) {
     return;
   }
   case PPC::ADDISdtprelHA:
-    // Transform: %Xd = ADDISdtprelHA %Xs, <ga:@sym>
-    // Into:      %Xd = ADDIS8 %Xs, sym@dtprel@ha
+    // Transform: %Xd = ADDISdtprelHA %X3, <ga:@sym>
+    // Into:      %Xd = ADDIS8 %X3, sym@dtprel@ha
   case PPC::ADDISdtprelHA32: {
-    // Transform: %Rd = ADDISdtprelHA32 %Rs, <ga:@sym>
-    // Into:      %Rd = ADDIS %Rs, sym@dtprel@ha
+    // Transform: %Rd = ADDISdtprelHA32 %R3, <ga:@sym>
+    // Into:      %Rd = ADDIS %R3, sym@dtprel@ha
     const MachineOperand &MO = MI->getOperand(2);
     const GlobalValue *GValue = MO.getGlobal();
     MCSymbol *MOSymbol = getSymbol(GValue);
@@ -962,7 +962,7 @@ void PPCAsmPrinter::EmitInstruction(const MachineInstr *MI) {
         *OutStreamer,
         MCInstBuilder(Subtarget->isPPC64() ? PPC::ADDIS8 : PPC::ADDIS)
             .addReg(MI->getOperand(0).getReg())
-            .addReg(MI->getOperand(1).getReg())
+            .addReg(Subtarget->isPPC64() ? PPC::X3 : PPC::R3)
             .addExpr(SymDtprel));
     return;
   }
diff --git a/lib/Target/PowerPC/PPCCTRLoops.cpp b/lib/Target/PowerPC/PPCCTRLoops.cpp
index fd150be..baadf08 100644
--- a/lib/Target/PowerPC/PPCCTRLoops.cpp
+++ b/lib/Target/PowerPC/PPCCTRLoops.cpp
@@ -197,18 +197,10 @@ static bool isLargeIntegerTy(bool Is32Bit, Type *Ty) {
 // Determining the address of a TLS variable results in a function call in
 // certain TLS models.
 static bool memAddrUsesCTR(const PPCTargetMachine *TM,
-                           const Value *MemAddr) {
+                           const llvm::Value *MemAddr) {
   const auto *GV = dyn_cast<GlobalValue>(MemAddr);
-  if (!GV) {
-    // Recurse to check for constants that refer to TLS global variables.
-    if (const auto *CV = dyn_cast<Constant>(MemAddr))
-      for (const auto &CO : CV->operands())
-        if (memAddrUsesCTR(TM, CO))
-          return true;
-
+  if (!GV)
     return false;
-  }
-
   if (!GV->isThreadLocal())
     return false;
   if (!TM)
@@ -247,11 +239,6 @@ bool PPCCTRLoops::mightUseCTR(const Triple &TT, BasicBlock *BB) {
         if (F->getIntrinsicID() != Intrinsic::not_intrinsic) {
           switch (F->getIntrinsicID()) {
           default: continue;
-          // If we have a call to ppc_is_decremented_ctr_nonzero, or ppc_mtctr
-          // we're definitely using CTR.
-          case Intrinsic::ppc_is_decremented_ctr_nonzero:
-	  case Intrinsic::ppc_mtctr:
-	    return true;
 
 // VisualStudio defines setjmp as _setjmp
 #if defined(_MSC_VER) && defined(setjmp) && \
@@ -439,7 +426,6 @@ bool PPCCTRLoops::convertToCTRLoop(Loop *L) {
   // Process nested loops first.
   for (Loop::iterator I = L->begin(), E = L->end(); I != E; ++I) {
     MadeChange |= convertToCTRLoop(*I);
-    DEBUG(dbgs() << "Nested loop converted\n");
   }
 
   // If a nested loop has been converted, then we can't convert this loop.
diff --git a/lib/Target/PowerPC/PPCISelDAGToDAG.cpp b/lib/Target/PowerPC/PPCISelDAGToDAG.cpp
index 9322268..01a3acb 100644
--- a/lib/Target/PowerPC/PPCISelDAGToDAG.cpp
+++ b/lib/Target/PowerPC/PPCISelDAGToDAG.cpp
@@ -2305,15 +2305,14 @@ SDNode *PPCDAGToDAGISel::SelectSETCC(SDNode *N) {
     if (Swap)
       std::swap(LHS, RHS);
 
-    EVT ResVT = VecVT.changeVectorElementTypeToInteger();
     if (Negate) {
-      SDValue VCmp(CurDAG->getMachineNode(VCmpInst, dl, ResVT, LHS, RHS), 0);
+      SDValue VCmp(CurDAG->getMachineNode(VCmpInst, dl, VecVT, LHS, RHS), 0);
       return CurDAG->SelectNodeTo(N, PPCSubTarget->hasVSX() ? PPC::XXLNOR :
                                                               PPC::VNOR,
-                                  ResVT, VCmp, VCmp);
+                                  VecVT, VCmp, VCmp);
     }
 
-    return CurDAG->SelectNodeTo(N, VCmpInst, ResVT, LHS, RHS);
+    return CurDAG->SelectNodeTo(N, VCmpInst, VecVT, LHS, RHS);
   }
 
   if (PPCSubTarget->useCRBits())
@@ -2570,25 +2569,13 @@ SDNode *PPCDAGToDAGISel::Select(SDNode *N) {
       return nullptr;
     }
     // ISD::OR doesn't get all the bitfield insertion fun.
-    // (and (or x, c1), c2) where isRunOfOnes(~(c1^c2)) might be a
-    // bitfield insert.
+    // (and (or x, c1), c2) where isRunOfOnes(~(c1^c2)) is a bitfield insert
     if (isInt32Immediate(N->getOperand(1), Imm) &&
         N->getOperand(0).getOpcode() == ISD::OR &&
         isInt32Immediate(N->getOperand(0).getOperand(1), Imm2)) {
-      // The idea here is to check whether this is equivalent to:
-      //   (c1 & m) | (x & ~m)
-      // where m is a run-of-ones mask. The logic here is that, for each bit in
-      // c1 and c2:
-      //  - if both are 1, then the output will be 1.
-      //  - if both are 0, then the output will be 0.
-      //  - if the bit in c1 is 0, and the bit in c2 is 1, then the output will
-      //    come from x.
-      //  - if the bit in c1 is 1, and the bit in c2 is 0, then the output will
-      //    be 0.
-      //  If that last condition is never the case, then we can form m from the
-      //  bits that are the same between c1 and c2.
       unsigned MB, ME;
-      if (isRunOfOnes(~(Imm^Imm2), MB, ME) && !(~Imm & Imm2)) {
+      Imm = ~(Imm^Imm2);
+      if (isRunOfOnes(Imm, MB, ME)) {
         SDValue Ops[] = { N->getOperand(0).getOperand(0),
                             N->getOperand(0).getOperand(1),
                             getI32Imm(0, dl), getI32Imm(MB, dl),
@@ -2799,8 +2786,6 @@ SDNode *PPCDAGToDAGISel::Select(SDNode *N) {
         SDValue Base, Offset;
 
         if (LD->isUnindexed() &&
-            (LD->getMemoryVT() == MVT::f64 ||
-             LD->getMemoryVT() == MVT::i64) &&
             SelectAddrIdxOnly(LD->getBasePtr(), Base, Offset)) {
           SDValue Chain = LD->getChain();
           SDValue Ops[] = { Base, Offset, Chain };
diff --git a/lib/Target/PowerPC/PPCISelLowering.cpp b/lib/Target/PowerPC/PPCISelLowering.cpp
index 1b8f8fb..c1473b2 100644
--- a/lib/Target/PowerPC/PPCISelLowering.cpp
+++ b/lib/Target/PowerPC/PPCISelLowering.cpp
@@ -431,8 +431,6 @@ PPCTargetLowering::PPCTargetLowering(const PPCTargetMachine &TM,
       AddPromotedToType (ISD::LOAD  , VT, MVT::v4i32);
       setOperationAction(ISD::SELECT, VT, Promote);
       AddPromotedToType (ISD::SELECT, VT, MVT::v4i32);
-      setOperationAction(ISD::SELECT_CC, VT, Promote);
-      AddPromotedToType (ISD::SELECT_CC, VT, MVT::v4i32);
       setOperationAction(ISD::STORE, VT, Promote);
       AddPromotedToType (ISD::STORE, VT, MVT::v4i32);
 
@@ -7177,6 +7175,7 @@ SDValue PPCTargetLowering::LowerVECTOR_SHUFFLE(SDValue Op,
         PPC::isSplatShuffleMask(SVOp, 4) ||
         PPC::isVPKUWUMShuffleMask(SVOp, 1, DAG) ||
         PPC::isVPKUHUMShuffleMask(SVOp, 1, DAG) ||
+        PPC::isVPKUDUMShuffleMask(SVOp, 1, DAG) ||
         PPC::isVSLDOIShuffleMask(SVOp, 1, DAG) != -1 ||
         PPC::isVMRGLShuffleMask(SVOp, 1, 1, DAG) ||
         PPC::isVMRGLShuffleMask(SVOp, 2, 1, DAG) ||
@@ -7184,10 +7183,8 @@ SDValue PPCTargetLowering::LowerVECTOR_SHUFFLE(SDValue Op,
         PPC::isVMRGHShuffleMask(SVOp, 1, 1, DAG) ||
         PPC::isVMRGHShuffleMask(SVOp, 2, 1, DAG) ||
         PPC::isVMRGHShuffleMask(SVOp, 4, 1, DAG) ||
-        (Subtarget.hasP8Altivec() && (
-         PPC::isVPKUDUMShuffleMask(SVOp, 1, DAG) ||
-         PPC::isVMRGEOShuffleMask(SVOp, true, 1, DAG) ||
-         PPC::isVMRGEOShuffleMask(SVOp, false, 1, DAG)))) {
+        PPC::isVMRGEOShuffleMask(SVOp, true, 1, DAG)   ||
+        PPC::isVMRGEOShuffleMask(SVOp, false, 1, DAG)) {
       return Op;
     }
   }
@@ -7198,6 +7195,7 @@ SDValue PPCTargetLowering::LowerVECTOR_SHUFFLE(SDValue Op,
   unsigned int ShuffleKind = isLittleEndian ? 2 : 0;
   if (PPC::isVPKUWUMShuffleMask(SVOp, ShuffleKind, DAG) ||
       PPC::isVPKUHUMShuffleMask(SVOp, ShuffleKind, DAG) ||
+      PPC::isVPKUDUMShuffleMask(SVOp, ShuffleKind, DAG) ||
       PPC::isVSLDOIShuffleMask(SVOp, ShuffleKind, DAG) != -1 ||
       PPC::isVMRGLShuffleMask(SVOp, 1, ShuffleKind, DAG) ||
       PPC::isVMRGLShuffleMask(SVOp, 2, ShuffleKind, DAG) ||
@@ -7205,10 +7203,8 @@ SDValue PPCTargetLowering::LowerVECTOR_SHUFFLE(SDValue Op,
       PPC::isVMRGHShuffleMask(SVOp, 1, ShuffleKind, DAG) ||
       PPC::isVMRGHShuffleMask(SVOp, 2, ShuffleKind, DAG) ||
       PPC::isVMRGHShuffleMask(SVOp, 4, ShuffleKind, DAG) ||
-      (Subtarget.hasP8Altivec() && (
-       PPC::isVPKUDUMShuffleMask(SVOp, ShuffleKind, DAG) ||
-       PPC::isVMRGEOShuffleMask(SVOp, true, ShuffleKind, DAG) ||
-       PPC::isVMRGEOShuffleMask(SVOp, false, ShuffleKind, DAG))))
+      PPC::isVMRGEOShuffleMask(SVOp, true, ShuffleKind, DAG)             ||
+      PPC::isVMRGEOShuffleMask(SVOp, false, ShuffleKind, DAG))
     return Op;
 
   // Check to see if this is a shuffle of 4-byte values.  If so, we can use our
@@ -9970,9 +9966,6 @@ SDValue PPCTargetLowering::combineFPToIntToFP(SDNode *N,
     if (Src.getValueType() == MVT::f32) {
       Src = DAG.getNode(ISD::FP_EXTEND, dl, MVT::f64, Src);
       DCI.AddToWorklist(Src.getNode());
-    } else if (Src.getValueType() != MVT::f64) {
-      // Make sure that we don't pick up a ppc_fp128 source value.
-      return SDValue();
     }
 
     unsigned FCTOp =
diff --git a/lib/Target/PowerPC/PPCInstrInfo.cpp b/lib/Target/PowerPC/PPCInstrInfo.cpp
index d4e666c..bf6e402 100644
--- a/lib/Target/PowerPC/PPCInstrInfo.cpp
+++ b/lib/Target/PowerPC/PPCInstrInfo.cpp
@@ -309,11 +309,6 @@ PPCInstrInfo::commuteInstruction(MachineInstr *MI, bool NewMI) const {
   unsigned MB = MI->getOperand(4).getImm();
   unsigned ME = MI->getOperand(5).getImm();
 
-  // We can't commute a trivial mask (there is no way to represent an all-zero
-  // mask).
-  if (MB == 0 && ME == 31)
-    return nullptr;
-
   if (NewMI) {
     // Create a new instruction.
     unsigned Reg0 = ChangeReg0 ? Reg2 : MI->getOperand(0).getReg();
diff --git a/lib/Target/PowerPC/PPCInstrInfo.td b/lib/Target/PowerPC/PPCInstrInfo.td
index 24fd9bd..b50124d 100644
--- a/lib/Target/PowerPC/PPCInstrInfo.td
+++ b/lib/Target/PowerPC/PPCInstrInfo.td
@@ -2835,84 +2835,24 @@ def : Pat<(i64 (anyext i1:$in)),
           (SELECT_I8 $in, (LI8 1), (LI8 0))>;
 
 // match setcc on i1 variables.
-// CRANDC is:
-//   1 1 : F
-//   1 0 : T
-//   0 1 : F
-//   0 0 : F
-//
-// LT is:
-//  -1 -1  : F
-//  -1  0  : T
-//   0 -1  : F
-//   0  0  : F
-//
-// ULT is:
-//   1 1 : F
-//   1 0 : F
-//   0 1 : T
-//   0 0 : F
 def : Pat<(i1 (setcc i1:$s1, i1:$s2, SETLT)),
-          (CRANDC $s1, $s2)>;
+          (CRANDC $s2, $s1)>;
 def : Pat<(i1 (setcc i1:$s1, i1:$s2, SETULT)),
           (CRANDC $s2, $s1)>;
-// CRORC is:
-//   1 1 : T
-//   1 0 : T
-//   0 1 : F
-//   0 0 : T
-//
-// LE is:
-//  -1 -1 : T
-//  -1  0 : T
-//   0 -1 : F
-//   0  0 : T
-//
-// ULE is:
-//   1 1 : T
-//   1 0 : F
-//   0 1 : T
-//   0 0 : T
 def : Pat<(i1 (setcc i1:$s1, i1:$s2, SETLE)),
-          (CRORC $s1, $s2)>;
+          (CRORC $s2, $s1)>;
 def : Pat<(i1 (setcc i1:$s1, i1:$s2, SETULE)),
           (CRORC $s2, $s1)>;
-
 def : Pat<(i1 (setcc i1:$s1, i1:$s2, SETEQ)),
           (CREQV $s1, $s2)>;
-
-// GE is:
-//  -1 -1 : T
-//  -1  0 : F
-//   0 -1 : T
-//   0  0 : T
-//
-// UGE is:
-//   1 1 : T
-//   1 0 : T
-//   0 1 : F
-//   0 0 : T
 def : Pat<(i1 (setcc i1:$s1, i1:$s2, SETGE)),
-          (CRORC $s2, $s1)>;
+          (CRORC $s1, $s2)>;
 def : Pat<(i1 (setcc i1:$s1, i1:$s2, SETUGE)),
           (CRORC $s1, $s2)>;
-
-// GT is:
-//  -1 -1 : F
-//  -1  0 : F
-//   0 -1 : T
-//   0  0 : F
-//
-// UGT is:
-//  1 1 : F
-//  1 0 : T
-//  0 1 : F
-//  0 0 : F
 def : Pat<(i1 (setcc i1:$s1, i1:$s2, SETGT)),
-          (CRANDC $s2, $s1)>;
+          (CRANDC $s1, $s2)>;
 def : Pat<(i1 (setcc i1:$s1, i1:$s2, SETUGT)),
           (CRANDC $s1, $s2)>;
-
 def : Pat<(i1 (setcc i1:$s1, i1:$s2, SETNE)),
           (CRXOR $s1, $s2)>;
 
@@ -3263,30 +3203,18 @@ def : Pat<(i1 (select i1:$cond, i1:$tval, i1:$fval)),
 //   select (lhs == rhs), tval, fval is:
 //   ((lhs == rhs) & tval) | (!(lhs == rhs) & fval)
 def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETLT)),
-           (CROR (CRAND (CRANDC $lhs, $rhs), $tval),
-                 (CRAND (CRORC  $rhs, $lhs), $fval))>;
-def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETULT)),
            (CROR (CRAND (CRANDC $rhs, $lhs), $tval),
                  (CRAND (CRORC  $lhs, $rhs), $fval))>;
 def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETLE)),
-           (CROR (CRAND (CRORC  $lhs, $rhs), $tval),
-                 (CRAND (CRANDC $rhs, $lhs), $fval))>;
-def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETULE)),
            (CROR (CRAND (CRORC  $rhs, $lhs), $tval),
                  (CRAND (CRANDC $lhs, $rhs), $fval))>;
 def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETEQ)),
            (CROR (CRAND (CREQV $lhs, $rhs), $tval),
                  (CRAND (CRXOR $lhs, $rhs), $fval))>;
 def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETGE)),
-           (CROR (CRAND (CRORC  $rhs, $lhs), $tval),
-                 (CRAND (CRANDC $lhs, $rhs), $fval))>;
-def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETUGE)),
            (CROR (CRAND (CRORC  $lhs, $rhs), $tval),
                  (CRAND (CRANDC $rhs, $lhs), $fval))>;
 def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETGT)),
-           (CROR (CRAND (CRANDC $rhs, $lhs), $tval),
-                 (CRAND (CRORC  $lhs, $rhs), $fval))>;
-def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETUGT)),
            (CROR (CRAND (CRANDC $lhs, $rhs), $tval),
                  (CRAND (CRORC  $rhs, $lhs), $fval))>;
 def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETNE)),
@@ -3295,106 +3223,66 @@ def : Pat <(i1 (selectcc i1:$lhs, i1:$rhs, i1:$tval, i1:$fval, SETNE)),
 
 // match selectcc on i1 variables with non-i1 output.
 def : Pat<(i32 (selectcc i1:$lhs, i1:$rhs, i32:$tval, i32:$fval, SETLT)),
-          (SELECT_I4 (CRANDC $lhs, $rhs), $tval, $fval)>;
-def : Pat<(i32 (selectcc i1:$lhs, i1:$rhs, i32:$tval, i32:$fval, SETULT)),
           (SELECT_I4 (CRANDC $rhs, $lhs), $tval, $fval)>;
 def : Pat<(i32 (selectcc i1:$lhs, i1:$rhs, i32:$tval, i32:$fval, SETLE)),
-          (SELECT_I4 (CRORC  $lhs, $rhs), $tval, $fval)>;
-def : Pat<(i32 (selectcc i1:$lhs, i1:$rhs, i32:$tval, i32:$fval, SETULE)),
           (SELECT_I4 (CRORC  $rhs, $lhs), $tval, $fval)>;
 def : Pat<(i32 (selectcc i1:$lhs, i1:$rhs, i32:$tval, i32:$fval, SETEQ)),
           (SELECT_I4 (CREQV $lhs, $rhs), $tval, $fval)>;
 def : Pat<(i32 (selectcc i1:$lhs, i1:$rhs, i32:$tval, i32:$fval, SETGE)),
-          (SELECT_I4 (CRORC  $rhs, $lhs), $tval, $fval)>;
-def : Pat<(i32 (selectcc i1:$lhs, i1:$rhs, i32:$tval, i32:$fval, SETUGE)),
           (SELECT_I4 (CRORC  $lhs, $rhs), $tval, $fval)>;
 def : Pat<(i32 (selectcc i1:$lhs, i1:$rhs, i32:$tval, i32:$fval, SETGT)),
-          (SELECT_I4 (CRANDC $rhs, $lhs), $tval, $fval)>;
-def : Pat<(i32 (selectcc i1:$lhs, i1:$rhs, i32:$tval, i32:$fval, SETUGT)),
           (SELECT_I4 (CRANDC $lhs, $rhs), $tval, $fval)>;
 def : Pat<(i32 (selectcc i1:$lhs, i1:$rhs, i32:$tval, i32:$fval, SETNE)),
           (SELECT_I4 (CRXOR $lhs, $rhs), $tval, $fval)>;
 
 def : Pat<(i64 (selectcc i1:$lhs, i1:$rhs, i64:$tval, i64:$fval, SETLT)),
-          (SELECT_I8 (CRANDC $lhs, $rhs), $tval, $fval)>;
-def : Pat<(i64 (selectcc i1:$lhs, i1:$rhs, i64:$tval, i64:$fval, SETULT)),
           (SELECT_I8 (CRANDC $rhs, $lhs), $tval, $fval)>;
 def : Pat<(i64 (selectcc i1:$lhs, i1:$rhs, i64:$tval, i64:$fval, SETLE)),
-          (SELECT_I8 (CRORC  $lhs, $rhs), $tval, $fval)>;
-def : Pat<(i64 (selectcc i1:$lhs, i1:$rhs, i64:$tval, i64:$fval, SETULE)),
           (SELECT_I8 (CRORC  $rhs, $lhs), $tval, $fval)>;
 def : Pat<(i64 (selectcc i1:$lhs, i1:$rhs, i64:$tval, i64:$fval, SETEQ)),
           (SELECT_I8 (CREQV $lhs, $rhs), $tval, $fval)>;
 def : Pat<(i64 (selectcc i1:$lhs, i1:$rhs, i64:$tval, i64:$fval, SETGE)),
-          (SELECT_I8 (CRORC  $rhs, $lhs), $tval, $fval)>;
-def : Pat<(i64 (selectcc i1:$lhs, i1:$rhs, i64:$tval, i64:$fval, SETUGE)),
           (SELECT_I8 (CRORC  $lhs, $rhs), $tval, $fval)>;
 def : Pat<(i64 (selectcc i1:$lhs, i1:$rhs, i64:$tval, i64:$fval, SETGT)),
-          (SELECT_I8 (CRANDC $rhs, $lhs), $tval, $fval)>;
-def : Pat<(i64 (selectcc i1:$lhs, i1:$rhs, i64:$tval, i64:$fval, SETUGT)),
           (SELECT_I8 (CRANDC $lhs, $rhs), $tval, $fval)>;
 def : Pat<(i64 (selectcc i1:$lhs, i1:$rhs, i64:$tval, i64:$fval, SETNE)),
           (SELECT_I8 (CRXOR $lhs, $rhs), $tval, $fval)>;
 
 def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETLT)),
-          (SELECT_F4 (CRANDC $lhs, $rhs), $tval, $fval)>;
-def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETULT)),
           (SELECT_F4 (CRANDC $rhs, $lhs), $tval, $fval)>;
 def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETLE)),
-          (SELECT_F4 (CRORC  $lhs, $rhs), $tval, $fval)>;
-def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETULE)),
           (SELECT_F4 (CRORC  $rhs, $lhs), $tval, $fval)>;
 def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETEQ)),
           (SELECT_F4 (CREQV $lhs, $rhs), $tval, $fval)>;
 def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETGE)),
-          (SELECT_F4 (CRORC  $rhs, $lhs), $tval, $fval)>;
-def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETUGE)),
           (SELECT_F4 (CRORC  $lhs, $rhs), $tval, $fval)>;
 def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETGT)),
-          (SELECT_F4 (CRANDC $rhs, $lhs), $tval, $fval)>;
-def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETUGT)),
           (SELECT_F4 (CRANDC $lhs, $rhs), $tval, $fval)>;
 def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETNE)),
           (SELECT_F4 (CRXOR $lhs, $rhs), $tval, $fval)>;
 
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETLT)),
-          (SELECT_F8 (CRANDC $lhs, $rhs), $tval, $fval)>;
-def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETULT)),
           (SELECT_F8 (CRANDC $rhs, $lhs), $tval, $fval)>;
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETLE)),
-          (SELECT_F8 (CRORC  $lhs, $rhs), $tval, $fval)>;
-def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETULE)),
           (SELECT_F8 (CRORC  $rhs, $lhs), $tval, $fval)>;
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETEQ)),
           (SELECT_F8 (CREQV $lhs, $rhs), $tval, $fval)>;
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETGE)),
-          (SELECT_F8 (CRORC  $rhs, $lhs), $tval, $fval)>;
-def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETUGE)),
           (SELECT_F8 (CRORC  $lhs, $rhs), $tval, $fval)>;
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETGT)),
-          (SELECT_F8 (CRANDC $rhs, $lhs), $tval, $fval)>;
-def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETUGT)),
           (SELECT_F8 (CRANDC $lhs, $rhs), $tval, $fval)>;
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETNE)),
           (SELECT_F8 (CRXOR $lhs, $rhs), $tval, $fval)>;
 
 def : Pat<(v4i32 (selectcc i1:$lhs, i1:$rhs, v4i32:$tval, v4i32:$fval, SETLT)),
-          (SELECT_VRRC (CRANDC $lhs, $rhs), $tval, $fval)>;
-def : Pat<(v4i32 (selectcc i1:$lhs, i1:$rhs, v4i32:$tval, v4i32:$fval, SETULT)),
           (SELECT_VRRC (CRANDC $rhs, $lhs), $tval, $fval)>;
 def : Pat<(v4i32 (selectcc i1:$lhs, i1:$rhs, v4i32:$tval, v4i32:$fval, SETLE)),
-          (SELECT_VRRC (CRORC  $lhs, $rhs), $tval, $fval)>;
-def : Pat<(v4i32 (selectcc i1:$lhs, i1:$rhs, v4i32:$tval, v4i32:$fval, SETULE)),
           (SELECT_VRRC (CRORC  $rhs, $lhs), $tval, $fval)>;
 def : Pat<(v4i32 (selectcc i1:$lhs, i1:$rhs, v4i32:$tval, v4i32:$fval, SETEQ)),
           (SELECT_VRRC (CREQV $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4i32 (selectcc i1:$lhs, i1:$rhs, v4i32:$tval, v4i32:$fval, SETGE)),
-          (SELECT_VRRC (CRORC  $rhs, $lhs), $tval, $fval)>;
-def : Pat<(v4i32 (selectcc i1:$lhs, i1:$rhs, v4i32:$tval, v4i32:$fval, SETUGE)),
           (SELECT_VRRC (CRORC  $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4i32 (selectcc i1:$lhs, i1:$rhs, v4i32:$tval, v4i32:$fval, SETGT)),
-          (SELECT_VRRC (CRANDC $rhs, $lhs), $tval, $fval)>;
-def : Pat<(v4i32 (selectcc i1:$lhs, i1:$rhs, v4i32:$tval, v4i32:$fval, SETUGT)),
           (SELECT_VRRC (CRANDC $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4i32 (selectcc i1:$lhs, i1:$rhs, v4i32:$tval, v4i32:$fval, SETNE)),
           (SELECT_VRRC (CRXOR $lhs, $rhs), $tval, $fval)>;
diff --git a/lib/Target/PowerPC/PPCInstrQPX.td b/lib/Target/PowerPC/PPCInstrQPX.td
index 0a044c5..5c66b42 100644
--- a/lib/Target/PowerPC/PPCInstrQPX.td
+++ b/lib/Target/PowerPC/PPCInstrQPX.td
@@ -1115,64 +1115,40 @@ def : Pat<(v4f64 (PPCqbflt v4i1:$src)),
           (COPY_TO_REGCLASS $src, QFRC)>;
 
 def : Pat<(v4f64 (selectcc i1:$lhs, i1:$rhs, v4f64:$tval, v4f64:$fval, SETLT)),
-          (SELECT_QFRC (CRANDC $lhs, $rhs), $tval, $fval)>;
-def : Pat<(v4f64 (selectcc i1:$lhs, i1:$rhs, v4f64:$tval, v4f64:$fval, SETULT)),
           (SELECT_QFRC (CRANDC $rhs, $lhs), $tval, $fval)>;
 def : Pat<(v4f64 (selectcc i1:$lhs, i1:$rhs, v4f64:$tval, v4f64:$fval, SETLE)),
-          (SELECT_QFRC (CRORC  $lhs, $rhs), $tval, $fval)>;
-def : Pat<(v4f64 (selectcc i1:$lhs, i1:$rhs, v4f64:$tval, v4f64:$fval, SETULE)),
           (SELECT_QFRC (CRORC  $rhs, $lhs), $tval, $fval)>;
 def : Pat<(v4f64 (selectcc i1:$lhs, i1:$rhs, v4f64:$tval, v4f64:$fval, SETEQ)),
           (SELECT_QFRC (CREQV $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4f64 (selectcc i1:$lhs, i1:$rhs, v4f64:$tval, v4f64:$fval, SETGE)),
-          (SELECT_QFRC (CRORC  $rhs, $lhs), $tval, $fval)>;
-def : Pat<(v4f64 (selectcc i1:$lhs, i1:$rhs, v4f64:$tval, v4f64:$fval, SETUGE)),
           (SELECT_QFRC (CRORC  $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4f64 (selectcc i1:$lhs, i1:$rhs, v4f64:$tval, v4f64:$fval, SETGT)),
-          (SELECT_QFRC (CRANDC $rhs, $lhs), $tval, $fval)>;
-def : Pat<(v4f64 (selectcc i1:$lhs, i1:$rhs, v4f64:$tval, v4f64:$fval, SETUGT)),
           (SELECT_QFRC (CRANDC $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4f64 (selectcc i1:$lhs, i1:$rhs, v4f64:$tval, v4f64:$fval, SETNE)),
           (SELECT_QFRC (CRXOR $lhs, $rhs), $tval, $fval)>;
 
 def : Pat<(v4f32 (selectcc i1:$lhs, i1:$rhs, v4f32:$tval, v4f32:$fval, SETLT)),
-          (SELECT_QSRC (CRANDC $lhs, $rhs), $tval, $fval)>;
-def : Pat<(v4f32 (selectcc i1:$lhs, i1:$rhs, v4f32:$tval, v4f32:$fval, SETULT)),
           (SELECT_QSRC (CRANDC $rhs, $lhs), $tval, $fval)>;
 def : Pat<(v4f32 (selectcc i1:$lhs, i1:$rhs, v4f32:$tval, v4f32:$fval, SETLE)),
-          (SELECT_QSRC (CRORC  $lhs, $rhs), $tval, $fval)>;
-def : Pat<(v4f32 (selectcc i1:$lhs, i1:$rhs, v4f32:$tval, v4f32:$fval, SETULE)),
           (SELECT_QSRC (CRORC  $rhs, $lhs), $tval, $fval)>;
 def : Pat<(v4f32 (selectcc i1:$lhs, i1:$rhs, v4f32:$tval, v4f32:$fval, SETEQ)),
           (SELECT_QSRC (CREQV $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4f32 (selectcc i1:$lhs, i1:$rhs, v4f32:$tval, v4f32:$fval, SETGE)),
-          (SELECT_QSRC (CRORC  $rhs, $lhs), $tval, $fval)>;
-def : Pat<(v4f32 (selectcc i1:$lhs, i1:$rhs, v4f32:$tval, v4f32:$fval, SETUGE)),
           (SELECT_QSRC (CRORC  $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4f32 (selectcc i1:$lhs, i1:$rhs, v4f32:$tval, v4f32:$fval, SETGT)),
-          (SELECT_QSRC (CRANDC $rhs, $lhs), $tval, $fval)>;
-def : Pat<(v4f32 (selectcc i1:$lhs, i1:$rhs, v4f32:$tval, v4f32:$fval, SETUGT)),
           (SELECT_QSRC (CRANDC $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4f32 (selectcc i1:$lhs, i1:$rhs, v4f32:$tval, v4f32:$fval, SETNE)),
           (SELECT_QSRC (CRXOR $lhs, $rhs), $tval, $fval)>;
 
 def : Pat<(v4i1 (selectcc i1:$lhs, i1:$rhs, v4i1:$tval, v4i1:$fval, SETLT)),
-          (SELECT_QBRC (CRANDC $lhs, $rhs), $tval, $fval)>;
-def : Pat<(v4i1 (selectcc i1:$lhs, i1:$rhs, v4i1:$tval, v4i1:$fval, SETULT)),
           (SELECT_QBRC (CRANDC $rhs, $lhs), $tval, $fval)>;
 def : Pat<(v4i1 (selectcc i1:$lhs, i1:$rhs, v4i1:$tval, v4i1:$fval, SETLE)),
-          (SELECT_QBRC (CRORC  $lhs, $rhs), $tval, $fval)>;
-def : Pat<(v4i1 (selectcc i1:$lhs, i1:$rhs, v4i1:$tval, v4i1:$fval, SETULE)),
           (SELECT_QBRC (CRORC  $rhs, $lhs), $tval, $fval)>;
 def : Pat<(v4i1 (selectcc i1:$lhs, i1:$rhs, v4i1:$tval, v4i1:$fval, SETEQ)),
           (SELECT_QBRC (CREQV $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4i1 (selectcc i1:$lhs, i1:$rhs, v4i1:$tval, v4i1:$fval, SETGE)),
-          (SELECT_QBRC (CRORC  $rhs, $lhs), $tval, $fval)>;
-def : Pat<(v4i1 (selectcc i1:$lhs, i1:$rhs, v4i1:$tval, v4i1:$fval, SETUGE)),
           (SELECT_QBRC (CRORC  $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4i1 (selectcc i1:$lhs, i1:$rhs, v4i1:$tval, v4i1:$fval, SETGT)),
-          (SELECT_QBRC (CRANDC $rhs, $lhs), $tval, $fval)>;
-def : Pat<(v4i1 (selectcc i1:$lhs, i1:$rhs, v4i1:$tval, v4i1:$fval, SETUGT)),
           (SELECT_QBRC (CRANDC $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v4i1 (selectcc i1:$lhs, i1:$rhs, v4i1:$tval, v4i1:$fval, SETNE)),
           (SELECT_QBRC (CRXOR $lhs, $rhs), $tval, $fval)>;
diff --git a/lib/Target/PowerPC/PPCInstrVSX.td b/lib/Target/PowerPC/PPCInstrVSX.td
index ce63c22..20c95fe 100644
--- a/lib/Target/PowerPC/PPCInstrVSX.td
+++ b/lib/Target/PowerPC/PPCInstrVSX.td
@@ -958,43 +958,27 @@ def : Pat<(v4i32 (PPCxxswapd v4i32:$src)), (XXPERMDI $src, $src, 2)>;
 
 // Selects.
 def : Pat<(v2f64 (selectcc i1:$lhs, i1:$rhs, v2f64:$tval, v2f64:$fval, SETLT)),
-          (SELECT_VSRC (CRANDC $lhs, $rhs), $tval, $fval)>;
-def : Pat<(v2f64 (selectcc i1:$lhs, i1:$rhs, v2f64:$tval, v2f64:$fval, SETULT)),
           (SELECT_VSRC (CRANDC $rhs, $lhs), $tval, $fval)>;
 def : Pat<(v2f64 (selectcc i1:$lhs, i1:$rhs, v2f64:$tval, v2f64:$fval, SETLE)),
-          (SELECT_VSRC (CRORC  $lhs, $rhs), $tval, $fval)>;
-def : Pat<(v2f64 (selectcc i1:$lhs, i1:$rhs, v2f64:$tval, v2f64:$fval, SETULE)),
           (SELECT_VSRC (CRORC  $rhs, $lhs), $tval, $fval)>;
 def : Pat<(v2f64 (selectcc i1:$lhs, i1:$rhs, v2f64:$tval, v2f64:$fval, SETEQ)),
           (SELECT_VSRC (CREQV $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v2f64 (selectcc i1:$lhs, i1:$rhs, v2f64:$tval, v2f64:$fval, SETGE)),
-          (SELECT_VSRC (CRORC  $rhs, $lhs), $tval, $fval)>;
-def : Pat<(v2f64 (selectcc i1:$lhs, i1:$rhs, v2f64:$tval, v2f64:$fval, SETUGE)),
           (SELECT_VSRC (CRORC  $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v2f64 (selectcc i1:$lhs, i1:$rhs, v2f64:$tval, v2f64:$fval, SETGT)),
-          (SELECT_VSRC (CRANDC $rhs, $lhs), $tval, $fval)>;
-def : Pat<(v2f64 (selectcc i1:$lhs, i1:$rhs, v2f64:$tval, v2f64:$fval, SETUGT)),
           (SELECT_VSRC (CRANDC $lhs, $rhs), $tval, $fval)>;
 def : Pat<(v2f64 (selectcc i1:$lhs, i1:$rhs, v2f64:$tval, v2f64:$fval, SETNE)),
           (SELECT_VSRC (CRXOR $lhs, $rhs), $tval, $fval)>;
 
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETLT)),
-          (SELECT_VSFRC (CRANDC $lhs, $rhs), $tval, $fval)>;
-def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETULT)),
           (SELECT_VSFRC (CRANDC $rhs, $lhs), $tval, $fval)>;
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETLE)),
-          (SELECT_VSFRC (CRORC  $lhs, $rhs), $tval, $fval)>;
-def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETULE)),
           (SELECT_VSFRC (CRORC  $rhs, $lhs), $tval, $fval)>;
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETEQ)),
           (SELECT_VSFRC (CREQV $lhs, $rhs), $tval, $fval)>;
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETGE)),
-          (SELECT_VSFRC (CRORC  $rhs, $lhs), $tval, $fval)>;
-def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETUGE)),
           (SELECT_VSFRC (CRORC  $lhs, $rhs), $tval, $fval)>;
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETGT)),
-          (SELECT_VSFRC (CRANDC $rhs, $lhs), $tval, $fval)>;
-def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETUGT)),
           (SELECT_VSFRC (CRANDC $lhs, $rhs), $tval, $fval)>;
 def : Pat<(f64 (selectcc i1:$lhs, i1:$rhs, f64:$tval, f64:$fval, SETNE)),
           (SELECT_VSFRC (CRXOR $lhs, $rhs), $tval, $fval)>;
@@ -1076,27 +1060,18 @@ let AddedComplexity = 400 in { // Prefer VSX patterns over non-VSX patterns.
             (COPY_TO_REGCLASS (LXSSPX xoaddr:$src), VSFRC)>;
   def : Pat<(f64 (fextend f32:$src)),
             (COPY_TO_REGCLASS $src, VSFRC)>;
-
   def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETLT)),
-            (SELECT_VSSRC (CRANDC $lhs, $rhs), $tval, $fval)>;
-  def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETULT)),
             (SELECT_VSSRC (CRANDC $rhs, $lhs), $tval, $fval)>;
   def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETLE)),
-            (SELECT_VSSRC (CRORC  $lhs, $rhs), $tval, $fval)>;
-  def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETULE)),
             (SELECT_VSSRC (CRORC  $rhs, $lhs), $tval, $fval)>;
   def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETEQ)),
             (SELECT_VSSRC (CREQV $lhs, $rhs), $tval, $fval)>;
   def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETGE)),
-            (SELECT_VSSRC (CRORC  $rhs, $lhs), $tval, $fval)>;
-  def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETUGE)),
             (SELECT_VSSRC (CRORC  $lhs, $rhs), $tval, $fval)>;
   def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETGT)),
-            (SELECT_VSSRC (CRANDC $rhs, $lhs), $tval, $fval)>;
-  def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETUGT)),
             (SELECT_VSSRC (CRANDC $lhs, $rhs), $tval, $fval)>;
   def : Pat<(f32 (selectcc i1:$lhs, i1:$rhs, f32:$tval, f32:$fval, SETNE)),
-            (SELECT_VSSRC (CRXOR $lhs, $rhs), $tval, $fval)>;
+          (SELECT_VSSRC (CRXOR $lhs, $rhs), $tval, $fval)>;
 
   // VSX Elementary Scalar FP arithmetic (SP)
   let isCommutable = 1 in {
diff --git a/lib/Target/PowerPC/PPCVSXFMAMutate.cpp b/lib/Target/PowerPC/PPCVSXFMAMutate.cpp
index 46b8d13..58d3c3d 100644
--- a/lib/Target/PowerPC/PPCVSXFMAMutate.cpp
+++ b/lib/Target/PowerPC/PPCVSXFMAMutate.cpp
@@ -103,11 +103,6 @@ protected:
 
         VNInfo *AddendValNo =
           LIS->getInterval(MI->getOperand(1).getReg()).Query(FMAIdx).valueIn();
-        if (!AddendValNo) {
-          // This can be null if the register is undef.
-          continue;
-        }
-
         MachineInstr *AddendMI = LIS->getInstructionFromIndex(AddendValNo->def);
 
         // The addend and this instruction must be in the same block.
@@ -186,14 +181,11 @@ protected:
         if (!KilledProdOp)
           continue;
 
-	// If the addend copy is used only by this MI, then the addend source
-	// register is likely not live here. This could be fixed (based on the
-	// legality checks above, the live range for the addend source register
-	// could be extended), but it seems likely that such a trivial copy can
-	// be coalesced away later, and thus is not worth the effort.
-	if (TargetRegisterInfo::isVirtualRegister(AddendSrcReg) &&
-            !LIS->getInterval(AddendSrcReg).liveAt(FMAIdx))
-          continue;
+        // For virtual registers, verify that the addend source register
+        // is live here (as should have been assured above).
+        assert((!TargetRegisterInfo::isVirtualRegister(AddendSrcReg) ||
+                LIS->getInterval(AddendSrcReg).liveAt(FMAIdx)) &&
+               "Addend source register is not live!");
 
         // Transform: (O2 * O3) + O1 -> (O2 * O1) + O3.
 
diff --git a/lib/Target/PowerPC/PPCVSXSwapRemoval.cpp b/lib/Target/PowerPC/PPCVSXSwapRemoval.cpp
index d7132d5..3fb1dcc 100644
--- a/lib/Target/PowerPC/PPCVSXSwapRemoval.cpp
+++ b/lib/Target/PowerPC/PPCVSXSwapRemoval.cpp
@@ -240,9 +240,6 @@ bool PPCVSXSwapRemoval::gatherVectorInstructions() {
   for (MachineBasicBlock &MBB : *MF) {
     for (MachineInstr &MI : MBB) {
 
-      if (MI.isDebugValue())
-        continue;
-
       bool RelevantInstr = false;
       bool Partial = false;
 
diff --git a/lib/Target/RISCV/AsmParser/CMakeLists.txt b/lib/Target/RISCV/AsmParser/CMakeLists.txt
new file mode 100644
index 0000000..fd3ce99
--- /dev/null
+++ b/lib/Target/RISCV/AsmParser/CMakeLists.txt
@@ -0,0 +1,7 @@
+include_directories( ${CMAKE_CURRENT_BINARY_DIR}/.. ${CMAKE_CURRENT_SOURCE_DIR}/.. )
+
+add_llvm_library(LLVMRISCVAsmParser
+  RISCVAsmParser.cpp
+  )
+
+add_dependencies(LLVMRISCVAsmParser RISCVCommonTableGen)
diff --git a/lib/Target/RISCV/AsmParser/LLVMBuild.txt b/lib/Target/RISCV/AsmParser/LLVMBuild.txt
new file mode 100644
index 0000000..e9a683a
--- /dev/null
+++ b/lib/Target/RISCV/AsmParser/LLVMBuild.txt
@@ -0,0 +1,23 @@
+;===- ./lib/Target/RISCV/AsmParser/LLVMBuild.txt ---------------*- Conf -*--===;
+;
+;                     The LLVM Compiler Infrastructure
+;
+; This file is distributed under the University of Illinois Open Source
+; License. See LICENSE.TXT for details.
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[component_0]
+type = Library
+name = RISCVAsmParser
+parent = RISCV
+required_libraries = RISCVDesc RISCVInfo MC MCParser Support
+add_to_library_groups = RISCV
diff --git a/lib/Target/RISCV/AsmParser/Makefile b/lib/Target/RISCV/AsmParser/Makefile
new file mode 100644
index 0000000..69373f3
--- /dev/null
+++ b/lib/Target/RISCV/AsmParser/Makefile
@@ -0,0 +1,16 @@
+##===- lib/Target/RISCV/AsmParser/Makefile -----------------*- Makefile -*-===##
+#
+#                     The LLVM Compiler Infrastructure
+#
+# This file is distributed under the University of Illinois Open Source
+# License. See LICENSE.TXT for details.
+#
+##===----------------------------------------------------------------------===##
+
+LEVEL = ../../../..
+LIBRARYNAME = LLVMRISCVAsmParser
+
+# Hack: we need to include 'main' RISCV target directory to grab private headers
+CPP.Flags += -I$(PROJ_OBJ_DIR)/.. -I$(PROJ_SRC_DIR)/..
+
+include $(LEVEL)/Makefile.common
diff --git a/lib/Target/RISCV/AsmParser/RISCVAsmParser.cpp b/lib/Target/RISCV/AsmParser/RISCVAsmParser.cpp
new file mode 100644
index 0000000..e9ecca4
--- /dev/null
+++ b/lib/Target/RISCV/AsmParser/RISCVAsmParser.cpp
@@ -0,0 +1,853 @@
+//===-- RISCVAsmParser.cpp - Parse RISCV assembly instructions --*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "MCTargetDesc/RISCVMCTargetDesc.h"
+#include "llvm/ADT/STLExtras.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCParser/MCParsedAsmOperand.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/MC/MCSubtargetInfo.h"
+#include "llvm/MC/MCTargetAsmParser.h"
+#include "llvm/Support/TargetRegistry.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "riscv-asm-parser"
+
+// Return true if Expr is in the range [MinValue, MaxValue].
+static bool inRange(const MCExpr *Expr, int64_t MinValue, int64_t MaxValue) {
+  if (const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(Expr)) {
+    int64_t Value = CE->getValue();
+    return Value >= MinValue && Value <= MaxValue;
+  }
+  return false;
+}
+
+namespace {
+class RISCVOperand : public MCParsedAsmOperand {
+public:
+  enum RegisterKind {
+    PCReg,
+    PCRReg,
+    PCR64Reg,
+    GR32Reg,
+    GR64Reg,
+    PairGR64Reg,
+    PairGR128Reg,
+    GR128Reg,
+    FP32Reg,
+    FP64Reg,
+    PairFP64Reg,
+    PairFP128Reg,
+    FP128Reg
+  };
+
+private:
+  enum OperandKind {
+    KindToken,
+    KindReg,
+    KindImm,
+    KindMem
+  };
+
+  OperandKind Kind;
+  SMLoc StartLoc, EndLoc;
+
+  // A string of length Length, starting at Data.
+  struct TokenOp {
+    const char *Data;
+    unsigned Length;
+  };
+
+  // LLVM register Num, which has kind Kind.
+  struct RegOp {
+    RegisterKind Kind;
+    unsigned Num;
+  };
+
+  // Base + Disp + Index, where Base and Index are LLVM registers or 0.
+  // RegKind says what type the registers have
+  struct MemOp {
+    unsigned Base : 8;
+    unsigned Index : 8;
+    unsigned RegKind : 8;
+    unsigned Unused : 8;
+    const MCExpr *Disp;
+  };
+
+  union {
+    TokenOp Token;
+    RegOp Reg;
+    const MCExpr *Imm;
+    MemOp Mem;
+  };
+
+  void addExpr(MCInst &Inst, const MCExpr *Expr) const {
+    // Add as immediates when possible.  Null MCExpr = 0.
+    if (Expr == 0)
+      Inst.addOperand(MCOperand::createImm(0));
+    else if (const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(Expr))
+      Inst.addOperand(MCOperand::createImm(CE->getValue()));
+    else
+      Inst.addOperand(MCOperand::createExpr(Expr));
+  }
+
+public:
+  RISCVOperand(OperandKind kind, SMLoc startLoc, SMLoc endLoc)
+      : Kind(kind), StartLoc(startLoc), EndLoc(endLoc) {}
+
+  // Create particular kinds of operand.
+  static std::unique_ptr<RISCVOperand> createToken(StringRef Str, SMLoc Loc) {
+    auto Op = make_unique<RISCVOperand>(KindToken, Loc, Loc);
+    Op->Token.Data = Str.data();
+    Op->Token.Length = Str.size();
+    return Op;
+  }
+  static std::unique_ptr<RISCVOperand>
+  createReg(RegisterKind Kind, unsigned Num, SMLoc StartLoc, SMLoc EndLoc) {
+    auto Op = make_unique<RISCVOperand>(KindReg, StartLoc, EndLoc);
+    Op->Reg.Kind = Kind;
+    Op->Reg.Num = Num;
+    return Op;
+  }
+  static std::unique_ptr<RISCVOperand> createImm(const MCExpr *Expr,
+                                                 SMLoc StartLoc, SMLoc EndLoc) {
+    auto Op = make_unique<RISCVOperand>(KindImm, StartLoc, EndLoc);
+    Op->Imm = Expr;
+    return Op;
+  }
+  static std::unique_ptr<RISCVOperand>
+  createMem(RegisterKind RegKind, unsigned Base, const MCExpr *Disp,
+            unsigned Index, SMLoc StartLoc, SMLoc EndLoc) {
+    auto Op = make_unique<RISCVOperand>(KindMem, StartLoc, EndLoc);
+    Op->Mem.RegKind = RegKind;
+    Op->Mem.Base = Base;
+    Op->Mem.Index = Index;
+    Op->Mem.Disp = Disp;
+    return Op;
+  }
+
+  // Token operands
+  bool isToken() const override {
+    return Kind == KindToken;
+  }
+  StringRef getToken() const {
+    assert(Kind == KindToken && "Not a token");
+    return StringRef(Token.Data, Token.Length);
+  }
+
+  // Register operands.
+  bool isReg() const override {
+    return Kind == KindReg;
+  }
+  bool isReg(RegisterKind RegKind) const {
+    return Kind == KindReg && Reg.Kind == RegKind;
+  }
+  unsigned getReg() const override {
+    assert(Kind == KindReg && "Not a register");
+    return Reg.Num;
+  }
+
+  // Immediate operands.
+  bool isImm() const override {
+    return Kind == KindImm;
+  }
+  bool isImm(int64_t MinValue, int64_t MaxValue) const {
+    return Kind == KindImm && inRange(Imm, MinValue, MaxValue);
+  }
+  const MCExpr *getImm() const {
+    assert(Kind == KindImm && "Not an immediate");
+    return Imm;
+  }
+
+  // Memory operands.
+  bool isMem() const override {
+    return Kind == KindMem;
+  }
+  bool isMem(RegisterKind RegKind, bool HasIndex) const {
+    return (Kind == KindMem &&
+            Mem.RegKind == RegKind &&
+            (HasIndex || !Mem.Index));
+  }
+  bool isMemDisp12(RegisterKind RegKind, bool HasIndex) const {
+    return isMem(RegKind, HasIndex) && inRange(Mem.Disp, 0, 0xfff);
+  }
+  bool isMemDisp20(RegisterKind RegKind, bool HasIndex) const {
+    return isMem(RegKind, HasIndex) && inRange(Mem.Disp, -524288, 524287);
+  }
+
+  // Override MCParsedAsmOperand.
+  SMLoc getStartLoc() const override { return StartLoc; }
+  SMLoc getEndLoc() const override { return EndLoc; }
+  void print(raw_ostream &OS) const override;
+
+  // Used by the TableGen code to add particular types of operand
+  // to an instruction.
+  void addRegOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands");
+    Inst.addOperand(MCOperand::createReg(getReg()));
+  }
+  void addImmOperands(MCInst &Inst, unsigned N) const {
+    assert(N == 1 && "Invalid number of operands");
+    addExpr(Inst, getImm());
+  }
+
+  // Used by the TableGen code to check for particular operand types.
+  bool isPCReg() const { return isReg(PCReg); }
+  bool isPCRReg() const { return isReg(PCRReg); }
+  bool isPCR64Reg() const { return isReg(PCR64Reg); }
+  bool isGR32() const { return isReg(GR32Reg); }
+  bool isGR64() const { return isReg(GR64Reg); }
+  bool isPairGR64() const { return isReg(PairGR64Reg); }
+  bool isPairGR128() const { return isReg(PairGR128Reg); }
+  bool isGR128() const { return isReg(GR128Reg); }
+  bool isFP32() const { return isReg(FP32Reg); }
+  bool isFP64() const { return isReg(FP64Reg); }
+  bool isPairFP64() const { return isReg(PairFP64Reg); }
+  bool isPairFP128() const { return isReg(PairFP128Reg); }
+  bool isFP128() const { return isReg(FP128Reg); }
+  bool isU4Imm() const { return isImm(0, 15); }
+  bool isU12Imm() const { return isImm(0, 4096); }
+  bool isS12Imm() const { return isImm(-2048, 2047); }
+  bool isU20Imm() const { return isImm(0, 1048576); }
+  bool isS20Imm() const { return isImm(-524288, 524287); }
+  bool isU32Imm() const { return isImm(0, (1LL << 32) - 1); }
+  bool isS32Imm() const { return isImm(-(1LL << 31), (1LL << 31) - 1); }
+  bool isU64Imm() const { return isImm(0, 18446744073709551615UL); }
+  bool isS64Imm() const { return isImm(-9223372036854775807LL,9223372036854775807LL); }
+};
+
+// Maps of asm register numbers to LLVM register numbers, with 0 indicating
+// an invalid register.  We don't use register class directly because that
+// specifies the allocation order.
+static const unsigned GR32Regs[] = {
+  RISCV::zero, RISCV::ra, RISCV::sp, RISCV::gp, RISCV::tp,
+  RISCV::t0, RISCV::t1, RISCV::t2,
+  RISCV::s0, RISCV::s1,
+  RISCV::a0, RISCV::a1, RISCV::a2, RISCV::a3, RISCV::a4, RISCV::a5, RISCV::a6, RISCV::a7, 
+  RISCV::s2, RISCV::s3, RISCV::s4, RISCV::s5, RISCV::s6, RISCV::s7, RISCV::s8, RISCV::s9, RISCV::s10, RISCV::s11,
+  RISCV::t3, RISCV::t4, RISCV::t5, RISCV::t6
+};
+
+static const unsigned GR64Regs[] = {
+  RISCV::zero_64, RISCV::ra_64, RISCV::sp_64, RISCV::gp_64, RISCV::tp_64,
+  RISCV::t0_64, RISCV::t1_64, RISCV::t2_64,
+  RISCV::s0_64, RISCV::s1_64,
+  RISCV::a0_64, RISCV::a1_64, RISCV::a2_64, RISCV::a3_64, RISCV::a4_64, RISCV::a5_64, RISCV::a6_64, RISCV::a7_64, 
+  RISCV::s2_64, RISCV::s3_64, RISCV::s4_64, RISCV::s5_64, RISCV::s6_64, RISCV::s7_64, RISCV::s8_64, RISCV::s9_64, RISCV::s10_64, RISCV::s11_64,
+  RISCV::t3_64, RISCV::t4_64, RISCV::t5_64, RISCV::t6_64
+};
+
+static const unsigned PairGR64Regs[] = {
+  RISCV::a0_p64, RISCV::a1_p64, RISCV::a2_p64, RISCV::a3_p64
+};
+
+static const unsigned PairGR128Regs[] = {
+  RISCV::a0_p128, RISCV::a1_p128, RISCV::a2_p128, RISCV::a3_p128
+};
+
+static const unsigned PCReg[] = { RISCV::PC };
+
+static const unsigned FP32Regs[] = {
+  RISCV::ft0, RISCV::ft1, RISCV::ft2, RISCV::ft3, RISCV::ft4, RISCV::ft5, RISCV::ft6, RISCV::ft7,
+  RISCV::fs0, RISCV::fs1, 
+  RISCV::fa0, RISCV::fa1, RISCV::fa2, RISCV::fa3, RISCV::fa4, RISCV::fa5, RISCV::fa6, RISCV::fa7,
+  RISCV::fs2, RISCV::fs3, RISCV::fs4, RISCV::fs5, RISCV::fs6, RISCV::fs7, RISCV::fs8, RISCV::fs9, RISCV::fs10, RISCV::fs11,
+  RISCV::ft8, RISCV::ft9, RISCV::ft10, RISCV::ft11
+};
+
+static const unsigned FP64Regs[] = {
+  RISCV::ft0_64, RISCV::ft1_64, RISCV::ft2_64, RISCV::ft3_64, RISCV::ft4_64, RISCV::ft5_64, RISCV::ft6_64, RISCV::ft7_64,
+  RISCV::fs0_64, RISCV::fs1_64, 
+  RISCV::fa0_64, RISCV::fa1_64, RISCV::fa2_64, RISCV::fa3_64, RISCV::fa4_64, RISCV::fa5_64, RISCV::fa6_64, RISCV::fa7_64,
+  RISCV::fs2_64, RISCV::fs3_64, RISCV::fs4_64, RISCV::fs5_64, RISCV::fs6_64, RISCV::fs7_64, RISCV::fs8_64, RISCV::fs9_64, RISCV::fs10_64, RISCV::fs11_64,
+  RISCV::ft8_64, RISCV::ft9_64, RISCV::ft10_64, RISCV::ft11_64
+};
+
+static const unsigned PairFP64Regs[] = {
+  RISCV::fa0_p64, RISCV::fa1_p64, RISCV::fa2_p64, RISCV::fa3_p64
+};
+
+static const unsigned PairFP128Regs[] = {
+  RISCV::fa0_p128, RISCV::fa1_p128, RISCV::fa2_p128, RISCV::fa3_p128
+};
+
+static const unsigned PCRRegs[] = {
+  RISCV::status, RISCV::epc, RISCV::evec, RISCV::ptbr, RISCV::asid,
+  RISCV::count, RISCV::compare, RISCV::sup0, RISCV::sup1, RISCV::tohost, RISCV::fromhost,
+  //read only
+  RISCV::badvaddr, RISCV::cause, RISCV::hartid, RISCV::impl, 
+  //write only
+  RISCV::fatc,  RISCV::send_ipi, RISCV::clear_ipi
+};
+
+static const unsigned PCR64Regs[] = {
+  RISCV::status_64, RISCV::epc_64, RISCV::evec_64, RISCV::ptbr_64, RISCV::asid_64,
+  RISCV::count_64, RISCV::compare_64, RISCV::sup0_64, RISCV::sup1_64, RISCV::tohost_64, RISCV::fromhost_64,
+  //read only
+  RISCV::badvaddr_64, RISCV::cause_64, RISCV::hartid_64, RISCV::impl_64, 
+  //write only
+  RISCV::fatc_64,  RISCV::send_ipi_64, RISCV::clear_ipi_64
+};
+
+class RISCVAsmParser : public MCTargetAsmParser {
+#define GET_ASSEMBLER_HEADER
+#include "RISCVGenAsmMatcher.inc"
+
+private:
+  MCSubtargetInfo &STI;
+  MCAsmParser &Parser;
+  struct Register {
+    char Prefix;
+    unsigned Number;
+    SMLoc StartLoc, EndLoc;
+  };
+
+  bool parseRegister(Register &Reg);
+  bool parseParenSuffix(StringRef Name, OperandVector &Operands);
+
+  OperandMatchResultTy parseRegister(Register &Reg, char Prefix,
+                                     const unsigned *Regs,
+                                     bool IsAddress = false);
+
+  OperandMatchResultTy parseRegister(OperandVector &Operands, char Prefix,
+                                     const unsigned *Regs,
+                                     RISCVOperand::RegisterKind Kind,
+                                     bool IsAddress = false);
+
+  OperandMatchResultTy parseAddress(OperandVector &Operands,
+                                    const unsigned *Regs,
+                                    RISCVOperand::RegisterKind RegKind,
+                                    bool HasIndex);
+
+  bool parseOperand(OperandVector &Operands, StringRef Mnemonic);
+
+public:
+  RISCVAsmParser(MCSubtargetInfo &sti, MCAsmParser &parser,
+                 const MCInstrInfo &MII, const MCTargetOptions &Options)
+      : MCTargetAsmParser(), STI(sti), Parser(parser) {
+    MCAsmParserExtension::Initialize(Parser);
+
+    // Initialize the set of available features.
+    setAvailableFeatures(ComputeAvailableFeatures(STI.getFeatureBits()));
+  }
+
+  // Override MCTargetAsmParser.
+  bool ParseDirective(AsmToken DirectiveID) override;
+  bool ParseRegister(unsigned &RegNo, SMLoc &StartLoc, SMLoc &EndLoc) override;
+  bool ParseInstruction(ParseInstructionInfo &Info, StringRef Name,
+                        SMLoc NameLoc, OperandVector &Operands) override;
+  bool MatchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
+                               OperandVector &Operands, MCStreamer &Out,
+                               uint64_t &ErrorInfo,
+                               bool MatchingInlineAsm) override;
+
+  // Used by the TableGen code to parse particular operand types.
+  OperandMatchResultTy parseGR32(OperandVector &Operands) {
+    return parseRegister(Operands, 'x', GR32Regs, RISCVOperand::GR32Reg);
+  }
+
+  OperandMatchResultTy parseGR64(OperandVector &Operands) {
+    return parseRegister(Operands, 'x', GR64Regs, RISCVOperand::GR64Reg);
+  }
+
+  OperandMatchResultTy parsePairGR64(OperandVector &Operands) {
+    return parseRegister(Operands, 'x', PairGR64Regs,
+                         RISCVOperand::PairGR64Reg);
+  }
+
+  OperandMatchResultTy parsePairGR128(OperandVector &Operands) {
+    return parseRegister(Operands, 'x', PairGR128Regs,
+                         RISCVOperand::PairGR128Reg);
+  }
+
+  OperandMatchResultTy parsePCReg(OperandVector &Operands) {
+    return parseRegister(Operands, 'p', PCReg, RISCVOperand::PCReg);
+  }
+
+  OperandMatchResultTy parseFP32(OperandVector &Operands) {
+    return parseRegister(Operands, 'f', FP32Regs, RISCVOperand::FP32Reg);
+  }
+
+  OperandMatchResultTy parseFP64(OperandVector &Operands) {
+    return parseRegister(Operands, 'f', FP64Regs, RISCVOperand::FP64Reg);
+  }
+
+  OperandMatchResultTy parsePairFP64(OperandVector &Operands) {
+    return parseRegister(Operands, 'f', PairFP64Regs,
+                         RISCVOperand::PairFP64Reg);
+  }
+
+  OperandMatchResultTy parsePairFP128(OperandVector &Operands) {
+    return parseRegister(Operands, 'f', PairFP128Regs,
+                         RISCVOperand::PairFP128Reg);
+  }
+
+  OperandMatchResultTy parsePCRReg(OperandVector &Operands) {
+    const AsmToken &Tok = Parser.getTok();
+    if(Tok.is(AsmToken::Identifier) && Tok.getIdentifier().equals("ASM_CR")) {
+      SMLoc S = Tok.getLoc();
+      const AsmToken Tok = Parser.getTok();
+      if(Tok.is(AsmToken::LParen)) {
+        const AsmToken Tok = Parser.getTok();
+        if(Tok.is(AsmToken::Identifier)) {
+          std::unique_ptr<RISCVOperand> op;
+          //TODO: make this a tablegen or something
+          if(Tok.getIdentifier().equals_lower("PCR_K0"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::sup0,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_K1"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::sup1,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_EPC"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::epc,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_badvaddr"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::badvaddr,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_ptbr"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::ptbr,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_ptbr"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::ptbr,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_asid"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::asid,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_count"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::count,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_compare"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::compare,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_evec"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::evec,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_cause"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::cause,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_status"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::status,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_hartid"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::hartid,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_impl"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::impl,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_fatc"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::fatc,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_send_ipi"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::send_ipi,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_clear_ipi"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::clear_ipi,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_tohost"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::tohost,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_fromhost"))
+            op = RISCVOperand::createReg(RISCVOperand::PCRReg,RISCV::fromhost,S, Tok.getLoc());
+          else
+            return MatchOperand_ParseFail;
+
+          Operands.push_back(std::move(op));
+
+          Parser.Lex();//eat close paren
+          return MatchOperand_Success;
+        }else {
+          return MatchOperand_ParseFail;
+        }
+      }else {
+        return MatchOperand_ParseFail;
+      }
+    }else {
+      return MatchOperand_NoMatch;
+    }
+    //fallback
+    return parseRegister(Operands, 'p', PCRRegs, RISCVOperand::PCRReg);
+  }
+
+  OperandMatchResultTy
+  parsePCR64Reg(OperandVector &Operands) {
+    const AsmToken &Tok = Parser.getTok();
+    if(Tok.is(AsmToken::Identifier) && Tok.getIdentifier().equals("ASM_CR")) {
+      SMLoc S = Tok.getLoc();
+      const AsmToken Tok = Parser.getTok();
+      if(Tok.is(AsmToken::LParen)) {
+        const AsmToken Tok = Parser.getTok();
+        if(Tok.is(AsmToken::Identifier)) {
+          std::unique_ptr<RISCVOperand> op;
+          //TODO: make this a tablegen or something
+          if(Tok.getIdentifier().equals_lower("PCR_K0"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::sup0_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_K1"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::sup1_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_EPC"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::epc_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_badvaddr"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::badvaddr_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_ptbr"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::ptbr_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_ptbr"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::ptbr_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_asid"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::asid_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_count"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::count_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_compare"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::compare_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_evec"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::evec_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_cause"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::cause_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_status"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::status_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_hartid"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::hartid_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_impl"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::impl_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_fatc"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::fatc_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_send_ipi"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::send_ipi_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_clear_ipi"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::clear_ipi_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_tohost"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::tohost_64,S, Tok.getLoc());
+          else if(Tok.getIdentifier().equals_lower("PCR_fromhost"))
+            op = RISCVOperand::createReg(RISCVOperand::PCR64Reg,RISCV::fromhost_64,S, Tok.getLoc());
+          else
+            return MatchOperand_ParseFail;
+
+          Operands.push_back(std::move(op));
+
+          Parser.Lex();//eat close paren
+          return MatchOperand_Success;
+        }else {
+          return MatchOperand_ParseFail;
+        }
+      }else {
+        return MatchOperand_ParseFail;
+      }
+    }else {
+      return MatchOperand_NoMatch;
+    }
+    //fallback
+    return parseRegister(Operands, 'p', PCR64Regs, RISCVOperand::PCR64Reg);
+  }
+
+};
+}
+
+#define GET_REGISTER_MATCHER
+#define GET_SUBTARGET_FEATURE_NAME
+#define GET_MATCHER_IMPLEMENTATION
+#include "RISCVGenAsmMatcher.inc"
+
+void RISCVOperand::print(raw_ostream &OS) const {
+  llvm_unreachable("Not implemented");
+}
+
+// Parse one register of the form %<prefix><number>.
+bool RISCVAsmParser::parseRegister(Register &Reg) {
+  Reg.StartLoc = Parser.getTok().getLoc();
+
+  // Expect a register name.
+  if (Parser.getTok().isNot(AsmToken::Identifier))
+    return true;
+
+  // Check the prefix.
+  StringRef Name = Parser.getTok().getString();
+  if (Name.size() < 2)
+    return true;
+  Reg.Prefix = Name[0];
+
+  // Treat the rest of the register name as a register number.
+  if (Name.substr(1).getAsInteger(10, Reg.Number))
+    return true;
+
+  Reg.EndLoc = Parser.getTok().getLoc();
+  Parser.Lex();
+  return false;
+}
+
+/// Sometimes (i.e. load/stores) the operand may be followed immediately by
+/// either this.
+/// ::= '(', register, ')'
+/// handle it before we iterate so we don't get tripped up by the lack of
+/// a comma.
+bool RISCVAsmParser::parseParenSuffix(StringRef Name, OperandVector &Operands) {
+
+  if (getLexer().is(AsmToken::LParen)) {
+
+    Parser.Lex();
+
+    if (parseOperand(Operands, Name)) {
+      SMLoc Loc = getLexer().getLoc();
+      Parser.eatToEndOfStatement();
+      return Error(Loc, "unexpected token in argument list" );
+    }
+    if (Parser.getTok().isNot(AsmToken::RParen)) {
+      SMLoc Loc = getLexer().getLoc();
+      Parser.eatToEndOfStatement();
+      return Error(Loc, "unexpected token, expected ')'" );
+    }
+    Parser.Lex();
+  }
+
+  return false;
+}
+
+// Parse a register with prefix Prefix and convert it to LLVM numbering.
+// Regs maps asm register numbers to LLVM register numbers, with zero
+// entries indicating an invalid register.  IsAddress says whether the
+// register appears in an address context.
+RISCVAsmParser::OperandMatchResultTy
+RISCVAsmParser::parseRegister(Register &Reg, char Prefix,
+                                const unsigned *Regs, bool IsAddress) {
+  if (parseRegister(Reg))
+    return MatchOperand_NoMatch;
+  if (Reg.Prefix != Prefix || Reg.Number > 31 || Regs[Reg.Number] == 0) {
+    Error(Reg.StartLoc, "invalid register");
+    return MatchOperand_ParseFail;
+  }
+  if (Reg.Number == 0 && IsAddress) {
+    Error(Reg.StartLoc, "%r0 used in an address");
+    return MatchOperand_ParseFail;
+  }
+  Reg.Number = Regs[Reg.Number];
+  return MatchOperand_Success;
+}
+
+// Parse a register and add it to Operands.  Prefix is 'r' for GPRs,
+// 'f' for FPRs, etc.  Regs maps asm register numbers to LLVM register numbers,
+// with zero entries indicating an invalid register.  Kind is the type of
+// register represented by Regs and IsAddress says whether the register is
+// being parsed in an address context, meaning that %r0 evaluates as 0.
+RISCVAsmParser::OperandMatchResultTy
+RISCVAsmParser::parseRegister(OperandVector &Operands, char Prefix,
+                              const unsigned *Regs,
+                              RISCVOperand::RegisterKind Kind, bool IsAddress) {
+  Register Reg;
+  OperandMatchResultTy Result = parseRegister(Reg, Prefix, Regs, IsAddress);
+  if (Result == MatchOperand_Success)
+    Operands.push_back(RISCVOperand::createReg(Kind, Reg.Number,
+                                                 Reg.StartLoc, Reg.EndLoc));
+  return Result;
+}
+
+// Parse a memory operand and add it to Operands.  Regs maps asm register
+// numbers to LLVM address registers and RegKind says what kind of address
+// register we're using (GR32Reg or GR64Reg).  HasIndex says whether
+// the address allows index registers.
+RISCVAsmParser::OperandMatchResultTy
+RISCVAsmParser::parseAddress(OperandVector &Operands, const unsigned *Regs,
+                             RISCVOperand::RegisterKind RegKind,
+                             bool HasIndex) {
+  SMLoc StartLoc = Parser.getTok().getLoc();
+
+  // Parse the displacement, which must always be present.
+  const MCExpr *Disp;
+  if (getParser().parseExpression(Disp))
+    return MatchOperand_NoMatch;
+
+  // Parse the optional base and index.
+  unsigned Index = 0;
+  unsigned Base = 0;
+  if (getLexer().is(AsmToken::LParen)) {
+    Parser.Lex();
+
+    // Parse the first register.
+    Register Reg;
+    OperandMatchResultTy Result = parseRegister(Reg, 'x', GR32Regs, true);
+    if (Result != MatchOperand_Success)
+      return Result;
+
+    // Check whether there's a second register.  If so, the one that we
+    // just parsed was the index.
+    if (getLexer().is(AsmToken::Comma)) {
+      Parser.Lex();
+
+      if (!HasIndex) {
+        Error(Reg.StartLoc, "invalid use of indexed addressing");
+        return MatchOperand_ParseFail;
+      }
+
+      Index = Reg.Number;
+      Result = parseRegister(Reg, 'x', GR32Regs, true);
+      if (Result != MatchOperand_Success)
+        return Result;
+    }
+    Base = Reg.Number;
+
+    // Consume the closing bracket.
+    if (getLexer().isNot(AsmToken::RParen))
+      return MatchOperand_NoMatch;
+    Parser.Lex();
+  }
+
+  SMLoc EndLoc =
+    SMLoc::getFromPointer(Parser.getTok().getLoc().getPointer() - 1);
+  Operands.push_back(RISCVOperand::createMem(RegKind, Base, Disp, Index,
+                                               StartLoc, EndLoc));
+  return MatchOperand_Success;
+}
+
+bool RISCVAsmParser::ParseDirective(AsmToken DirectiveID) {
+  return true;
+}
+
+bool RISCVAsmParser::ParseRegister(unsigned &RegNo, SMLoc &StartLoc,
+                                     SMLoc &EndLoc) {
+  Register Reg;
+  if (parseRegister(Reg))
+    return Error(Reg.StartLoc, "register expected");
+  if (Reg.Prefix == 'x' && Reg.Number < 32)
+    RegNo = GR32Regs[Reg.Number];
+  else if (Reg.Prefix == 'f' && Reg.Number < 32)
+    RegNo = FP32Regs[Reg.Number];
+  else
+    return Error(Reg.StartLoc, "invalid register");
+  StartLoc = Reg.StartLoc;
+  EndLoc = Reg.EndLoc;
+  return false;
+}
+
+bool RISCVAsmParser::ParseInstruction(ParseInstructionInfo &Info,
+                                      StringRef Name, SMLoc NameLoc,
+                                      OperandVector &Operands) {
+
+  // Check if we have valid mnemonic
+  if (!mnemonicIsValid(Name, 0)) {
+    Parser.eatToEndOfStatement();
+    return Error(NameLoc, "unknown instruction");
+  }
+
+  // First operand in MCInst is instruction mnemonic.
+  Operands.push_back(RISCVOperand::createToken(Name, NameLoc));
+
+  // Read the remaining operands
+  if (getLexer().isNot(AsmToken::EndOfStatement)) {
+
+    // Read the first operand
+    if( parseOperand(Operands,Name)) {
+      SMLoc Loc = getLexer().getLoc();
+      Parser.eatToEndOfStatement();
+      return Error(Loc, "unexpected token in argument list");
+    }
+
+    if( getLexer().is(AsmToken::LBrac) )
+      return true;
+
+    while (getLexer().is(AsmToken::Comma)) {
+
+      Parser.Lex(); // Eat the comma.
+
+      // Parse and remember the operand
+      if( parseOperand(Operands, Name )){
+        SMLoc Loc = getLexer().getLoc();
+        Parser.eatToEndOfStatement();
+        return Error(Loc, "unexpected token in argument list");
+      }
+
+      // Parse parenthesis suffixes before we iterate
+      if( getLexer().is(AsmToken::LParen) &&
+                 parseParenSuffix(Name,Operands))
+        return true;
+    }
+  }
+
+  if( getLexer().isNot(AsmToken::EndOfStatement)) {
+
+    SMLoc Loc = getLexer().getLoc();
+    Parser.eatToEndOfStatement();
+    return Error(Loc, "unexpected token in argument list" );
+  }
+  Parser.Lex(); // Consume the EndOfStatement.
+  return false;
+
+}
+
+bool RISCVAsmParser::parseOperand(OperandVector &Operands, StringRef Mnemonic) {
+  // Check if the current operand has a custom associated parser, if so, try to
+  // custom parse the operand, or fallback to the general approach.
+  OperandMatchResultTy ResTy = MatchOperandParserImpl(Operands, Mnemonic);
+  if (ResTy == MatchOperand_Success)
+    return false;
+
+  // If there wasn't a custom match, try the generic matcher below. Otherwise,
+  // there was a match, but an error occurred, in which case, just return that
+  // the operand parsing failed.
+  if (ResTy == MatchOperand_ParseFail)
+    return true;
+
+  // The only other type of operand is an immediate.
+  const MCExpr *Expr;
+  SMLoc StartLoc = Parser.getTok().getLoc();
+  if (getParser().parseExpression(Expr))
+    return true;
+
+  SMLoc EndLoc =
+    SMLoc::getFromPointer(Parser.getTok().getLoc().getPointer() - 1);
+  Operands.push_back(RISCVOperand::createImm(Expr, StartLoc, EndLoc));
+  return false;
+}
+
+bool RISCVAsmParser::MatchAndEmitInstruction(SMLoc IDLoc, unsigned &Opcode,
+                                             OperandVector &Operands,
+                                             MCStreamer &Out,
+                                             uint64_t &ErrorInfo,
+                                             bool MatchingInlineAsm) {
+  MCInst Inst;
+  unsigned MatchResult;
+
+  MatchResult = MatchInstructionImpl(Operands, Inst, ErrorInfo,
+                                     MatchingInlineAsm);
+  switch (MatchResult) {
+  default: break;
+  case Match_Success:{
+#if 0
+    if(processInstruction(Inst,IDLoc, Instructions))
+      return true;
+    for(unsigned i=0; i< Instructions.size(); i++ ){ 
+      Out.EmitInstruction(Inst,STI);
+    }
+    return false;
+  }
+#endif
+    Inst.setLoc(IDLoc);
+    Out.EmitInstruction(Inst, STI);
+    return false;
+  }
+  case Match_MissingFeature: {
+    assert(ErrorInfo && "Unknown missing feature!");
+    // Special case the error message for the very common case where only
+    // a single subtarget feature is missing
+    std::string Msg = "instruction requires:";
+    unsigned Mask = 1;
+    for (unsigned I = 0; I < sizeof(ErrorInfo) * 8 - 1; ++I) {
+      if (ErrorInfo & Mask) {
+        Msg += " ";
+        Msg += getSubtargetFeatureName(ErrorInfo & Mask);
+      }
+      Mask <<= 1;
+    }
+    return Error(IDLoc, Msg);
+  }
+
+  case Match_InvalidOperand: {
+    SMLoc ErrorLoc = IDLoc;
+    if (ErrorInfo != ~0U) {
+      if (ErrorInfo >= Operands.size())
+        return Error(IDLoc, "too few operands for instruction");
+
+      ErrorLoc = ((RISCVOperand &)*Operands[ErrorInfo]).getStartLoc();
+      if (ErrorLoc == SMLoc())
+        ErrorLoc = IDLoc;
+    }
+    return Error(ErrorLoc, "invalid operand for instruction");
+  }
+
+  case Match_MnemonicFail:
+    return Error(IDLoc, "invalid instruction");
+  }
+
+  llvm_unreachable("Unexpected match type");
+}
+
+// Force static initialization.
+extern "C" void LLVMInitializeRISCVAsmParser() {
+  RegisterMCAsmParser<RISCVAsmParser> X(TheRISCVTarget);
+  RegisterMCAsmParser<RISCVAsmParser> Y(TheRISCV64Target);
+}
diff --git a/lib/Target/RISCV/CMakeLists.txt b/lib/Target/RISCV/CMakeLists.txt
new file mode 100644
index 0000000..c7fcb5b
--- /dev/null
+++ b/lib/Target/RISCV/CMakeLists.txt
@@ -0,0 +1,34 @@
+set(LLVM_TARGET_DEFINITIONS RISCV.td)
+
+tablegen(LLVM RISCVGenAsmMatcher.inc -gen-asm-matcher)
+tablegen(LLVM RISCVGenAsmWriter.inc -gen-asm-writer)
+tablegen(LLVM RISCVGenCallingConv.inc -gen-callingconv)
+tablegen(LLVM RISCVGenDAGISel.inc -gen-dag-isel)
+tablegen(LLVM RISCVGenMCCodeEmitter.inc -gen-emitter)
+tablegen(LLVM RISCVGenInstrInfo.inc -gen-instr-info)
+tablegen(LLVM RISCVGenRegisterInfo.inc -gen-register-info)
+tablegen(LLVM RISCVGenSubtargetInfo.inc -gen-subtarget)
+add_public_tablegen_target(RISCVCommonTableGen)
+
+add_llvm_target(RISCVCodeGen
+  RISCVAsmPrinter.cpp
+  RISCVBranchSelector.cpp
+  RISCVConstantPoolValue.cpp
+  RISCVFrameLowering.cpp
+  RISCVInstrInfo.cpp
+  RISCVISelDAGToDAG.cpp
+  RISCVISelLowering.cpp
+  RISCVMachineFunctionInfo.cpp
+  RISCVMCInstLower.cpp
+  RISCVRegisterInfo.cpp
+  RISCVSubtarget.cpp
+  RISCVTargetMachine.cpp
+  RISCVMachineFunctionInfo.cpp
+  )
+
+add_dependencies(LLVMRISCVCodeGen intrinsics_gen)
+
+add_subdirectory(AsmParser)
+add_subdirectory(InstPrinter)
+add_subdirectory(TargetInfo)
+add_subdirectory(MCTargetDesc)
diff --git a/lib/Target/RISCV/InstPrinter/CMakeLists.txt b/lib/Target/RISCV/InstPrinter/CMakeLists.txt
new file mode 100644
index 0000000..e710e9c
--- /dev/null
+++ b/lib/Target/RISCV/InstPrinter/CMakeLists.txt
@@ -0,0 +1,7 @@
+include_directories( ${CMAKE_CURRENT_BINARY_DIR}/.. ${CMAKE_CURRENT_SOURCE_DIR}/.. )
+
+add_llvm_library(LLVMRISCVAsmPrinter
+  RISCVInstPrinter.cpp
+  )
+
+add_dependencies(LLVMRISCVAsmPrinter RISCVCommonTableGen)
diff --git a/lib/Target/RISCV/InstPrinter/LLVMBuild.txt b/lib/Target/RISCV/InstPrinter/LLVMBuild.txt
new file mode 100644
index 0000000..5f4545e
--- /dev/null
+++ b/lib/Target/RISCV/InstPrinter/LLVMBuild.txt
@@ -0,0 +1,23 @@
+;===- ./lib/Target/RISCV/InstPrinter/LLVMBuild.txt -------------*- Conf -*--===;
+;
+;                     The LLVM Compiler Infrastructure
+;
+; This file is distributed under the University of Illinois Open Source
+; License. See LICENSE.TXT for details.
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[component_0]
+type = Library
+name = RISCVAsmPrinter
+parent = RISCV
+required_libraries = MC Support
+add_to_library_groups = RISCV
diff --git a/lib/Target/RISCV/InstPrinter/Makefile b/lib/Target/RISCV/InstPrinter/Makefile
new file mode 100644
index 0000000..8cde5cc
--- /dev/null
+++ b/lib/Target/RISCV/InstPrinter/Makefile
@@ -0,0 +1,16 @@
+##===- lib/Target/RISCV/InstPrinter/Makefile ---------------*- Makefile -*-===##
+#
+#                     The LLVM Compiler Infrastructure
+#
+# This file is distributed under the University of Illinois Open Source
+# License. See LICENSE.TXT for details.
+#
+##===----------------------------------------------------------------------===##
+
+LEVEL = ../../../..
+LIBRARYNAME = LLVMRISCVAsmPrinter
+
+# Hack: we need to include 'main' mips target directory to grab private headers
+CPP.Flags += -I$(PROJ_OBJ_DIR)/.. -I$(PROJ_SRC_DIR)/..
+
+include $(LEVEL)/Makefile.common
diff --git a/lib/Target/RISCV/InstPrinter/RISCVInstPrinter.cpp b/lib/Target/RISCV/InstPrinter/RISCVInstPrinter.cpp
new file mode 100644
index 0000000..ac0a3c6
--- /dev/null
+++ b/lib/Target/RISCV/InstPrinter/RISCVInstPrinter.cpp
@@ -0,0 +1,241 @@
+//===-- RISCVInstPrinter.cpp - Convert RISCV MCInst to assembly syntax ----===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#define DEBUG_TYPE "asm-printer"
+
+#include "RISCVInstPrinter.h"
+#include "RISCVInstrInfo.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCInstrInfo.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/CodeGen/MachineOperand.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#include "RISCVGenAsmWriter.inc"
+
+void RISCVInstPrinter::printAddress(unsigned Base, int64_t Disp,
+                                      raw_ostream &O) {
+  O << Disp;
+  if (Base) {
+    O << '(';
+    O << getRegisterName(Base) << ')';
+  }
+}
+
+static void printExpr(const MCExpr *Expr, raw_ostream &OS) {
+  int Offset = 0;
+  const MCSymbolRefExpr *SRE;
+  
+  if (const MCBinaryExpr *BE = dyn_cast<MCBinaryExpr>(Expr)) {
+    SRE = dyn_cast<MCSymbolRefExpr>(BE->getLHS());
+    const MCConstantExpr *CE = dyn_cast<MCConstantExpr>(BE->getRHS());
+    assert(SRE && CE && "Binary expression must be sym+const.");
+    Offset = CE->getValue();
+  }
+  else if (!(SRE = dyn_cast<MCSymbolRefExpr>(Expr)))
+    assert(false && "Unexpected MCExpr type.");
+ 
+  MCSymbolRefExpr::VariantKind Kind = SRE->getKind();
+    
+  switch (Kind) {
+  default:                                 llvm_unreachable("Invalid kind!");
+  case MCSymbolRefExpr::VK_None:           break;
+  case MCSymbolRefExpr::VK_Mips_ABS_HI:    OS << "%hi(";     break;
+  case MCSymbolRefExpr::VK_Mips_ABS_LO:    OS << "%lo(";     break;
+  case MCSymbolRefExpr::VK_Mips_TPREL_HI:    OS << "%tprel_hi(";     break;
+  case MCSymbolRefExpr::VK_Mips_TPREL_LO:    OS << "%tprel_lo(";     break;
+  }
+
+  OS << SRE->getSymbol();
+      
+  if (Offset) {
+    if (Offset > 0)
+      OS << '+';
+    OS << Offset; 
+  }                   
+                          
+  if (Kind != MCSymbolRefExpr::VK_None)
+    OS << ')';
+}
+
+void RISCVInstPrinter::printOperand(const MCOperand &MC, raw_ostream &O) {
+  if (MC.isReg())
+    O << getRegisterName(MC.getReg());
+  else if (MC.isImm())
+    O << MC.getImm();
+  else if (MC.isExpr())
+    printExpr(MC.getExpr(), O);
+  else
+    llvm_unreachable("Invalid operand");
+}
+
+void RISCVInstPrinter::printInst(const MCInst *MI, raw_ostream &O,
+                                   StringRef Annot, const MCSubtargetInfo &STI) {
+  printInstruction(MI, O);
+  printAnnotation(O, Annot);
+}
+
+void RISCVInstPrinter::printRegName(raw_ostream &O, unsigned RegNo) const {
+  O << getRegisterName(RegNo);
+}
+
+void RISCVInstPrinter::printMemOperand(const MCInst *MI, int opNum, 
+                                         raw_ostream &OS) {
+     printOperand(MI, opNum, OS);
+     OS << "(";
+     OS << getRegisterName(MI->getOperand(opNum+1).getReg());
+     OS << ")";
+}
+
+void RISCVInstPrinter::printJALRMemOperand(const MCInst *MI, int opNum, 
+                                         raw_ostream &OS) {
+     OS << getRegisterName(MI->getOperand(opNum+1).getReg());
+     OS << ", ";
+     printOperand(MI, opNum, OS);
+}
+
+void RISCVInstPrinter::printBranchTarget(const MCInst *MI, int opNum, 
+                                         raw_ostream &OS) {
+    if(MI->getOperand(opNum).isImm()){
+      OS << ".+";//constant branch
+    }
+    printOperand(MI, opNum, OS);
+}
+
+void RISCVInstPrinter::printMemRegOperand(const MCInst *MI, int opNum, 
+                                         raw_ostream &OS) {
+     OS << "0"; //No offset for this ever
+     OS << "(";
+     OS << getRegisterName(MI->getOperand(opNum).getReg());
+     OS << ")";
+}
+
+void RISCVInstPrinter::printS12ImmOperand(const MCInst *MI, int OpNum,
+                                           raw_ostream &O) {
+  if(MI->getOperand(OpNum).isImm()){
+    int64_t Value = MI->getOperand(OpNum).getImm();
+    assert(isInt<12>(Value) && "Invalid s12imm argument");
+    O << Value;
+  }else
+    printOperand(MI, OpNum, O);
+}
+
+void RISCVInstPrinter::printU12ImmOperand(const MCInst *MI, int OpNum,
+                                           raw_ostream &O) {
+  if(MI->getOperand(OpNum).isImm()){
+    int64_t Value = MI->getOperand(OpNum).getImm();
+    assert(isUInt<12>(Value) && "Invalid u12imm argument");
+    O << Value;
+  }else
+    printOperand(MI, OpNum, O);
+}
+
+void RISCVInstPrinter::printS20ImmOperand(const MCInst *MI, int OpNum,
+                                           raw_ostream &O) {
+  if(MI->getOperand(OpNum).isImm()){
+    int64_t Value = MI->getOperand(OpNum).getImm();
+    assert(isInt<20>(Value) && "Invalid s20imm argument");
+    O << Value;
+  }else
+    printOperand(MI, OpNum, O);
+}
+
+void RISCVInstPrinter::printU20ImmOperand(const MCInst *MI, int OpNum,
+                                           raw_ostream &O) {
+  if(MI->getOperand(OpNum).isImm()){
+    int64_t Value = MI->getOperand(OpNum).getImm();
+    assert(isUInt<20>(Value) && "Invalid u20imm argument");
+    O << Value;
+  }else
+    printOperand(MI, OpNum, O);
+}
+
+void RISCVInstPrinter::printS32ImmOperand(const MCInst *MI, int OpNum,
+                                            raw_ostream &O) {
+  if(MI->getOperand(OpNum).isImm()){
+    int64_t Value = MI->getOperand(OpNum).getImm();
+    assert(isInt<32>(Value) && "Invalid s32imm argument");
+    O << Value;
+  }else
+    printOperand(MI, OpNum, O);
+}
+
+void RISCVInstPrinter::printU32ImmOperand(const MCInst *MI, int OpNum,
+                                            raw_ostream &O) {
+  if(MI->getOperand(OpNum).isImm()){
+    int64_t Value = MI->getOperand(OpNum).getImm();
+    assert(isUInt<32>(Value) && "Invalid u32imm argument");
+    O << Value;
+  }else
+    printOperand(MI, OpNum, O);
+}
+
+void RISCVInstPrinter::printS64ImmOperand(const MCInst *MI, int OpNum,
+                                            raw_ostream &O) {
+  if(MI->getOperand(OpNum).isImm()){
+    int64_t Value = MI->getOperand(OpNum).getImm();
+    assert(isInt<64>(Value) && "Invalid s64imm argument");
+    O << Value;
+  }else
+    printOperand(MI, OpNum, O);
+}
+
+void RISCVInstPrinter::printU64ImmOperand(const MCInst *MI, int OpNum,
+                                            raw_ostream &O) {
+  if(MI->getOperand(OpNum).isImm()){
+    int64_t Value = MI->getOperand(OpNum).getImm();
+    assert(isUInt<64>(Value) && "Invalid u64imm argument");
+    O << Value;
+  }else
+    printOperand(MI, OpNum, O);
+}
+
+void RISCVInstPrinter::printAccessRegOperand(const MCInst *MI, int OpNum,
+                                               raw_ostream &O) {
+  uint64_t Value = MI->getOperand(OpNum).getImm();
+  assert(Value < 16 && "Invalid access register number");
+  O << "%a" << (unsigned int)Value;
+}
+
+void RISCVInstPrinter::printCallOperand(const MCInst *MI, int OpNum,
+                                          raw_ostream &O) {
+  printOperand(MI, OpNum, O);
+  //O << "@PLT";
+}
+
+void RISCVInstPrinter::printOperand(const MCInst *MI, int OpNum,
+                                      raw_ostream &O) {
+  printOperand(MI->getOperand(OpNum), O);
+}
+
+void RISCVInstPrinter::printBDAddrOperand(const MCInst *MI, int OpNum,
+                                            raw_ostream &O) {
+  printAddress(MI->getOperand(OpNum).getReg(),
+               MI->getOperand(OpNum + 1).getImm(), O);
+}
+
+void RISCVInstPrinter::printBDXAddrOperand(const MCInst *MI, int OpNum,
+                                             raw_ostream &O) {
+  printAddress(MI->getOperand(OpNum).getReg(),
+               MI->getOperand(OpNum + 1).getImm(),
+               O);
+}
+
+void RISCVInstPrinter::printCond4Operand(const MCInst *MI, int OpNum,
+                                           raw_ostream &O) {
+  static const char *const CondNames[] = {
+    "o", "h", "nle", "l", "nhe", "lh", "ne",
+    "e", "nlh", "he", "nl", "le", "nh", "no"
+  };
+  uint64_t Imm = MI->getOperand(OpNum).getImm();
+  assert(Imm > 0 && Imm < 15 && "Invalid condition");
+  O << CondNames[Imm - 1];
+}
diff --git a/lib/Target/RISCV/InstPrinter/RISCVInstPrinter.h b/lib/Target/RISCV/InstPrinter/RISCVInstPrinter.h
new file mode 100644
index 0000000..2e74b54
--- /dev/null
+++ b/lib/Target/RISCV/InstPrinter/RISCVInstPrinter.h
@@ -0,0 +1,71 @@
+//===- RISCVInstPrinter.h - Convert RISCV MCInst to assembly ----*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This class prints a RISCV MCInst to a .s file.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVINSTPRINTER_H
+#define LLVM_LIB_TARGET_RISCV_RISCVINSTPRINTER_H
+
+#include "llvm/MC/MCInstPrinter.h"
+#include "llvm/Support/Compiler.h"
+
+namespace llvm {
+class MCOperand;
+
+class RISCVInstPrinter : public MCInstPrinter {
+public:
+  RISCVInstPrinter(const MCAsmInfo &MAI, const MCInstrInfo &MII,
+                     const MCRegisterInfo &MRI)
+    : MCInstPrinter(MAI, MII, MRI) {}
+
+  // Automatically generated by tblgen.
+  void printInstruction(const MCInst *MI, raw_ostream &O);
+  static const char *getRegisterName(unsigned RegNo);
+
+  // Print an address with the given base, displacement and index.
+  static void printAddress(unsigned Base, int64_t Disp,
+                           raw_ostream &O);
+
+  // Print the given operand.
+  static void printOperand(const MCOperand &MO, raw_ostream &O);
+
+  // Override MCInstPrinter.
+  void printRegName(raw_ostream &O, unsigned RegNo) const override;
+  void printInst(const MCInst *MI, raw_ostream &O, StringRef Annot,
+      const MCSubtargetInfo &STI) override;
+
+private:
+  // Print various types of operand.
+  void printOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printBranchTarget(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printBDAddrOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printBDXAddrOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printS12ImmOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printU12ImmOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printS20ImmOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printU20ImmOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printMemOperand(const MCInst *MI, int OpNUm, raw_ostream &O);
+  void printJALRMemOperand(const MCInst *MI, int OpNUm, raw_ostream &O);
+  void printMemRegOperand(const MCInst *MI, int OpNUm, raw_ostream &O);
+  void printS32ImmOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printU32ImmOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printS64ImmOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printU64ImmOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printCallOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+  void printAccessRegOperand(const MCInst *MI, int OpNum, raw_ostream &O);
+
+  // Print the mnemonic for a condition-code mask ("ne", "lh", etc.)
+  // This forms part of the instruction name rather than the operand list.
+  void printCond4Operand(const MCInst *MI, int OpNum, raw_ostream &O);
+};
+} // end namespace llvm
+
+#endif
diff --git a/lib/Target/RISCV/LLVMBuild.txt b/lib/Target/RISCV/LLVMBuild.txt
new file mode 100644
index 0000000..404e76b
--- /dev/null
+++ b/lib/Target/RISCV/LLVMBuild.txt
@@ -0,0 +1,34 @@
+;===- ./lib/Target/RISCV/LLVMBuild.txt -------------------------*- Conf -*--===;
+;
+;                     The LLVM Compiler Infrastructure
+;
+; This file is distributed under the University of Illinois Open Source
+; License. See LICENSE.TXT for details.
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[common]
+subdirectories = AsmParser InstPrinter MCTargetDesc TargetInfo
+
+[component_0]
+type = TargetGroup
+name = RISCV
+parent = Target
+has_asmparser = 1
+has_asmprinter = 1
+has_jit = 1
+
+[component_1]
+type = Library
+name = RISCVCodeGen
+parent = RISCV
+required_libraries = AsmPrinter CodeGen Core MC SelectionDAG RISCVDesc RISCVInfo Support Target
+add_to_library_groups = RISCV
diff --git a/lib/Target/RISCV/MCTargetDesc/CMakeLists.txt b/lib/Target/RISCV/MCTargetDesc/CMakeLists.txt
new file mode 100644
index 0000000..bc935a3
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/CMakeLists.txt
@@ -0,0 +1,9 @@
+add_llvm_library(LLVMRISCVDesc
+  RISCVMCAsmBackend.cpp
+  RISCVMCAsmInfo.cpp
+  RISCVMCCodeEmitter.cpp
+  RISCVMCObjectWriter.cpp
+  RISCVMCTargetDesc.cpp
+  )
+
+add_dependencies(LLVMRISCVDesc RISCVCommonTableGen)
diff --git a/lib/Target/RISCV/MCTargetDesc/LLVMBuild.txt b/lib/Target/RISCV/MCTargetDesc/LLVMBuild.txt
new file mode 100644
index 0000000..92daae3
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/LLVMBuild.txt
@@ -0,0 +1,23 @@
+;===- ./lib/Target/RISCV/MCTargetDesc/LLVMBuild.txt ------------*- Conf -*--===;
+;
+;                     The LLVM Compiler Infrastructure
+;
+; This file is distributed under the University of Illinois Open Source
+; License. See LICENSE.TXT for details.
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[component_0]
+type = Library
+name = RISCVDesc
+parent = RISCV
+required_libraries = MC RISCVAsmPrinter RISCVInfo Support
+add_to_library_groups = RISCV
diff --git a/lib/Target/RISCV/MCTargetDesc/Makefile b/lib/Target/RISCV/MCTargetDesc/Makefile
new file mode 100644
index 0000000..8052779
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/Makefile
@@ -0,0 +1,16 @@
+##===- lib/Target/RISCV/TargetDesc/Makefile ----------------*- Makefile -*-===##
+#
+#                     The LLVM Compiler Infrastructure
+#
+# This file is distributed under the University of Illinois Open Source
+# License. See LICENSE.TXT for details.
+#
+##===----------------------------------------------------------------------===##
+
+LEVEL = ../../../..
+LIBRARYNAME = LLVMRISCVDesc
+
+# Hack: we need to include 'main' target directory to grab private headers
+CPP.Flags += -I$(PROJ_OBJ_DIR)/.. -I$(PROJ_SRC_DIR)/..
+
+include $(LEVEL)/Makefile.common
diff --git a/lib/Target/RISCV/MCTargetDesc/RISCVMCAsmBackend.cpp b/lib/Target/RISCV/MCTargetDesc/RISCVMCAsmBackend.cpp
new file mode 100644
index 0000000..413e8ea
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/RISCVMCAsmBackend.cpp
@@ -0,0 +1,142 @@
+//===-- RISCVMCAsmBackend.cpp - RISCV assembler backend ---------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "MCTargetDesc/RISCVMCTargetDesc.h"
+#include "MCTargetDesc/RISCVMCFixups.h"
+#include "llvm/MC/MCAsmBackend.h"
+#include "llvm/MC/MCELFObjectWriter.h"
+#include "llvm/MC/MCFixupKindInfo.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCObjectWriter.h"
+
+using namespace llvm;
+
+// Value is a fully-resolved relocation value: Symbol + Addend [- Pivot].
+// Return the bits that should be installed in a relocation field for
+// fixup kind Kind.
+static uint64_t extractBitsForFixup(MCFixupKind Kind, uint64_t Value) {
+  if (Kind < FirstTargetFixupKind)
+    return Value;
+
+  switch (unsigned(Kind)) {
+  case RISCV::fixup_riscv_brlo:
+  case RISCV::fixup_riscv_brhi:
+  case RISCV::fixup_riscv_jal:
+    return (int64_t)Value / 2;
+  }
+
+  llvm_unreachable("Unknown fixup kind!");
+}
+
+// If Opcode can be relaxed, return the relaxed form, otherwise return 0.
+static unsigned getRelaxedOpcode(unsigned Opcode) {
+  switch (Opcode) {
+  //case RISCV::BRC:  return RISCV::BRCL;
+  //case RISCV::J:    return RISCV::JG;
+  //case RISCV::BRAS: return RISCV::BRASL;
+  }
+  return 0;
+}
+
+namespace {
+class RISCVMCAsmBackend : public MCAsmBackend {
+  uint8_t OSABI;
+public:
+  RISCVMCAsmBackend(uint8_t osABI)
+    : OSABI(osABI) {}
+
+  // Override MCAsmBackend
+  unsigned getNumFixupKinds() const override {
+    return RISCV::NumTargetFixupKinds;
+  }
+  const MCFixupKindInfo &getFixupKindInfo(MCFixupKind Kind) const override;
+  void applyFixup(const MCFixup &Fixup, char *Data, unsigned DataSize,
+                  uint64_t Value, bool IsPCRel) const override;
+  bool mayNeedRelaxation(const MCInst &Inst) const override;
+  bool fixupNeedsRelaxation(const MCFixup &Fixup, uint64_t Value,
+                            const MCRelaxableFragment *Fragment,
+                            const MCAsmLayout &Layout) const override;
+  void relaxInstruction(const MCInst &Inst, MCInst &Res) const override;
+  bool writeNopData(uint64_t Count, MCObjectWriter *OW) const override;
+  MCObjectWriter *createObjectWriter(raw_pwrite_stream &OS) const override {
+    return createRISCVObjectWriter(OS, OSABI);
+  }
+};
+} // end anonymous namespace
+
+const MCFixupKindInfo &
+RISCVMCAsmBackend::getFixupKindInfo(MCFixupKind Kind) const {
+  const static MCFixupKindInfo Infos[RISCV::NumTargetFixupKinds] = {
+    { "fixup_riscv_brlo",  10, 7, MCFixupKindInfo::FKF_IsPCRel },
+    { "fixup_riscv_brhi",  27, 5, MCFixupKindInfo::FKF_IsPCRel },
+    { "fixup_riscv_jal", 0, 20, MCFixupKindInfo::FKF_IsPCRel },
+    // target offset(0) doesn't make sense here: bits are non continuous
+  };
+
+  if (Kind < FirstTargetFixupKind)
+    return MCAsmBackend::getFixupKindInfo(Kind);
+
+  assert(unsigned(Kind - FirstTargetFixupKind) < getNumFixupKinds() &&
+         "Invalid kind!");
+  return Infos[Kind - FirstTargetFixupKind];
+}
+
+void RISCVMCAsmBackend::applyFixup(const MCFixup &Fixup, char *Data,
+                                   unsigned DataSize, uint64_t Value,
+                                   bool IsPCRel) const {
+  MCFixupKind Kind = Fixup.getKind();
+  unsigned Offset = Fixup.getOffset();
+  unsigned Size = (getFixupKindInfo(Kind).TargetSize + 7) / 8;
+
+  assert(Offset + Size <= DataSize && "Invalid fixup offset!");
+
+  // Big-endian insertion of Size bytes.
+  Value = extractBitsForFixup(Kind, Value);
+  unsigned ShiftValue = (Size * 8) - 8;
+  for (unsigned I = 0; I != Size; ++I) {
+    Data[Offset + I] |= uint8_t(Value >> ShiftValue);
+    ShiftValue -= 8;
+  }
+}
+
+bool RISCVMCAsmBackend::mayNeedRelaxation(const MCInst &Inst) const {
+  return getRelaxedOpcode(Inst.getOpcode()) != 0;
+}
+
+bool
+RISCVMCAsmBackend::fixupNeedsRelaxation(const MCFixup &Fixup,
+                                          uint64_t Value,
+                                          const MCRelaxableFragment *Fragment,
+                                          const MCAsmLayout &Layout) const {
+  // At the moment we just need to relax 16-bit fields to wider fields.
+  Value = extractBitsForFixup(Fixup.getKind(), Value);
+  return (int16_t)Value != (int64_t)Value;
+}
+
+void RISCVMCAsmBackend::relaxInstruction(const MCInst &Inst,
+                                           MCInst &Res) const {
+  unsigned Opcode = getRelaxedOpcode(Inst.getOpcode());
+  assert(Opcode && "Unexpected insn to relax");
+  Res = Inst;
+  Res.setOpcode(Opcode);
+}
+
+bool RISCVMCAsmBackend::writeNopData(uint64_t Count,
+                                       MCObjectWriter *OW) const {
+  for (uint64_t I = 0; I != Count; ++I)
+    OW->write8(7);
+  return true;
+}
+
+MCAsmBackend *llvm::createRISCVMCAsmBackend(const Target &T,
+                                            const MCRegisterInfo &MRI,
+                                            const Triple &TT, StringRef CPU) {
+  uint8_t OSABI = MCELFObjectTargetWriter::getOSABI(TT.getOS());
+  return new RISCVMCAsmBackend(OSABI);
+}
diff --git a/lib/Target/RISCV/MCTargetDesc/RISCVMCAsmInfo.cpp b/lib/Target/RISCV/MCTargetDesc/RISCVMCAsmInfo.cpp
new file mode 100644
index 0000000..3647a8f
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/RISCVMCAsmInfo.cpp
@@ -0,0 +1,28 @@
+//===-- RISCVMCAsmInfo.cpp - RISCV asm properties ---------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVMCAsmInfo.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCSectionELF.h"
+
+using namespace llvm;
+
+RISCVMCAsmInfo::RISCVMCAsmInfo() {
+  PointerSize = 8;
+  CalleeSaveStackSlotSize = 8;
+  IsLittleEndian = true;
+
+  CommentString = "#";
+  ZeroDirective = "\t.space\t";
+  Data64bitsDirective = "\t.quad\t";
+  UsesELFSectionDirectiveForBSS = true;
+  SupportsDebugInformation = true;
+  ExceptionsType = ExceptionHandling::DwarfCFI;
+  AlignmentIsInBytes = false;
+}
diff --git a/lib/Target/RISCV/MCTargetDesc/RISCVMCAsmInfo.h b/lib/Target/RISCV/MCTargetDesc/RISCVMCAsmInfo.h
new file mode 100644
index 0000000..e2ebd38
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/RISCVMCAsmInfo.h
@@ -0,0 +1,27 @@
+//====-- RISCVMCAsmInfo.h - RISCV asm properties ---------------*- C++ -*--===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVTARGETASMINFO_H
+#define LLVM_LIB_TARGET_RISCV_RISCVTARGETASMINFO_H
+
+#include "llvm/MC/MCAsmInfo.h"
+#include "llvm/Support/Compiler.h"
+
+namespace llvm {
+class StringRef;
+
+class RISCVMCAsmInfo : public MCAsmInfo {
+public:
+  explicit RISCVMCAsmInfo();
+
+};
+
+} // namespace llvm
+
+#endif
diff --git a/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp b/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp
new file mode 100644
index 0000000..b864414
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/RISCVMCCodeEmitter.cpp
@@ -0,0 +1,169 @@
+//===-- RISCVMCCodeEmitter.cpp - Convert RISCV code to machine code -------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the RISCVMCCodeEmitter class.
+//
+//===----------------------------------------------------------------------===//
+
+#define DEBUG_TYPE "mccodeemitter"
+#include "MCTargetDesc/RISCVMCTargetDesc.h"
+#include "MCTargetDesc/RISCVMCFixups.h"
+#include "llvm/MC/MCCodeEmitter.h"
+#include "llvm/MC/MCContext.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCInstrInfo.h"
+#include "llvm/MC/MCRegisterInfo.h"
+
+using namespace llvm;
+
+namespace {
+class RISCVMCCodeEmitter : public MCCodeEmitter {
+  const MCInstrInfo &MCII;
+  MCContext &Ctx;
+
+public:
+  RISCVMCCodeEmitter(const MCInstrInfo &mcii, MCContext &ctx)
+    : MCII(mcii), Ctx(ctx) {
+  }
+
+  ~RISCVMCCodeEmitter() {}
+
+  // OVerride MCCodeEmitter.
+  void encodeInstruction(const MCInst &MI, raw_ostream &OS,
+                         SmallVectorImpl<MCFixup> &Fixups,
+                         const MCSubtargetInfo &STI) const override;
+
+private:
+  // Automatically generated by TableGen.
+  uint64_t getBinaryCodeForInstr(const MCInst &MI,
+                                 SmallVectorImpl<MCFixup> &Fixups,
+                                 const MCSubtargetInfo &STI) const;
+
+  // Called by the TableGen code to get the binary encoding of operand
+  // MO in MI.  Fixups is the list of fixups against MI.
+  unsigned getMachineOpValue(const MCInst &MI, const MCOperand &MO,
+                             SmallVectorImpl<MCFixup> &Fixups,
+                             const MCSubtargetInfo &STI) const;
+
+  //RISCV
+  unsigned getJumpTargetEncoding(const MCInst &MI, unsigned int OpNum,
+                                 SmallVectorImpl<MCFixup> &Fixups,
+                                 const MCSubtargetInfo &STI) const {
+    const MCOperand &MO = MI.getOperand(OpNum);
+    //TODO: do we need to sign extend explicitly?
+    if (MO.isImm())
+      return MO.getImm() << 1;
+    llvm_unreachable("Jump with no immediate field");
+  }
+
+  unsigned getBranchTargetEncoding(const MCInst &MI, unsigned int OpNum,
+                                   SmallVectorImpl<MCFixup> &Fixups,
+                                   const MCSubtargetInfo &STI) const {
+    const MCOperand &MO = MI.getOperand(OpNum);
+    //TODO: do we need to sign extend explicitly?
+    if (MO.isImm())
+      return MO.getImm() << 1;
+    // Branch target is expr add fixup
+    Fixups.push_back(MCFixup::create(0, MO.getExpr(),
+          (MCFixupKind)RISCV::fixup_riscv_brlo));
+    Fixups.push_back(MCFixup::create(0, MO.getExpr(),
+          (MCFixupKind)RISCV::fixup_riscv_brhi));
+    return 0;
+  }
+
+  unsigned getPCImmEncoding(const MCInst &MI, unsigned int OpNum,
+                            SmallVectorImpl<MCFixup> &Fixups,
+                            const MCSubtargetInfo &STI) const {
+    const MCOperand &MO = MI.getOperand(OpNum);
+    //TODO: do we need to sign extend explicitly?
+    if (MO.isImm())
+      return MO.getImm();
+    llvm_unreachable("Branch with no immediate field");
+  }
+
+  unsigned getPCImm64Encoding(const MCInst &MI, unsigned int OpNum,
+                              SmallVectorImpl<MCFixup> &Fixups,
+                              const MCSubtargetInfo &STI) const {
+    const MCOperand &MO = MI.getOperand(OpNum);
+    //TODO: do we need to sign extend explicitly?
+    if (MO.isImm())
+      return MO.getImm() << 12;
+    llvm_unreachable("Branch with no immediate field");
+  }
+
+  // Operand OpNum of MI needs a PC-relative fixup of kind Kind at
+  // Offset bytes from the start of MI.  Add the fixup to Fixups
+  // and return the in-place addend, which since we're a RELA target
+  // is always 0.
+  unsigned getPCRelEncoding(const MCInst &MI, unsigned int OpNum,
+                            SmallVectorImpl<MCFixup> &Fixups,
+                            unsigned Kind, int64_t Offset) const;
+
+  unsigned getCallEncoding(const MCInst &MI, unsigned int OpNum,
+                               SmallVectorImpl<MCFixup> &Fixups) const {
+    return getPCRelEncoding(MI, OpNum, Fixups, RISCV::fixup_riscv_call, 0);
+  }
+};
+}
+
+MCCodeEmitter *llvm::createRISCVMCCodeEmitter(const MCInstrInfo &MCII,
+                                                const MCRegisterInfo &MRI,
+                                                MCContext &Ctx) {
+  return new RISCVMCCodeEmitter(MCII, Ctx);
+}
+
+void RISCVMCCodeEmitter::encodeInstruction(const MCInst &MI, raw_ostream &OS,
+                                           SmallVectorImpl<MCFixup> &Fixups,
+                                           const MCSubtargetInfo &STI) const {
+  uint64_t Bits = getBinaryCodeForInstr(MI, Fixups, STI);
+  unsigned Size = MCII.get(MI.getOpcode()).getSize();
+  // Little-endian insertion of Size bytes.
+  unsigned ShiftValue = 0;
+  for (unsigned I = 0; I != Size; ++I) {
+    OS << uint8_t(Bits >> ShiftValue);
+    ShiftValue += 8;
+  }
+}
+
+unsigned
+RISCVMCCodeEmitter::getMachineOpValue(const MCInst &MI, const MCOperand &MO,
+                                      SmallVectorImpl<MCFixup> &Fixups,
+                                      const MCSubtargetInfo &STI) const {
+  if (MO.isReg())
+    return Ctx.getRegisterInfo()->getEncodingValue(MO.getReg());
+  if (MO.isImm())
+    return static_cast<unsigned>(MO.getImm());
+  llvm_unreachable("Unexpected operand type!");
+}
+
+unsigned
+RISCVMCCodeEmitter::getPCRelEncoding(const MCInst &MI, unsigned int OpNum,
+                                       SmallVectorImpl<MCFixup> &Fixups,
+                                       unsigned Kind, int64_t Offset) const {
+  const MCOperand &MO = MI.getOperand(OpNum);
+  // For compatibility with the GNU assembler, treat constant operands as
+  // unadjusted PC-relative offsets.
+  if (MO.isImm())
+    return MO.getImm() / 2;
+
+  const MCExpr *Expr = MO.getExpr();
+  if (Offset) {
+    // The operand value is relative to the start of MI, but the fixup
+    // is relative to the operand field itself, which is Offset bytes
+    // into MI.  Add Offset to the relocation value to cancel out
+    // this difference.
+    const MCExpr *OffsetExpr = MCConstantExpr::create(Offset, Ctx);
+    Expr = MCBinaryExpr::createAdd(Expr, OffsetExpr, Ctx);
+  }
+  Fixups.push_back(MCFixup::create(Offset, Expr, (MCFixupKind)Kind));
+  return 0;
+}
+
+#include "RISCVGenMCCodeEmitter.inc"
diff --git a/lib/Target/RISCV/MCTargetDesc/RISCVMCFixups.h b/lib/Target/RISCV/MCTargetDesc/RISCVMCFixups.h
new file mode 100644
index 0000000..c295e96
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/RISCVMCFixups.h
@@ -0,0 +1,32 @@
+//===-- RISCVMCFixups.h - RISCV-specific fixup entries ----------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVMCFIXUPS_H
+#define LLVM_LIB_TARGET_RISCV_RISCVMCFIXUPS_H
+
+#include "llvm/MC/MCFixup.h"
+
+namespace llvm {
+namespace RISCV {
+  enum FixupKind {
+    // These correspond directly to RISCV relocations.
+    fixup_riscv_brlo = FirstTargetFixupKind,
+    fixup_riscv_brhi,
+    fixup_riscv_jal,
+    fixup_riscv_call,
+    fixup_riscv_call_plt,
+
+    // Marker
+    LastTargetFixupKind,
+    NumTargetFixupKinds = LastTargetFixupKind - FirstTargetFixupKind
+  };
+}
+} // end namespace llvm
+
+#endif
diff --git a/lib/Target/RISCV/MCTargetDesc/RISCVMCObjectWriter.cpp b/lib/Target/RISCV/MCTargetDesc/RISCVMCObjectWriter.cpp
new file mode 100644
index 0000000..dc75a6f
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/RISCVMCObjectWriter.cpp
@@ -0,0 +1,111 @@
+//===-- RISCVMCObjectWriter.cpp - RISCV ELF writer --------------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "MCTargetDesc/RISCVMCTargetDesc.h"
+#include "MCTargetDesc/RISCVMCFixups.h"
+#include "llvm/MC/MCELFObjectWriter.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCValue.h"
+
+using namespace llvm;
+
+namespace {
+class RISCVObjectWriter : public MCELFObjectTargetWriter {
+public:
+  RISCVObjectWriter(uint8_t OSABI);
+
+  virtual ~RISCVObjectWriter();
+
+protected:
+  // Override MCELFObjectTargetWriter.
+  unsigned GetRelocType(const MCValue &Target, const MCFixup &Fixup,
+                        bool IsPCRel) const override;
+};
+} // end anonymouse namespace
+
+RISCVObjectWriter::RISCVObjectWriter(uint8_t OSABI)
+  : MCELFObjectTargetWriter(/*Is64Bit=*/true, OSABI, ELF::EM_RISCV,
+                            /*HasRelocationAddend=*/ true) {}
+
+RISCVObjectWriter::~RISCVObjectWriter() {
+}
+
+// Return the relocation type for an absolute value of MCFixupKind Kind.
+static unsigned getAbsoluteReloc(unsigned Kind) {
+  switch (Kind) {
+  case FK_Data_4: return ELF::R_RISCV_32;
+  case FK_Data_8: return ELF::R_RISCV_64;
+  }
+  llvm_unreachable("Unsupported absolute address");
+}
+
+//TODO: fix relocation types
+// Return the relocation type for a PC-relative value of MCFixupKind Kind.
+static unsigned getPCRelReloc(unsigned Kind) {
+  switch (Kind) {
+  case FK_Data_4:                return ELF::R_RISCV_CALL;
+  case RISCV::fixup_riscv_brlo:  return ELF::R_RISCV_BRANCH;
+  case RISCV::fixup_riscv_brhi:  return ELF::R_RISCV_BRANCH;
+  case RISCV::fixup_riscv_jal:   return ELF::R_RISCV_JAL;
+  case RISCV::fixup_riscv_call:  return ELF::R_RISCV_CALL;
+  }
+  llvm_unreachable("Unsupported PC-relative address");
+}
+
+// Return the R_RISCV_TLS* relocation type for MCFixupKind Kind.
+static unsigned getTLSLEReloc(unsigned Kind) {
+  switch (Kind) {
+  case FK_Data_4: return ELF::R_RISCV_TLS_TPREL32;
+  case FK_Data_8: return ELF::R_RISCV_TLS_TPREL64;
+  }
+  llvm_unreachable("Unsupported absolute address");
+}
+
+// Return the PLT relocation counterpart of MCFixupKind Kind.
+static unsigned getPLTReloc(unsigned Kind) {
+  switch (Kind) {
+  case RISCV::fixup_riscv_call_plt: return ELF::R_RISCV_CALL_PLT;
+  }
+  llvm_unreachable("Unsupported absolute address");
+}
+
+unsigned RISCVObjectWriter::GetRelocType(const MCValue &Target,
+                                         const MCFixup &Fixup,
+                                         bool IsPCRel) const {
+  MCSymbolRefExpr::VariantKind Modifier = (Target.isAbsolute() ?
+                                           MCSymbolRefExpr::VK_None :
+                                           Target.getSymA()->getKind());
+  unsigned Kind = Fixup.getKind();
+  switch (Modifier) {
+  case MCSymbolRefExpr::VK_None:
+    if (IsPCRel)
+      return getPCRelReloc(Kind);
+    return getAbsoluteReloc(Kind);
+
+  case MCSymbolRefExpr::VK_NTPOFF:
+    assert(!IsPCRel && "NTPOFF shouldn't be PC-relative");
+    return getTLSLEReloc(Kind);
+
+  case MCSymbolRefExpr::VK_GOT:
+    llvm_unreachable("GOT accesses are not supported yet");
+
+  case MCSymbolRefExpr::VK_PLT:
+    assert(IsPCRel && "@PLT shouldt be PC-relative");
+    return getPLTReloc(Kind);
+
+  default:
+    llvm_unreachable("Modifier not supported");
+  }
+}
+
+MCObjectWriter *llvm::createRISCVObjectWriter(raw_pwrite_stream &OS,
+                                                uint8_t OSABI) {
+  MCELFObjectTargetWriter *MOTW = new RISCVObjectWriter(OSABI);
+  return createELFObjectWriter(MOTW, OS, /*IsLittleEndian=*/false);
+}
diff --git a/lib/Target/RISCV/MCTargetDesc/RISCVMCTargetDesc.cpp b/lib/Target/RISCV/MCTargetDesc/RISCVMCTargetDesc.cpp
new file mode 100644
index 0000000..f90c470
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/RISCVMCTargetDesc.cpp
@@ -0,0 +1,175 @@
+//===-- RISCVMCTargetDesc.cpp - RISCV target descriptions -------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVMCTargetDesc.h"
+#include "InstPrinter/RISCVInstPrinter.h"
+#include "RISCVMCAsmInfo.h"
+#include "llvm/MC/MCCodeGenInfo.h"
+#include "llvm/MC/MCInstrInfo.h"
+#include "llvm/MC/MCRegisterInfo.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/MC/MCSubtargetInfo.h"
+#include "llvm/Support/TargetRegistry.h"
+
+#define GET_INSTRINFO_MC_DESC
+#include "RISCVGenInstrInfo.inc"
+
+#define GET_SUBTARGETINFO_MC_DESC
+#include "RISCVGenSubtargetInfo.inc"
+
+#define GET_REGINFO_MC_DESC
+#include "RISCVGenRegisterInfo.inc"
+
+using namespace llvm;
+
+static MCAsmInfo *createRISCVMCAsmInfo(const MCRegisterInfo &MRI,
+                                       const Triple &TT) {
+  MCAsmInfo *MAI = new RISCVMCAsmInfo();
+  MCCFIInstruction Inst = MCCFIInstruction::createDefCfa(
+      nullptr, MRI.getDwarfRegNum(RISCV::sp, true),
+      RISCVMC::CFAOffsetFromInitialSP);
+  MAI->addInitialFrameState(Inst);
+  return MAI;
+}
+
+static MCInstrInfo *createRISCVMCInstrInfo() {
+  MCInstrInfo *X = new MCInstrInfo();
+  InitRISCVMCInstrInfo(X);
+  return X;
+}
+
+static MCRegisterInfo *createRISCVMCRegisterInfo(const Triple &TT) {
+  MCRegisterInfo *X = new MCRegisterInfo();
+  InitRISCVMCRegisterInfo(X, RISCV::sp);
+  return X;
+}
+
+static MCSubtargetInfo *createRISCVMCSubtargetInfo(const Triple &TT,
+                                                     StringRef CPU,
+                                                     StringRef FS) {
+  return createRISCVMCSubtargetInfoImpl(TT, CPU, FS);
+}
+
+static MCCodeGenInfo *createRISCVMCCodeGenInfo(const Triple &TT, Reloc::Model RM,
+                                                 CodeModel::Model CM,
+                                                 CodeGenOpt::Level OL) {
+  MCCodeGenInfo *X = new MCCodeGenInfo();
+
+  // Static code is suitable for use in a dynamic executable; there is no
+  // separate DynamicNoPIC model.
+  if (RM == Reloc::Default || RM == Reloc::DynamicNoPIC)
+    RM = Reloc::Static;
+
+  // For RISCV we define the models as follows:
+  //
+  // Small:  BRASL can call any function and will use a stub if necessary.
+  //         Locally-binding symbols will always be in range of LARL.
+  //
+  // Medium: BRASL can call any function and will use a stub if necessary.
+  //         GOT slots and locally-defined text will always be in range
+  //         of LARL, but other symbols might not be.
+  //
+  // Large:  Equivalent to Medium for now.
+  //
+  // Kernel: Equivalent to Medium for now.
+  //
+  // This means that any PIC module smaller than 4GB meets the
+  // requirements of Small, so Small seems like the best default there.
+  //
+  // All symbols bind locally in a non-PIC module, so the choice is less
+  // obvious.  There are two cases:
+  //
+  // - When creating an executable, PLTs and copy relocations allow
+  //   us to treat external symbols as part of the executable.
+  //   Any executable smaller than 4GB meets the requirements of Small,
+  //   so that seems like the best default.
+  //
+  // - When creating JIT code, stubs will be in range of BRASL if the
+  //   image is less than 4GB in size.  GOT entries will likewise be
+  //   in range of LARL.  However, the JIT environment has no equivalent
+  //   of copy relocs, so locally-binding data symbols might not be in
+  //   the range of LARL.  We need the Medium model in that case.
+  if (CM == CodeModel::Default)
+    CM = CodeModel::Small;
+  else if (CM == CodeModel::JITDefault)
+    CM = RM == Reloc::PIC_ ? CodeModel::Small : CodeModel::Medium;
+  X->initMCCodeGenInfo(RM, CM, OL);
+  return X;
+}
+
+static MCInstPrinter *createRISCVMCInstPrinter(const Triple &TT,
+                                                 unsigned SyntaxVariant,
+                                                 const MCAsmInfo &MAI,
+                                                 const MCInstrInfo &MII,
+                                                 const MCRegisterInfo &MRI) {
+  return new RISCVInstPrinter(MAI, MII, MRI);
+}
+
+static MCStreamer *
+createRISCVMCObjectStreamer(const Triple &TT, MCContext &Ctx,
+                            MCAsmBackend &MAB, raw_pwrite_stream &OS,
+                            MCCodeEmitter *Emitter, bool RelaxAll) {
+  return createELFStreamer(Ctx, MAB, OS, Emitter, RelaxAll);
+}
+
+extern "C" void LLVMInitializeRISCVTargetMC() {
+  // Register the MCAsmInfo.
+  TargetRegistry::RegisterMCAsmInfo(TheRISCVTarget,
+                                    createRISCVMCAsmInfo);
+  TargetRegistry::RegisterMCAsmInfo(TheRISCV64Target,
+                                    createRISCVMCAsmInfo);
+
+  // Register the MCCodeGenInfo.
+  TargetRegistry::RegisterMCCodeGenInfo(TheRISCVTarget,
+                                        createRISCVMCCodeGenInfo);
+  TargetRegistry::RegisterMCCodeGenInfo(TheRISCV64Target,
+                                        createRISCVMCCodeGenInfo);
+
+  // Register the MCCodeEmitter.
+  TargetRegistry::RegisterMCCodeEmitter(TheRISCVTarget,
+					createRISCVMCCodeEmitter);
+  TargetRegistry::RegisterMCCodeEmitter(TheRISCV64Target,
+					createRISCVMCCodeEmitter);
+
+  // Register the MCInstrInfo.
+  TargetRegistry::RegisterMCInstrInfo(TheRISCVTarget,
+                                      createRISCVMCInstrInfo);
+  TargetRegistry::RegisterMCInstrInfo(TheRISCVTarget,
+                                      createRISCVMCInstrInfo);
+
+  // Register the MCRegisterInfo.
+  TargetRegistry::RegisterMCRegInfo(TheRISCVTarget,
+                                    createRISCVMCRegisterInfo);
+  TargetRegistry::RegisterMCRegInfo(TheRISCV64Target,
+                                    createRISCVMCRegisterInfo);
+
+  // Register the MCSubtargetInfo.
+  TargetRegistry::RegisterMCSubtargetInfo(TheRISCVTarget,
+                                          createRISCVMCSubtargetInfo);
+  TargetRegistry::RegisterMCSubtargetInfo(TheRISCV64Target,
+                                          createRISCVMCSubtargetInfo);
+
+  // Register the MCAsmBackend.
+  TargetRegistry::RegisterMCAsmBackend(TheRISCVTarget,
+                                       createRISCVMCAsmBackend);
+  TargetRegistry::RegisterMCAsmBackend(TheRISCV64Target,
+                                       createRISCVMCAsmBackend);
+
+  // Register the MCInstPrinter.
+  TargetRegistry::RegisterMCInstPrinter(TheRISCVTarget,
+                                        createRISCVMCInstPrinter);
+  TargetRegistry::RegisterMCInstPrinter(TheRISCV64Target,
+                                        createRISCVMCInstPrinter);
+
+  // Register the MCObjectStreamer;
+  TargetRegistry::RegisterELFStreamer(TheRISCVTarget,
+                                           createRISCVMCObjectStreamer);
+  TargetRegistry::RegisterELFStreamer(TheRISCV64Target,
+                                           createRISCVMCObjectStreamer);
+}
diff --git a/lib/Target/RISCV/MCTargetDesc/RISCVMCTargetDesc.h b/lib/Target/RISCV/MCTargetDesc/RISCVMCTargetDesc.h
new file mode 100644
index 0000000..db702de
--- /dev/null
+++ b/lib/Target/RISCV/MCTargetDesc/RISCVMCTargetDesc.h
@@ -0,0 +1,64 @@
+//===-- RISCVMCTargetDesc.h - RISCV target descriptions ---------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVMCTARGETDESC_H
+#define LLVM_LIB_TARGET_RISCV_RISCVMCTARGETDESC_H
+
+#include "llvm/Support/DataTypes.h"
+#include "llvm/Support/TargetRegistry.h"
+
+namespace llvm {
+
+class MCAsmBackend;
+class MCCodeEmitter;
+class MCContext;
+class MCInstrInfo;
+class MCObjectWriter;
+class MCRegisterInfo;
+class MCSubtargetInfo;
+class StringRef;
+class Target;
+class raw_ostream;
+
+extern Target TheRISCVTarget;
+extern Target TheRISCV64Target;
+
+MCCodeEmitter *createRISCVMCCodeEmitter(const MCInstrInfo &MCII,
+                                          const MCRegisterInfo &MRI,
+                                          MCContext &Ctx);
+
+MCAsmBackend *createRISCVMCAsmBackend(const Target &T,
+                                      const MCRegisterInfo &MRI, const Triple &TT,
+                                      StringRef CPU);
+
+MCObjectWriter *createRISCVObjectWriter(raw_pwrite_stream &OS, uint8_t OSABI);
+
+namespace RISCVMC {
+  // How many bytes are in the ABI-defined, caller-allocated part of
+  // a stack frame.
+  const int64_t CallFrameSize = 160;
+
+  // The offset of the DWARF CFA from the incoming stack pointer.
+  const int64_t CFAOffsetFromInitialSP = CallFrameSize;
+}
+} // end namespace llvm
+
+// Defines symbolic names for RISCV registers.
+// This defines a mapping from register name to register number.
+#define GET_REGINFO_ENUM
+#include "RISCVGenRegisterInfo.inc"
+
+// Defines symbolic names for the RISCV instructions.
+#define GET_INSTRINFO_ENUM
+#include "RISCVGenInstrInfo.inc"
+
+#define GET_SUBTARGETINFO_ENUM
+#include "RISCVGenSubtargetInfo.inc"
+
+#endif
diff --git a/lib/Target/RISCV/Makefile b/lib/Target/RISCV/Makefile
new file mode 100644
index 0000000..720cdab
--- /dev/null
+++ b/lib/Target/RISCV/Makefile
@@ -0,0 +1,27 @@
+##===- lib/Target/RISCV/Makefile ---------------------------*- Makefile -*-===##
+#
+#                     The LLVM Compiler Infrastructure
+#
+# This file is distributed under the University of Illinois Open Source
+# License. See LICENSE.TXT for details.
+#
+##===----------------------------------------------------------------------===##
+
+LEVEL = ../../..
+LIBRARYNAME = LLVMRISCVCodeGen
+TARGET = RISCV
+
+# Make sure that tblgen is run, first thing.
+BUILT_SOURCES = RISCVGenRegisterInfo.inc \
+		RISCVGenAsmWriter.inc \
+		RISCVGenAsmMatcher.inc \
+		RISCVGenInstrInfo.inc \
+		RISCVGenDAGISel.inc \
+		RISCVGenSubtargetInfo.inc \
+		RISCVGenCallingConv.inc \
+		RISCVGenMCCodeEmitter.inc
+
+DIRS = InstPrinter AsmParser TargetInfo MCTargetDesc
+
+include $(LEVEL)/Makefile.common
+
diff --git a/lib/Target/RISCV/README.txt b/lib/Target/RISCV/README.txt
new file mode 100644
index 0000000..2fdd63f
--- /dev/null
+++ b/lib/Target/RISCV/README.txt
@@ -0,0 +1,7 @@
+//===---------------------------------------------------------------------===//
+// Random notes about and ideas for the RISC-V backend.
+//===---------------------------------------------------------------------===//
+TODO (very incomplete):
+-trap on illegal shamt
+-either sync bits with riscv-opcodes or remove them
+--removal may be hard due to LLVM spec for instructions?
diff --git a/lib/Target/RISCV/RISCV.h b/lib/Target/RISCV/RISCV.h
new file mode 100644
index 0000000..02620c6
--- /dev/null
+++ b/lib/Target/RISCV/RISCV.h
@@ -0,0 +1,78 @@
+//===- RISCV.h - Top-Level Interface for RISCV representation ---*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the entry points for global functions defined in
+// the LLVM RISCV backend.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCV_H
+#define LLVM_LIB_TARGET_RISCV_RISCV_H
+
+#include "MCTargetDesc/RISCVMCTargetDesc.h"
+#include "llvm/Support/CodeGen.h"
+
+namespace llvm {
+  class RISCVTargetMachine;
+  class FunctionPass;
+
+  namespace RISCV {
+    // Condition-code mask values.
+    const unsigned CCMASK_0 = 1 << 3;
+    const unsigned CCMASK_1 = 1 << 2;
+    const unsigned CCMASK_2 = 1 << 1;
+    const unsigned CCMASK_3 = 1 << 0;
+    const unsigned CCMASK_ANY = CCMASK_0 | CCMASK_1 | CCMASK_2 | CCMASK_3;
+
+    // Condition-code mask assignments for floating-point comparisons.
+    const unsigned CCMASK_CMP_EQ = CCMASK_0;
+    const unsigned CCMASK_CMP_LT = CCMASK_1;
+    const unsigned CCMASK_CMP_GT = CCMASK_2;
+    const unsigned CCMASK_CMP_UO = CCMASK_3;
+    const unsigned CCMASK_CMP_NE = CCMASK_CMP_LT | CCMASK_CMP_GT;
+    const unsigned CCMASK_CMP_LE = CCMASK_CMP_EQ | CCMASK_CMP_LT;
+    const unsigned CCMASK_CMP_GE = CCMASK_CMP_EQ | CCMASK_CMP_GT;
+    const unsigned CCMASK_CMP_O  = CCMASK_ANY ^ CCMASK_CMP_UO;
+
+    // Return true if Val fits an LLILL operand.
+    static inline bool isImmLL(uint64_t Val) {
+      return (Val & ~0x000000000000ffffULL) == 0;
+    }
+
+    // Return true if Val fits an LLILH operand.
+    static inline bool isImmLH(uint64_t Val) {
+      return (Val & ~0x00000000ffff0000ULL) == 0;
+    }
+
+    // Return true if Val fits an LLIHL operand.
+    static inline bool isImmHL(uint64_t Val) {
+      return (Val & ~0x00000ffff00000000ULL) == 0;
+    }
+
+    // Return true if Val fits an LLIHH operand.
+    static inline bool isImmHH(uint64_t Val) {
+      return (Val & ~0xffff000000000000ULL) == 0;
+    }
+
+    // Return true if Val fits an LLILF operand.
+    static inline bool isImmLF(uint64_t Val) {
+      return (Val & ~0x00000000ffffffffULL) == 0;
+    }
+
+    // Return true if Val fits an LLIHF operand.
+    static inline bool isImmHF(uint64_t Val) {
+      return (Val & ~0xffffffff00000000ULL) == 0;
+    }
+  }
+
+  FunctionPass *createRISCVISelDag(RISCVTargetMachine &TM,
+                                     CodeGenOpt::Level OptLevel);
+  FunctionPass *createRISCVBranchSelectionPass();
+} // end namespace llvm;
+#endif
diff --git a/lib/Target/RISCV/RISCV.td b/lib/Target/RISCV/RISCV.td
new file mode 100644
index 0000000..2ad5fa0
--- /dev/null
+++ b/lib/Target/RISCV/RISCV.td
@@ -0,0 +1,99 @@
+//===-- RISCV.td - Describe the RISCV target machine ---------*- tblgen -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// Target-independent interfaces which we are implementing
+//===----------------------------------------------------------------------===//
+
+include "llvm/Target/Target.td"
+
+//===----------------------------------------------------------------------===//
+// RISCV subtarget features
+//===----------------------------------------------------------------------===//
+
+def FeatureM : SubtargetFeature<"m", "HasM", "true",
+                                "Supports Integer Multiplication and Division.">;
+def FeatureA : SubtargetFeature<"a", "HasA", "true",
+                                "Supports Atomic Instructions.">;
+def FeatureF : SubtargetFeature<"f", "HasF", "true",
+                                "Supports Single-Precision Floating-Point.">;
+def FeatureD : SubtargetFeature<"d", "HasD", "true",
+                                "Supports Double-Precision Floating-Point.">;
+
+def FeatureRV32 : SubtargetFeature<"rv32", "RISCVArchVersion", "RV32", 
+                                   "RV32 ISA Support">;
+def FeatureRV64 : SubtargetFeature<"rv64", "RISCVArchVersion", "RV64", 
+                                   "RV64 ISA Support">;
+
+def FeatureSoftFloat : SubtargetFeature<"soft-float", "UseSoftFloat", "true",
+                                        "Use software floating point features.">;
+
+//===----------------------------------------------------------------------===//
+// RISCV supported processors
+//===----------------------------------------------------------------------===//
+
+class Proc<string Name, list<SubtargetFeature> Features>
+ : Processor<Name, NoItineraries, Features>;
+
+def : Proc<"RV32I", [FeatureRV32]>;
+def : Proc<"RV32IMAFD", [FeatureRV32,FeatureM,FeatureA,FeatureF,FeatureD]>;
+def : Proc<"RV64I", [FeatureRV64]>;
+def : Proc<"RV64IMAFD", [FeatureRV64,FeatureM,FeatureA,FeatureF,FeatureD]>;
+def : Proc<"Rocket", [FeatureRV64,FeatureM,FeatureA,FeatureF,FeatureD]>;
+
+//===----------------------------------------------------------------------===//
+// Register file description
+//===----------------------------------------------------------------------===//
+
+include "RISCVRegisterInfo.td"
+
+//===----------------------------------------------------------------------===//
+// Calling convention description
+//===----------------------------------------------------------------------===//
+
+include "RISCVCallingConv.td"
+
+//===----------------------------------------------------------------------===//
+// Instruction descriptions
+//===----------------------------------------------------------------------===//
+
+include "RISCVOperators.td"
+include "RISCVOperands.td"
+include "RISCVInstrFormats.td"
+include "RISCVInstrInfo.td"
+
+def RISCVInstrInfo : InstrInfo {}
+
+//===----------------------------------------------------------------------===//
+// Assembly parser
+//===----------------------------------------------------------------------===//
+
+def RISCVAsmParser : AsmParser {
+  let ShouldEmitMatchRegisterName = 0;
+  let MnemonicContainsDot = 1;
+}
+
+//===----------------------------------------------------------------------===//
+// Assembly writer
+//===----------------------------------------------------------------------===//
+
+def RISCVAsmWriter : AsmWriter {
+  string AsmWriterClassName = "InstPrinter";
+  bit isMCAsmWriter = 1;
+}
+
+//===----------------------------------------------------------------------===//
+// Top-level target declaration
+//===----------------------------------------------------------------------===//
+
+def RISCV : Target {
+  let InstructionSet = RISCVInstrInfo;
+  let AssemblyParsers = [RISCVAsmParser];
+  let AssemblyWriters = [RISCVAsmWriter];
+}
diff --git a/lib/Target/RISCV/RISCVAsmPrinter.cpp b/lib/Target/RISCV/RISCVAsmPrinter.cpp
new file mode 100644
index 0000000..60c7cba
--- /dev/null
+++ b/lib/Target/RISCV/RISCVAsmPrinter.cpp
@@ -0,0 +1,156 @@
+//===-- RISCVAsmPrinter.cpp - RISCV LLVM assembly printer -------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// Streams RISCV assembly language and associated data, in the form of
+// MCInsts and MCExprs respectively.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVAsmPrinter.h"
+#include "InstPrinter/RISCVInstPrinter.h"
+#include "RISCVConstantPoolValue.h"
+#include "RISCVMCInstLower.h"
+#include "llvm/CodeGen/MachineModuleInfoImpls.h"
+#include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCStreamer.h"
+#include "llvm/MC/MCSymbol.h"
+#include "llvm/Support/TargetRegistry.h"
+
+using namespace llvm;
+
+void RISCVAsmPrinter::EmitInstruction(const MachineInstr *MI) {
+  RISCVMCInstLower Lower(MF->getContext(), *this);
+  MCInst LoweredMI;
+  Lower.lower(MI, LoweredMI);
+  EmitToStreamer(*OutStreamer, LoweredMI);
+}
+
+// Convert a RISCV-specific constant pool modifier into the associated
+// MCSymbolRefExpr variant kind.
+static MCSymbolRefExpr::VariantKind
+getModifierVariantKind(RISCVCP::RISCVCPModifier Modifier) {
+  switch (Modifier) {
+  case RISCVCP::NTPOFF: return MCSymbolRefExpr::VK_NTPOFF;
+  }
+  llvm_unreachable("Invalid SystemCPModifier!");
+}
+
+void RISCVAsmPrinter::
+EmitMachineConstantPoolValue(MachineConstantPoolValue *MCPV) {
+  RISCVConstantPoolValue *RVCPV =
+    static_cast<RISCVConstantPoolValue*>(MCPV);
+
+  const MCExpr *Expr = MCSymbolRefExpr::create(
+      getSymbol(RVCPV->getGlobalValue()),
+      getModifierVariantKind(RVCPV->getModifier()), OutContext);
+  uint64_t Size =
+      getDataLayout().getTypeAllocSize(RVCPV->getType());
+
+  OutStreamer->EmitValue(Expr, Size);
+}
+
+void RISCVAsmPrinter::printOperand(const MachineInstr *MI, int OpNo, raw_ostream &O) {
+  const MachineOperand &MO = MI->getOperand(OpNo); 
+  //look at target flags to see if we should wrap this operand
+  switch(MO.getTargetFlags()){
+    case RISCVII::MO_ABS_HI: O << "%hi("; break;
+    case RISCVII::MO_ABS_LO: O << "%lo("; break;
+    case RISCVII::MO_TPREL_HI: O << "%tprel_hi("; break;
+    case RISCVII::MO_TPREL_LO: O << "%tprel_lo("; break;
+  }
+ switch (MO.getType()) {
+    case MachineOperand::MO_Register:
+    case MachineOperand::MO_Immediate: {
+      RISCVMCInstLower Lower(MF->getContext(), *this);
+      MCOperand MC(Lower.lowerOperand(MI->getOperand(OpNo)));
+      RISCVInstPrinter::printOperand(MC, O);
+      break;
+      }
+    case MachineOperand::MO_GlobalAddress:
+      O << *getSymbol(MO.getGlobal());
+      break;
+    default:
+      llvm_unreachable("<unknown operand type>");
+  }
+
+  if(MO.getTargetFlags()) {
+    O << ")";
+  }
+}
+
+
+bool RISCVAsmPrinter::PrintAsmOperand(const MachineInstr *MI,
+                                        unsigned OpNo,
+                                        unsigned AsmVariant,
+                                        const char *ExtraCode,
+                                        raw_ostream &OS) {
+  if (ExtraCode && *ExtraCode == 'n') {
+    if (!MI->getOperand(OpNo).isImm())
+      return true;
+    OS << -int64_t(MI->getOperand(OpNo).getImm());
+  } else {
+    printOperand(MI, OpNo, OS);
+  }
+  return false;
+}
+
+bool RISCVAsmPrinter::PrintAsmMemoryOperand(const MachineInstr *MI,
+                                              unsigned OpNo,
+                                              unsigned AsmVariant,
+                                              const char *ExtraCode,
+                                              raw_ostream &OS) {
+  RISCVInstPrinter::printAddress(MI->getOperand(OpNo).getReg(),
+                                   MI->getOperand(OpNo + 1).getImm(),
+                                   OS);
+  return false;
+}
+
+void RISCVAsmPrinter::printMemOperand(const MachineInstr *MI, int opNum,
+                                      raw_ostream &OS) {
+    OS << '%' << RISCVInstPrinter::getRegisterName(MI->getOperand(opNum).getReg());
+    OS << "(";
+    OS << MI->getOperand(opNum+1).getImm();
+    OS << ")";
+}
+
+void RISCVAsmPrinter::EmitEndOfAsmFile(Module &M) {
+  const Triple &TT = TM.getTargetTriple();
+  if (TT.isOSBinFormatELF()) {
+    const TargetLoweringObjectFileELF &TLOFELF =
+      static_cast<const TargetLoweringObjectFileELF &>(getObjFileLowering());
+
+    MachineModuleInfoELF &MMIELF = MMI->getObjFileInfo<MachineModuleInfoELF>();
+
+    // Output stubs for external and common global variables.
+    MachineModuleInfoELF::SymbolListTy Stubs = MMIELF.GetGVStubList();
+    if (!Stubs.empty()) {
+      OutStreamer->SwitchSection(TLOFELF.getDataRelSection());
+      const DataLayout TD = getDataLayout();
+
+      for (unsigned i = 0, e = Stubs.size(); i != e; ++i) {
+        OutStreamer->EmitLabel(Stubs[i].first);
+        OutStreamer->EmitSymbolValue(Stubs[i].second.getPointer(),
+                                    TD.getPointerSize(0), 0);
+      }
+      Stubs.clear();
+    }
+  }
+}
+
+bool RISCVAsmPrinter::runOnMachineFunction(MachineFunction &MF) {
+  Subtarget = &MF.getSubtarget<RISCVSubtarget>();
+  return AsmPrinter::runOnMachineFunction(MF);
+}
+
+// Force static initialization.
+extern "C" void LLVMInitializeRISCVAsmPrinter() {
+  RegisterAsmPrinter<RISCVAsmPrinter> A(TheRISCVTarget);
+  RegisterAsmPrinter<RISCVAsmPrinter> B(TheRISCV64Target);
+}
diff --git a/lib/Target/RISCV/RISCVAsmPrinter.h b/lib/Target/RISCV/RISCVAsmPrinter.h
new file mode 100644
index 0000000..c354ccb
--- /dev/null
+++ b/lib/Target/RISCV/RISCVAsmPrinter.h
@@ -0,0 +1,51 @@
+//===-- RISCVAsmPrinter.h - RISCV LLVM assembly printer --------*- C++ -*--===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVASMPRINTER_H
+#define LLVM_LIB_TARGET_RISCV_RISCVASMPRINTER_H
+
+#include "RISCVTargetMachine.h"
+#include "llvm/CodeGen/AsmPrinter.h"
+#include "llvm/Support/Compiler.h"
+
+namespace llvm {
+class MCStreamer;
+class MachineBasicBlock;
+class MachineInstr;
+class Module;
+class raw_ostream;
+
+class LLVM_LIBRARY_VISIBILITY RISCVAsmPrinter : public AsmPrinter {
+private:
+  const RISCVSubtarget *Subtarget;
+
+public:
+  RISCVAsmPrinter(TargetMachine &TM, std::unique_ptr<MCStreamer> Streamer)
+    : AsmPrinter(TM, std::move(Streamer)) {}
+
+  // Override AsmPrinter.
+  const char *getPassName() const override {
+    return "RISCV Assembly Printer";
+  }
+  void EmitInstruction(const MachineInstr *MI) override;
+  void EmitMachineConstantPoolValue(MachineConstantPoolValue *MCPV) override;
+  void printOperand(const MachineInstr *MI, int opNum, raw_ostream &O);
+  bool PrintAsmOperand(const MachineInstr *MI, unsigned OpNo,
+                       unsigned AsmVariant, const char *ExtraCode,
+                       raw_ostream &OS) override;
+  bool PrintAsmMemoryOperand(const MachineInstr *MI, unsigned OpNo,
+                             unsigned AsmVariant, const char *ExtraCode,
+                             raw_ostream &OS) override;
+  void printMemOperand(const MachineInstr *MI, int opNum, raw_ostream &OS);
+  void EmitEndOfAsmFile(Module &M) override;
+  bool runOnMachineFunction(MachineFunction &MF) override;
+};
+} // end namespace llvm
+
+#endif
diff --git a/lib/Target/RISCV/RISCVBranchSelector.cpp b/lib/Target/RISCV/RISCVBranchSelector.cpp
new file mode 100644
index 0000000..318bbf9
--- /dev/null
+++ b/lib/Target/RISCV/RISCVBranchSelector.cpp
@@ -0,0 +1,193 @@
+//===-- RISCVBranchSelector.cpp - Emit long conditional branches ----------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains a pass that scans a machine function to determine which
+// conditional branches need more than 12 bits of displacement to reach their
+// target basic block.  It does this in two passes; a calculation of basic block
+// positions pass, and a branch pseudo op to machine branch opcode pass.  This
+// pass should be run last, just before the assembly printer.
+//
+//===----------------------------------------------------------------------===//
+
+#define DEBUG_TYPE "riscv-branch-select"
+#include "RISCV.h"
+#include "RISCVInstrBuilder.h"
+#include "RISCVInstrInfo.h"
+#include "llvm/ADT/Statistic.h"
+#include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/Support/MathExtras.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+#include "llvm/Target/TargetMachine.h"
+using namespace llvm;
+
+STATISTIC(NumExpanded, "Number of branches expanded to long format");
+
+namespace llvm {
+  void initializeRISCVBSelPass(PassRegistry&);
+}
+
+namespace {
+  struct RISCVBSel : public MachineFunctionPass {
+    static char ID;
+    RISCVBSel() : MachineFunctionPass(ID) {
+      initializeRISCVBSelPass(*PassRegistry::getPassRegistry());
+    }
+
+    /// BlockSizes - The sizes of the basic blocks in the function.
+    std::vector<unsigned> BlockSizes;
+
+    virtual bool runOnMachineFunction(MachineFunction &Fn);
+
+    virtual const char *getPassName() const {
+      return "RISCV Branch Selector";
+    }
+  };
+  char RISCVBSel::ID = 0;
+}
+
+INITIALIZE_PASS(RISCVBSel, "riscv-branch-select", "RISCV Branch Selector",
+                false, false)
+
+/// createRISCVBranchSelectionPass - returns an instance of the Branch Selection
+/// Pass
+///
+FunctionPass *llvm::createRISCVBranchSelectionPass() {
+  return new RISCVBSel();
+}
+
+bool RISCVBSel::runOnMachineFunction(MachineFunction &Fn) {
+  const RISCVInstrInfo *TII = static_cast<const RISCVInstrInfo *>(
+      Fn.getTarget().getSubtargetImpl(*Fn.getFunction())->getInstrInfo());
+  // Give the blocks of the function a dense, in-order, numbering.
+  Fn.RenumberBlocks();
+  BlockSizes.resize(Fn.getNumBlockIDs());
+
+  // Measure each MBB and compute a size for the entire function.
+  unsigned FuncSize = 0;
+  for (MachineFunction::iterator MFI = Fn.begin(), E = Fn.end(); MFI != E;
+       ++MFI) {
+    MachineBasicBlock *MBB = MFI;
+
+    unsigned BlockSize = 0;
+    for (MachineBasicBlock::iterator MBBI = MBB->begin(), EE = MBB->end();
+         MBBI != EE; ++MBBI)
+      BlockSize += TII->GetInstSizeInBytes(MBBI);
+    
+    BlockSizes[MBB->getNumber()] = BlockSize;
+    FuncSize += BlockSize;
+  }
+  
+  // If the entire function is smaller than the displacement of a branch field,
+  // we know we don't need to shrink any branches in this function.  This is a
+  // common case.
+  if (FuncSize < (1 << 11)) {
+    BlockSizes.clear();
+    return false;
+  }
+  
+  // For each conditional branch, if the offset to its destination is larger
+  // than the offset field allows, transform it into a long branch sequence
+  // like this:
+  //   short branch:
+  //     bCC MBB
+  //   long branch:
+  //     b!CC $PC+8
+  //     j MBB
+  //
+  bool MadeChange = true;
+  bool EverMadeChange = false;
+  while (MadeChange) {
+    // Iteratively expand branches until we reach a fixed point.
+    MadeChange = false;
+  
+    for (MachineFunction::iterator MFI = Fn.begin(), E = Fn.end(); MFI != E;
+         ++MFI) {
+      MachineBasicBlock &MBB = *MFI;
+      unsigned MBBStartOffset = 0;
+      for (MachineBasicBlock::iterator I = MBB.begin(), E = MBB.end();
+           I != E; ++I) {
+        MachineBasicBlock *Dest = 0;
+        //All RISCV Branches have their dest MBB as the first machine operand
+        SmallVector<MachineOperand, 4> Cond;
+        Cond.push_back(MachineOperand::CreateImm(0));
+        const MachineOperand *DestOp;
+
+        //const MachineInstr *const_i = I;
+        if (!TII->isBranch(I, Cond, DestOp)){
+          MBBStartOffset += TII->GetInstSizeInBytes(I);
+          continue;
+        }
+
+        MachineBasicBlock *FBB = 0;
+        Cond.clear();
+        if(TII->AnalyzeBranch(MBB, Dest, FBB, Cond, false)){
+          //we can't fix this branch since we can't even analyze it
+          MBBStartOffset += TII->GetInstSizeInBytes(I);
+          continue;
+        }
+
+        if(Cond.empty() || Cond[0].getImm() == RISCV::CCMASK_ANY){
+          MBBStartOffset += TII->GetInstSizeInBytes(I);
+          continue;
+        }
+        
+        // Determine the offset from the current branch to the destination
+        // block.
+        int BranchSize;
+        if (Dest->getNumber() <= MBB.getNumber()) {
+          // If this is a backwards branch, the delta is the offset from the
+          // start of this block to this branch, plus the sizes of all blocks
+          // from this block to the dest.
+          BranchSize = MBBStartOffset;
+          
+          for (unsigned i = Dest->getNumber(), e = MBB.getNumber(); i != e; ++i)
+            BranchSize += BlockSizes[i];
+        } else {
+          // Otherwise, add the size of the blocks between this block and the
+          // dest to the number of bytes left in this block.
+          BranchSize = -MBBStartOffset;
+
+          for (unsigned i = MBB.getNumber(), e = Dest->getNumber(); i != e; ++i)
+            BranchSize += BlockSizes[i];
+        }
+
+        // If this branch is in range, ignore it.
+        if (isInt<12>(BranchSize)) {
+          MBBStartOffset += 4;
+          continue;
+        }
+
+        // Otherwise, we have to expand it to a long branch.
+        MachineInstr *OldBranch = I;
+        DebugLoc dl = OldBranch->getDebugLoc();
+ 
+        TII->ReverseBranchCondition(Cond);
+        TII->InsertConstBranchAtInst(MBB, I, 8, Cond, dl);
+
+        // Uncond branch to the real destination.
+        I = BuildMI(MBB, I, dl, TII->get(RISCV::J)).addMBB(Dest);
+
+        // Remove the old branch from the function.
+        OldBranch->eraseFromParent();
+        
+        // Remember that this instruction is 8-bytes, increase the size of the
+        // block by 4, remember to iterate.
+        BlockSizes[MBB.getNumber()] += 4;
+        MBBStartOffset += 8;
+        ++NumExpanded;
+        MadeChange = true;
+      }
+    }
+    EverMadeChange |= MadeChange;
+  }
+  
+  BlockSizes.clear();
+  return true;
+}
+
diff --git a/lib/Target/RISCV/RISCVCallingConv.h b/lib/Target/RISCV/RISCVCallingConv.h
new file mode 100644
index 0000000..9ae515c
--- /dev/null
+++ b/lib/Target/RISCV/RISCVCallingConv.h
@@ -0,0 +1,21 @@
+//===-- RISCVCallingConv.h - Calling conventions for RISCV ------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVCALLINGCONV_H
+#define LLVM_LIB_TARGET_RISCV_RISCVCALLINGCONV_H
+
+namespace llvm {
+  namespace RISCV {
+    const unsigned NumArgGPRs = 8;
+
+    const unsigned NumArgFPRs = 8;
+  }
+}
+
+#endif
diff --git a/lib/Target/RISCV/RISCVCallingConv.td b/lib/Target/RISCV/RISCVCallingConv.td
new file mode 100644
index 0000000..b79e00d
--- /dev/null
+++ b/lib/Target/RISCV/RISCVCallingConv.td
@@ -0,0 +1,166 @@
+//===- RISCVCallingConv.td - Calling conventions for RISCV -*- tablegen -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+// This describes the calling conventions for the RISCV ABI.
+//===----------------------------------------------------------------------===//
+
+class CCIfExtend<CCAction A>
+  : CCIf<"ArgFlags.isSExt() || ArgFlags.isZExt()", A>;
+
+/// CCIfSubtarget - Match if the current subtarget has a feature F.
+class CCIfSubtarget<string F, CCAction A>
+    : CCIf<!strconcat("static_cast<const RISCVSubtarget&>"
+		      "(State.getMachineFunction().getSubtarget()).",
+		     F),
+          A>;
+
+//===----------------------------------------------------------------------===//
+// RV32 return value calling convention
+//===----------------------------------------------------------------------===//
+def RetCC_RISCV32 : CallingConv<[
+
+  //First two return values go in a0,a1
+  CCIfType<[i32], CCAssignToReg<[a0, a1]>>,
+
+  //Floating points go in their respective fa0,fa1
+  CCIfType<[f32], CCIfSubtarget<"hasF()", CCAssignToReg<[fa0, fa1]>>>,
+  CCIfType<[f32], CCIfSubtarget<"hasD()", CCPromoteToType<f64>>>,
+  CCIfType<[f64], CCIfSubtarget<"hasD()", CCAssignToReg<[fa0_64, fa1_64]>>>,
+  CCIfType<[f64], CCIfSubtarget<"hasF()", CCAssignToReg<[fa0_p64]>>>,
+
+  CCIfType<[f32], CCAssignToReg<[a0, a1]>>
+  //Falling off the end of allocation here leads to SRet demotion
+]>;
+
+//===----------------------------------------------------------------------===//
+// RV32 argument calling conventions
+//===----------------------------------------------------------------------===//
+def CC_RISCV32 : CallingConv<[
+  //Always assign the sret pointer to the first arg reg
+  CCIfSRet<CCAssignToReg<[a0]>>,
+  //Promote small int types to i32
+  CCIfType<[i8,i16], CCPromoteToType<i32>>,
+
+  // The first 8 integer arguments are passed in a0-a7
+  CCIfType<[i32], CCAssignToRegWithShadow<
+                   [a0,   a1,  a2,  a3,  a4,  a5,  a6,  a7],
+                   [fa0, fa1, fa2, fa3, fa4, fa5, fa6, fa7]>>,
+
+  CCIfType<[i64], CCAssignToRegWithShadow<
+                   [a0_p64,   a1_p64,  a2_p64,  a3_p64],
+                   [fa0_p64, fa1_p64, fa2_p64, fa3_p64]>>,
+
+  //Single precision floating point
+  CCIfType<[f32], CCIfSubtarget<"hasF()", CCAssignToRegWithShadow<
+                     [fa0, fa1, fa2, fa3, fa4, fa5, fa6, fa7],
+                     [a0 ,  a1,  a2,  a3,  a4,  a5,  a6,  a7]>>>,
+  //double precision floating point
+  CCIfType<[f32, f64], CCIfSubtarget<"hasD()", CCAssignToRegWithShadow<
+        [fa0_64, fa1_64, fa2_64, fa3_64, fa4_64, fa5_64, fa6_64, fa7_64],
+        [ a0,     a1,     a2,     a3,     a4,     a5,     a6,     a7   ]>>>,
+  //double precision with no D
+  CCIfType<[f64], CCIfSubtarget<"hasF()", CCAssignToRegWithShadow<
+                     [fa0_p64, fa1_p64, fa2_p64, fa3_p64],
+                     [a0_p64 ,  a1_p64,  a2_p64,  a3_p64]>>>,
+
+  // Other arguments are passed in 8-byte-aligned 8-byte stack slots.
+  CCIfType<[i32, i64, f32, f64], CCAssignToStack<8, 8>>
+]>;
+//Var args are all passed in integer regs
+def CC_RISCV32_VAR : CallingConv<[
+  //Always assign the sret pointer to the first arg reg
+  CCIfSRet<CCAssignToReg<[a0]>>,
+  //Promote small int types to i32
+  CCIfType<[i8,i16], CCPromoteToType<i32>>,
+
+  // The first 8 arguments are passed in a0-a7.
+  CCIfType<[i32,f32], CCAssignToRegWithShadow<
+                   [a0,   a1,  a2,  a3,  a4,  a5,  a6,  a7],
+                   [fa0, fa1, fa2, fa3, fa4, fa5, fa6, fa7]>>,
+
+  // Other arguments are passed in 8-byte-aligned 8-byte stack slots.
+  CCIfType<[i32, i64, f32, f64], CCAssignToStack<8, 8>>
+]>;
+
+//===----------------------------------------------------------------------===//
+// RV64 return value calling convention
+//===----------------------------------------------------------------------===//
+def RetCC_RISCV64 : CallingConv<[
+  CCIfType<[i8,i16,i32], CCPromoteToType<i64>>,
+
+  //return the values in the correct size register class if possible
+  CCIfType<[i64], CCAssignToReg<[a0_64, a1_64]>>,
+
+  //use the best register class for floats as well
+  CCIfType<[f32], CCIfSubtarget<"hasF()", CCAssignToReg<[fa0, fa1]>>>,
+  CCIfType<[f32], CCIfSubtarget<"hasD()", CCPromoteToType<f64>>>,
+  CCIfType<[f64], CCIfSubtarget<"hasD()", CCAssignToReg<[fa0_64, fa1_64]>>>,
+
+  CCIfType<[f32], CCPromoteToType<f64>>,
+  CCIfType<[f64], CCAssignToReg<[a0_64, a1_64]>>
+
+  //Falling off the end of allocation here leads to SRet demotion
+]>;
+
+//===----------------------------------------------------------------------===//
+// RV64 argument calling conventions
+//===----------------------------------------------------------------------===//
+def CC_RISCV64 : CallingConv<[
+  //Always assign the sret pointer to the first arg reg
+  CCIfSRet<CCAssignToReg<[a0_64]>>,
+  //Promote small int types to i32
+  CCIfType<[i8, i16, i32], CCPromoteToType<i64>>,
+
+  // The first 8 integer arguments are passed in a0-a7
+  CCIfType<[i64], CCAssignToRegWithShadow<
+      [ a0_64, a1_64, a2_64, a3_64, a4_64, a5_64, a6_64, a7_64],
+      [fa0_64,fa1_64,fa2_64,fa3_64,fa4_64,fa5_64,fa6_64,fa7_64]>>,
+
+  //Single precision floating point
+  CCIfType<[f32], CCIfSubtarget<"hasF()", CCAssignToRegWithShadow<
+      [fa0, fa1, fa2, fa3, fa4, fa5, fa6, fa7],
+      [ a0,  a1,  a2,  a3,  a4,  a5,  a6,  a7]>>>,
+  //double precision floating point
+  CCIfType<[f32, f64], CCIfSubtarget<"hasD()", CCAssignToRegWithShadow<
+      [fa0_64, fa1_64, fa2_64, fa3_64, fa4_64, fa5_64, fa6_64, fa7_64],
+      [ a0_64,  a1_64,  a2_64,  a3_64,  a4_64,  a5_64,  a6_64,  a7_64]>>>,
+
+  // Other arguments are passed in 8-byte-aligned 8-byte stack slots.
+  CCIfType<[i32, i64, f32, f64], CCAssignToStack<8, 8>>
+]>;
+
+def CC_RISCV64_VAR : CallingConv<[
+  //Always assign the sret pointer to the first arg reg
+  CCIfSRet<CCAssignToReg<[a0_64]>>,
+  //Promote small int types to i64
+  CCIfType<[i8, i16, i32], CCPromoteToType<i64>>,
+
+  // The first 14 integer arguments are passed in a0-a13. 
+  CCIfType<[i64,f64], CCAssignToRegWithShadow<
+      [ a0_64, a1_64, a2_64, a3_64, a4_64, a5_64, a6_64, a7_64],
+      [fa0_64,fa1_64,fa2_64,fa3_64,fa4_64,fa5_64,fa6_64,fa7_64]>>,
+
+  // Other arguments are passed in 8-byte-aligned 8-byte stack slots.
+  CCIfType<[i32, i64, f32, f64], CCAssignToStack<8, 8>>
+]>;
+
+//===----------------------------------------------------------------------===//
+// Callee-saved register lists.
+//===----------------------------------------------------------------------===//
+
+def CSR_RV32  : CalleeSavedRegs<(add ra, sp, fp, tp, gp, (sequence "s%u", 11, 0))>;
+def CSR_RV32F : CalleeSavedRegs<(add (sequence "fs%u", 11, 0), ra, sp, fp, tp, gp,
+                                     (sequence "s%u", 11, 0))>;
+def CSR_RV32D : CalleeSavedRegs<(add (sequence "fs%u_64", 11, 0), ra, sp, fp, tp, gp,
+                                     (sequence "s%u", 11, 0))>;
+
+def CSR_RV64  : CalleeSavedRegs<(add ra_64, sp_64, fp_64, tp_64, gp_64, (sequence "s%u_64", 11, 0))>;
+def CSR_RV64F : CalleeSavedRegs<(add (sequence "fs%u", 11, 0), ra_64, sp_64, fp_64, tp_64, gp_64,
+                                     (sequence "s%u_64", 11, 0))>;
+def CSR_RV64D : CalleeSavedRegs<(add (sequence "fs%u_64", 11, 0), ra_64, sp_64, fp_64, tp_64, gp_64,
+                                     (sequence "s%u_64", 11, 0))>;
diff --git a/lib/Target/RISCV/RISCVConstantPoolValue.cpp b/lib/Target/RISCV/RISCVConstantPoolValue.cpp
new file mode 100644
index 0000000..85b39c2
--- /dev/null
+++ b/lib/Target/RISCV/RISCVConstantPoolValue.cpp
@@ -0,0 +1,62 @@
+//===-- RISCVConstantPoolValue.cpp - RISCV constant-pool value --*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVConstantPoolValue.h"
+#include "llvm/ADT/FoldingSet.h"
+#include "llvm/IR/DerivedTypes.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+RISCVConstantPoolValue::
+RISCVConstantPoolValue(const GlobalValue *gv,
+                         RISCVCP::RISCVCPModifier modifier)
+  : MachineConstantPoolValue(gv->getType()), GV(gv), Modifier(modifier) {}
+
+RISCVConstantPoolValue *
+RISCVConstantPoolValue::Create(const GlobalValue *GV,
+                                 RISCVCP::RISCVCPModifier Modifier) {
+  return new RISCVConstantPoolValue(GV, Modifier);
+}
+
+unsigned RISCVConstantPoolValue::getRelocationInfo() const {
+  switch (Modifier) {
+  case RISCVCP::NTPOFF:
+    // May require a relocation, but the relocations are always resolved
+    // by the static linker.
+    return 1;
+  }
+  llvm_unreachable("Unknown modifier");
+}
+
+int RISCVConstantPoolValue::
+getExistingMachineCPValue(MachineConstantPool *CP, unsigned Alignment) {
+  unsigned AlignMask = Alignment - 1;
+  const std::vector<MachineConstantPoolEntry> Constants = CP->getConstants();
+  for (unsigned I = 0, E = Constants.size(); I != E; ++I) {
+    if (Constants[I].isMachineConstantPoolEntry() &&
+        (Constants[I].getAlignment() & AlignMask) == 0) {
+      RISCVConstantPoolValue *RCPV =
+        static_cast<RISCVConstantPoolValue *>(Constants[I].Val.MachineCPVal);
+      if (RCPV->GV == GV && RCPV->Modifier == Modifier)
+        return I;
+    }
+  }
+  return -1;
+}
+
+void RISCVConstantPoolValue::addSelectionDAGCSEId(FoldingSetNodeID &ID) {
+  ID.AddPointer(GV);
+  ID.AddInteger(Modifier);
+}
+
+void RISCVConstantPoolValue::print(raw_ostream &O) const {
+  O << GV ;//<< "@" << int(Modifier);
+}
diff --git a/lib/Target/RISCV/RISCVConstantPoolValue.h b/lib/Target/RISCV/RISCVConstantPoolValue.h
new file mode 100644
index 0000000..002e97d
--- /dev/null
+++ b/lib/Target/RISCV/RISCVConstantPoolValue.h
@@ -0,0 +1,55 @@
+//===- RISCVConstantPoolValue.h - RISCV constant-pool value -----*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVCONSTANTPOOLVALUE_H
+#define LLVM_LIB_TARGET_RISCV_RISCVCONSTANTPOOLVALUE_H
+
+#include "llvm/CodeGen/MachineConstantPool.h"
+#include "llvm/Support/ErrorHandling.h"
+
+namespace llvm {
+
+class GlobalValue;
+
+namespace RISCVCP {
+  enum RISCVCPModifier {
+    NTPOFF
+  };
+}
+
+/// A RISCV-specific constant pool value.  At present, the only
+/// defined constant pool values are offsets of thread-local variables
+/// (written x@NTPOFF).
+class RISCVConstantPoolValue : public MachineConstantPoolValue {
+  const GlobalValue *GV;
+  RISCVCP::RISCVCPModifier Modifier;
+
+protected:
+  RISCVConstantPoolValue(const GlobalValue *GV,
+                           RISCVCP::RISCVCPModifier Modifier);
+
+public:
+  static RISCVConstantPoolValue *
+    Create(const GlobalValue *GV, RISCVCP::RISCVCPModifier Modifier);
+
+  // Override MachineConstantPoolValue.
+  unsigned getRelocationInfo() const override;
+  int getExistingMachineCPValue(MachineConstantPool *CP,
+                                unsigned Alignment) override;
+  void addSelectionDAGCSEId(FoldingSetNodeID &ID) override;
+  void print(raw_ostream &O) const override;
+
+  // Access RISCV-specific fields.
+  const GlobalValue *getGlobalValue() const { return GV; }
+  RISCVCP::RISCVCPModifier getModifier() const { return Modifier; }
+};
+
+} // End llvm namespace
+
+#endif
diff --git a/lib/Target/RISCV/RISCVFrameLowering.cpp b/lib/Target/RISCV/RISCVFrameLowering.cpp
new file mode 100644
index 0000000..b34095c
--- /dev/null
+++ b/lib/Target/RISCV/RISCVFrameLowering.cpp
@@ -0,0 +1,319 @@
+//===-- RISCVFrameLowering.cpp - Frame lowering for RISCV -------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVFrameLowering.h"
+#include "RISCVCallingConv.h"
+#include "RISCVInstrBuilder.h"
+#include "RISCVInstrInfo.h"
+#include "RISCVMachineFunctionInfo.h"
+#include "RISCVSubtarget.h"
+#include "llvm/CodeGen/MachineModuleInfo.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/RegisterScavenging.h"
+#include "llvm/IR/Function.h"
+
+using namespace llvm;
+
+RISCVFrameLowering::RISCVFrameLowering()
+    : TargetFrameLowering(TargetFrameLowering::StackGrowsDown, 16, 0) {}
+/*   RISCV stack frames look like:
+
+    +-------------------------------+
+    |  incoming stack arguments     |
+    +-------------------------------+
+  A |  caller-allocated save area   |
+    |  for register arguments       |
+    +-------------------------------+ <-- incoming stack pointer
+  B |  callee-allocated save area   |
+    |  for arguments that are       |
+    |  split between registers and  |
+    |  the stack                    |
+    +-------------------------------+ <-- arg_pointer_rtx
+  C |  callee-allocated save area   |
+    |  for register varargs         |
+    +-------------------------------+ <-- hard_frame_pointer_rtx;
+    |                               |     stack_pointer_rtx + gp_sp_offset
+    |  GPR save area                |       + UNITS_PER_WORD
+    +-------------------------------+ <-- stack_pointer_rtx + fp_sp_offset
+    |                               |       + UNITS_PER_HWVALUE
+    |  FPR save area                |
+    +-------------------------------+ <-- frame_pointer_rtx (virtual)
+    |  local variables              |
+  P +-------------------------------+
+    |  outgoing stack arguments     |
+    +-------------------------------+
+    |  caller-allocated save area   |
+    |  for register arguments       |
+    +-------------------------------+ <-- stack_pointer_rtx
+
+   At least two of A, B and C will be empty.
+
+   Dynamic stack allocations such as alloca insert data at point P.
+   They decrease stack_pointer_rtx but leave frame_pointer_rtx and
+   hard_frame_pointer_rtx unchanged.  */
+
+
+// hasFP - Return true if the specified function should have a dedicated frame
+// pointer register.  This is true if the function has variable sized allocas or
+// if frame pointer elimination is disabled.
+bool RISCVFrameLowering::hasFP(const MachineFunction &MF) const {
+  const MachineFrameInfo *MFI = MF.getFrameInfo();
+  return MF.getTarget().Options.DisableFramePointerElim(MF) ||
+      MFI->hasVarSizedObjects() || MFI->isFrameAddressTaken();
+}
+
+
+unsigned RISCVFrameLowering::ehDataReg(unsigned I) const {
+  static const unsigned EhDataReg[] = {
+    RISCV::sup0, RISCV::sup1
+  };
+
+  return EhDataReg[I];
+}
+
+void RISCVFrameLowering::emitPrologue(MachineFunction &MF, MachineBasicBlock &MBB) const {
+  assert(&MBB == &MF.front() && "Shrink-wrapping not yet implemented");
+  MachineFrameInfo *MFI    = MF.getFrameInfo();
+  RISCVFunctionInfo *RISCVFI = MF.getInfo<RISCVFunctionInfo>();
+  const RISCVRegisterInfo *RegInfo =
+    static_cast<const RISCVRegisterInfo*>(MF.getSubtarget().getRegisterInfo());
+  const RISCVInstrInfo &TII =
+    *static_cast<const RISCVInstrInfo*>(MF.getSubtarget().getInstrInfo());
+  MachineBasicBlock::iterator MBBI = MBB.begin();
+  DebugLoc dl = MBBI != MBB.end() ? MBBI->getDebugLoc() : DebugLoc();
+  const RISCVSubtarget &STI = MF.getSubtarget<RISCVSubtarget>();
+  unsigned SP = STI.isRV64() ? RISCV::sp_64 : RISCV::sp;
+  unsigned FP = STI.isRV64() ? RISCV::fp_64 : RISCV::fp;
+  unsigned ZERO = STI.isRV64() ? RISCV::zero_64 : RISCV::zero;
+  unsigned ADDu = STI.isRV64() ? RISCV::ADD64 : RISCV::ADD;
+
+  // First, compute final stack size.
+  uint64_t StackSize = MFI->getStackSize();
+
+  // No need to allocate space on the stack.
+  if (StackSize == 0 && !MFI->adjustsStack()) return;
+
+  MachineModuleInfo &MMI = MF.getMMI();
+  const MCRegisterInfo *MRI = MMI.getContext().getRegisterInfo();
+  MachineLocation DstML, SrcML;
+
+  // Adjust stack.
+  TII.adjustStackPtr(SP, -StackSize, MBB, MBBI);
+
+  // emit ".cfi_def_cfa_offset StackSize"
+  unsigned CFIIndex = MMI.addFrameInst(
+      MCCFIInstruction::createDefCfaOffset(nullptr, -StackSize));
+  BuildMI(MBB, MBBI, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+      .addCFIIndex(CFIIndex);
+
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+
+  if (CSI.size()) {
+    // Find the instruction past the last instruction that saves a callee-saved
+    // register to the stack.
+    for (unsigned i = 0; i < CSI.size(); ++i)
+      ++MBBI;
+
+    // Iterate over list of callee-saved registers and emit .cfi_offset
+    // directives.
+    for (const auto &I: CSI) {
+      int64_t Offset = MFI->getObjectOffset(I.getFrameIdx());
+      unsigned Reg = I.getReg();
+
+      // Reg is either in CPURegs or FGR32.
+      unsigned CFIIndex = MMI.addFrameInst(MCCFIInstruction::createOffset(
+          nullptr, MRI->getDwarfRegNum(Reg, 1), Offset));
+      BuildMI(MBB, MBBI, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+          .addCFIIndex(CFIIndex);
+    }
+  }
+
+  if (RISCVFI->getCallsEhReturn()) {
+    const TargetRegisterClass *RC = &RISCV::GR32BitRegClass;
+
+    // Insert instructions that spill eh data registers.
+    for (int I = 0; I < 4; ++I) {
+      if (!MBB.isLiveIn(ehDataReg(I)))
+        MBB.addLiveIn(ehDataReg(I));
+      TII.storeRegToStackSlot(MBB, MBBI, ehDataReg(I), false,
+                              RISCVFI->getEhDataRegFI(I), RC, RegInfo);
+    }
+
+    // Emit .cfi_offset directives for eh data registers.
+    for (int I = 0; I < 4; ++I) {
+      int64_t Offset = MFI->getObjectOffset(RISCVFI->getEhDataRegFI(I));
+      unsigned Reg = MRI->getDwarfRegNum(ehDataReg(I), true);
+      unsigned CFIIndex = MMI.addFrameInst(
+          MCCFIInstruction::createOffset(nullptr, Reg, Offset));
+      BuildMI(MBB, MBBI, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+          .addCFIIndex(CFIIndex);
+    }
+  }
+
+  // if framepointer enabled, set it to point to the stack pointer.
+  if (hasFP(MF)) {
+    // Insert instruction "move $fp, $sp" at this location.
+    BuildMI(MBB, MBBI, dl, TII.get(ADDu), FP)
+        .addReg(SP)
+        .addReg(ZERO)
+        .setMIFlag(MachineInstr::FrameSetup);
+
+    // emit ".cfi_def_cfa_register $fp"
+    unsigned CFIIndex = MMI.addFrameInst(MCCFIInstruction::createDefCfaRegister(
+        nullptr, MRI->getDwarfRegNum(FP, true)));
+    BuildMI(MBB, MBBI, dl, TII.get(TargetOpcode::CFI_INSTRUCTION))
+        .addCFIIndex(CFIIndex);
+  }
+}
+
+void RISCVFrameLowering::emitEpilogue(MachineFunction &MF,
+                                       MachineBasicBlock &MBB) const {
+  MachineBasicBlock::iterator MBBI = MBB.getLastNonDebugInstr();
+  MachineFrameInfo *MFI            = MF.getFrameInfo();
+  RISCVFunctionInfo *RISCVFI = MF.getInfo<RISCVFunctionInfo>();
+  const RISCVRegisterInfo *RegInfo =
+    static_cast<const RISCVRegisterInfo*>(MF.getSubtarget().getRegisterInfo());
+  const RISCVInstrInfo &TII =
+    *static_cast<const RISCVInstrInfo*>(MF.getSubtarget().getInstrInfo());
+  DebugLoc dl = MBBI->getDebugLoc();
+  const RISCVSubtarget &STI = MF.getSubtarget<RISCVSubtarget>();
+  unsigned SP   = STI.isRV64() ? RISCV::sp_64 : RISCV::sp;
+  unsigned FP   = STI.isRV64() ? RISCV::fp_64 : RISCV::fp;
+  unsigned ZERO = STI.isRV64() ? RISCV::zero_64 : RISCV::zero;
+  unsigned ADDu = STI.isRV64() ? RISCV::ADD64 : RISCV::ADD;
+
+  // if framepointer enabled, restore the stack pointer.
+  if (hasFP(MF)) {
+    // Find the first instruction that restores a callee-saved register.
+    MachineBasicBlock::iterator I = MBBI;
+
+    for (unsigned i = 0; i < MFI->getCalleeSavedInfo().size(); ++i)
+      --I;
+
+    // Insert instruction "move $sp, $fp" at this location.
+    BuildMI(MBB, I, dl, TII.get(ADDu), SP).addReg(FP).addReg(ZERO);
+  }
+
+  if (RISCVFI->getCallsEhReturn()) {
+    const TargetRegisterClass *RC = &RISCV::GR32BitRegClass;
+
+    // Find first instruction that restores a callee-saved register.
+    MachineBasicBlock::iterator I = MBBI;
+    for (unsigned i = 0; i < MFI->getCalleeSavedInfo().size(); ++i)
+      --I;
+
+    // Insert instructions that restore eh data registers.
+    for (int J = 0; J < 4; ++J) {
+      TII.loadRegFromStackSlot(MBB, I, ehDataReg(J), RISCVFI->getEhDataRegFI(J),
+                               RC, RegInfo);
+    }
+  }
+
+  // Get the number of bytes from FrameInfo
+  uint64_t StackSize = MFI->getStackSize();
+
+  if (!StackSize)
+    return;
+
+  // Adjust stack.
+  TII.adjustStackPtr(SP, StackSize, MBB, MBBI);
+}
+
+bool RISCVFrameLowering::
+spillCalleeSavedRegisters(MachineBasicBlock &MBB,
+                          MachineBasicBlock::iterator MI,
+                          const std::vector<CalleeSavedInfo> &CSI,
+                          const TargetRegisterInfo *TRI) const {
+  MachineFunction *MF = MBB.getParent();
+  MachineBasicBlock *EntryBlock = MF->begin();
+  const TargetInstrInfo &TII = *MF->getSubtarget().getInstrInfo();
+
+  for (unsigned i = 0, e = CSI.size(); i != e; ++i) {
+    // Add the callee-saved register as live-in. Do not add if the register is
+    // RA and return address is taken, because it has already been added in
+    // method RISCVTargetLowering::LowerRETURNADDR.
+    // It's killed at the spill, unless the register is RA and return address
+    // is taken.
+    unsigned Reg = CSI[i].getReg();
+    bool IsRAAndRetAddrIsTaken = (Reg == RISCV::ra || Reg == RISCV::ra_64)
+        && MF->getFrameInfo()->isReturnAddressTaken();
+    if (!IsRAAndRetAddrIsTaken)
+      EntryBlock->addLiveIn(Reg);
+
+    // Insert the spill to the stack frame.
+    bool IsKill = !IsRAAndRetAddrIsTaken;
+    const TargetRegisterClass *RC = TRI->getMinimalPhysRegClass(Reg);
+    TII.storeRegToStackSlot(*EntryBlock, MI, Reg, IsKill,
+                            CSI[i].getFrameIdx(), RC, TRI);
+  }
+
+  return true;
+}
+
+bool
+RISCVFrameLowering::hasReservedCallFrame(const MachineFunction &MF) const {
+  const MachineFrameInfo *MFI = MF.getFrameInfo();
+
+  // Reserve call frame if the size of the maximum call frame fits into 16-bit
+  // immediate field and there are no variable sized objects on the stack.
+  // Make sure the second register scavenger spill slot can be accessed with one
+  // instruction.
+  return isInt<12>(MFI->getMaxCallFrameSize() + getStackAlignment()) &&
+    !MFI->hasVarSizedObjects();
+}
+
+// Eliminate ADJCALLSTACKDOWN, ADJCALLSTACKUP pseudo instructions
+void RISCVFrameLowering::
+eliminateCallFramePseudoInstr(MachineFunction &MF, MachineBasicBlock &MBB,
+                              MachineBasicBlock::iterator I) const {
+  const RISCVInstrInfo &TII =
+    *static_cast<const RISCVInstrInfo*>(MF.getSubtarget().getInstrInfo());
+  const RISCVSubtarget &STI = MF.getSubtarget<RISCVSubtarget>();
+
+  if (!hasReservedCallFrame(MF)) {
+    int64_t Amount = I->getOperand(0).getImm();
+
+    if (I->getOpcode() == RISCV::ADJCALLSTACKDOWN)
+      Amount = -Amount;
+
+    unsigned SP = STI.isRV64() ? RISCV::sp_64 : RISCV::sp;
+    TII.adjustStackPtr(SP, Amount, MBB, I);
+  }
+
+  MBB.erase(I);
+}
+
+void RISCVFrameLowering::determineCalleeSaves(MachineFunction &MF, BitVector &SavedRegs,
+                                     RegScavenger *RS) const {
+  TargetFrameLowering::determineCalleeSaves(MF, SavedRegs, RS);
+  MachineFrameInfo *MFI = MF.getFrameInfo();
+  RISCVFunctionInfo *RISCVFI = MF.getInfo<RISCVFunctionInfo>();
+  const RISCVSubtarget &STI = MF.getSubtarget<RISCVSubtarget>();
+  unsigned FP = STI.isRV64() ? RISCV::fp_64 : RISCV::fp;
+
+  // Mark $fp as used if function has dedicated frame pointer.
+  if (hasFP(MF))
+    SavedRegs.set(FP);
+
+  // Create spill slots for eh data registers if function calls eh_return.
+  if (RISCVFI->getCallsEhReturn())
+    RISCVFI->createEhDataRegsFI();
+
+  // Set scavenging frame index if necessary.
+  uint64_t MaxSPOffset = MF.getInfo<RISCVFunctionInfo>()->getIncomingArgSize() +
+    MFI->estimateStackSize(MF);
+
+  if (isInt<12>(MaxSPOffset))
+    return;
+
+  const TargetRegisterClass *RC = &RISCV::GR32BitRegClass;
+  int FI = MF.getFrameInfo()->CreateStackObject(RC->getSize(),
+                                                RC->getAlignment(), false);
+  RS->addScavengingFrameIndex(FI);
+}
diff --git a/lib/Target/RISCV/RISCVFrameLowering.h b/lib/Target/RISCV/RISCVFrameLowering.h
new file mode 100644
index 0000000..0937226
--- /dev/null
+++ b/lib/Target/RISCV/RISCVFrameLowering.h
@@ -0,0 +1,48 @@
+//===-- RISCVFrameLowering.h - Frame lowering for RISCV ---------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVFRAMELOWERING_H
+#define LLVM_LIB_TARGET_RISCV_RISCVFRAMELOWERING_H
+
+#include "llvm/Target/TargetFrameLowering.h"
+
+namespace llvm {
+class RISCVTargetMachine;
+class RISCVSubtarget;
+
+class RISCVFrameLowering : public TargetFrameLowering {
+public:
+  RISCVFrameLowering();
+
+  bool hasFP(const MachineFunction &MF) const;
+
+  /// emitProlog/emitEpilog - These methods insert prolog and epilog code into
+  /// the function.
+  void emitPrologue(MachineFunction&, MachineBasicBlock&) const;
+  void emitEpilogue(MachineFunction &MF, MachineBasicBlock &MBB) const;
+
+  void eliminateCallFramePseudoInstr(MachineFunction &MF,
+                                     MachineBasicBlock &MBB,
+                                     MachineBasicBlock::iterator I) const;
+
+  bool spillCalleeSavedRegisters(MachineBasicBlock &MBB,
+                                 MachineBasicBlock::iterator MI,
+                                 const std::vector<CalleeSavedInfo> &CSI,
+                                 const TargetRegisterInfo *TRI) const;
+
+  bool hasReservedCallFrame(const MachineFunction &MF) const;
+
+  void determineCalleeSaves(MachineFunction &MF, BitVector &SavedRegs,
+                                            RegScavenger *RS) const override;
+  unsigned ehDataReg(unsigned I) const;
+};
+
+} // End llvm namespace
+
+#endif
diff --git a/lib/Target/RISCV/RISCVISelDAGToDAG.cpp b/lib/Target/RISCV/RISCVISelDAGToDAG.cpp
new file mode 100644
index 0000000..3de5b1d
--- /dev/null
+++ b/lib/Target/RISCV/RISCVISelDAGToDAG.cpp
@@ -0,0 +1,445 @@
+//===-- RISCVISelDAGToDAG.cpp - A dag to dag inst selector for RISCV ------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines an instruction selector for the RISCV target.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVTargetMachine.h"
+#include "llvm/CodeGen/SelectionDAGISel.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+#define DEBUG_TYPE "riscv-isel"
+
+namespace {
+// Used to build addressing modes.
+struct RISCVAddressingMode {
+  // The shape of the address.
+  enum AddrForm {
+    // base+offset
+    FormBO
+  };
+  AddrForm Form;
+
+  // The type of displacement. 
+  enum OffRange {
+    Off12Only
+  };
+  OffRange OffR;
+
+  // The parts of the address.  The address is equivalent to:
+  //
+  //     Base + Offset + Index + (IncludesDynAlloc ? ADJDYNALLOC : 0)
+  SDValue Base;
+  int64_t Offset;
+
+  RISCVAddressingMode(AddrForm form, OffRange offr)
+    : Form(form), OffR(offr), Base(), Offset(0) {}
+
+  void dump() {
+    errs() << "RISCVAddressingMode " << this << '\n';
+
+    errs() << " Base ";
+    if (Base.getNode() != 0)
+      Base.getNode()->dump();
+    else
+      errs() << "null\n";
+
+    errs() << " Offset " << Offset;
+  }
+};
+
+class RISCVDAGToDAGISel : public SelectionDAGISel {
+  const RISCVTargetLowering &Lowering;
+  const RISCVSubtarget &Subtarget;
+
+  // Used by RISCVOperands.td to create integer constants.
+  inline SDValue getImm(const SDNode *Node, uint64_t Imm) {
+    return CurDAG->getTargetConstant(Imm, SDLoc(Node), Node->getValueType(0));
+  }
+  /// getI32Imm - Return a target constant with the specified value, of type
+  /// i32.
+  SDValue getI32Imm(unsigned Imm, SDLoc DL) {
+    return CurDAG->getTargetConstant(Imm, DL, MVT::i32);
+  }
+
+  // Try to fold more of the base or index of AM into AM, where IsBase
+  // selects between the base and index.
+  bool expandAddress(RISCVAddressingMode &AM, bool IsBase);
+
+  // Try to describe N in AM, returning true on success.
+  bool selectAddress(SDValue N, RISCVAddressingMode &AM);
+
+  // Extract individual target operands from matched address AM.
+  void getAddressOperands(const RISCVAddressingMode &AM, EVT VT,
+                          SDValue &Base, SDValue &Disp);
+  void getAddressOperands(const RISCVAddressingMode &AM, EVT VT,
+                          SDValue &Base, SDValue &Disp, SDValue &Index);
+
+  //RISCV
+  bool selectMemRegAddr(SDValue Addr, SDValue &Offset, SDValue &Base) {
+      
+    EVT ValTy = Addr.getValueType();
+
+    // if Address is FI, get the TargetFrameIndex.
+    if (FrameIndexSDNode *FIN = dyn_cast<FrameIndexSDNode>(Addr)) {
+      Base   = CurDAG->getTargetFrameIndex(FIN->getIndex(), ValTy);
+      Offset = CurDAG->getTargetConstant(0, SDLoc(Addr), ValTy);
+      return true;
+    }
+
+    if (TM.getRelocationModel() != Reloc::PIC_) {
+      if ((Addr.getOpcode() == ISD::TargetExternalSymbol ||
+          Addr.getOpcode() == ISD::TargetGlobalAddress))
+        return false;
+    }
+
+    // Addresses of the form FI+const or FI|const
+    if (CurDAG->isBaseWithConstantOffset(Addr)) {
+      ConstantSDNode *CN = dyn_cast<ConstantSDNode>(Addr.getOperand(1));
+      if (isInt<12>(CN->getSExtValue())) {
+  
+        // If the first operand is a FI, get the TargetFI Node
+        if (FrameIndexSDNode *FIN = dyn_cast<FrameIndexSDNode>
+                                    (Addr.getOperand(0)))
+          Base = CurDAG->getTargetFrameIndex(FIN->getIndex(), ValTy);
+        else
+          Base = Addr.getOperand(0);
+  
+        Offset = CurDAG->getTargetConstant(CN->getZExtValue(), SDLoc(Addr), ValTy);
+        return true;
+      }
+    }
+
+    //Last case
+    Base = Addr;
+    Offset = CurDAG->getTargetConstant(0, SDLoc(Addr), Addr.getValueType());
+    return true;
+  }
+
+  bool selectRegAddr(SDValue Addr, SDValue &Base) {
+    //always just register
+    Base = Addr;
+    return true;
+  }
+
+  bool replaceUsesWithZeroReg(MachineRegisterInfo *MRI,
+                              const MachineInstr& MI) {
+    unsigned DstReg = 0, ZeroReg = 0;
+  
+    // Check if MI is "addiu $dst, $zero, 0" or "daddiu $dst, $zero, 0".
+    if ((MI.getOpcode() == RISCV::ADDI) &&
+        (MI.getOperand(1).isReg()) && //avoid frame-index
+        (MI.getOperand(1).getReg() == RISCV::zero) &&
+        (MI.getOperand(2).getImm() == 0)) {
+      DstReg = MI.getOperand(0).getReg();
+      ZeroReg = RISCV::zero;
+    } else if ((MI.getOpcode() == RISCV::ADDI64) &&
+               (MI.getOperand(1).isReg()) && //avoid frame-index
+               (MI.getOperand(1).getReg() == RISCV::zero_64) &&
+               (MI.getOperand(2).getImm() == 0)) {
+      DstReg = MI.getOperand(0).getReg();
+      ZeroReg = RISCV::zero_64;
+    } else if ((MI.getOpcode() == RISCV::ADDIW) &&
+               (MI.getOperand(1).isReg()) && //avoid frame-index
+               (MI.getOperand(1).getReg() == RISCV::zero_64) &&
+               (MI.getOperand(2).getImm() == 0)) {
+      DstReg = MI.getOperand(0).getReg();
+      ZeroReg = RISCV::zero_64;
+    }
+  
+    if (!DstReg)
+      return false;
+  
+    // Replace uses with ZeroReg.
+    for (MachineRegisterInfo::use_iterator U = MRI->use_begin(DstReg),
+         E = MRI->use_end(); U != E;) {
+      MachineOperand &MO = *U;
+      unsigned OpNo = U.getOperandNo();
+      MachineInstr *MI = MO.getParent();
+      ++U;
+  
+      // Do not replace if it is a phi's operand or is tied to def operand.
+      if (MI->isPHI() || MI->isRegTiedToDefOperand(OpNo) || MI->isPseudo())
+        continue;
+  
+      MO.setReg(ZeroReg);
+    }
+  
+    return true;
+  }
+
+  //End RISCV
+
+  // PC-relative address matching routines used by RISCVOperands.td.
+  bool selectPCRelAddress(SDValue Addr, SDValue &Target) {
+    if (Addr.getOpcode() == RISCVISD::PCREL_WRAPPER) {
+      Target = Addr.getOperand(0);
+      return true;
+    }
+    return false;
+  }
+
+  // If Op0 is null, then Node is a constant that can be loaded using:
+  //
+  //   (Opcode UpperVal LowerVal)
+  //
+  // If Op0 is nonnull, then Node can be implemented using:
+  //
+  //   (Opcode (Opcode Op0 UpperVal) LowerVal)
+  SDNode *splitLargeImmediate(unsigned Opcode, SDNode *Node, SDValue Op0,
+                              uint64_t UpperVal, uint64_t LowerVal);
+
+public:
+  RISCVDAGToDAGISel(RISCVTargetMachine &TM, CodeGenOpt::Level OptLevel)
+    : SelectionDAGISel(TM, OptLevel),
+      Lowering(*TM.getSubtargetImpl()->getTargetLowering()),
+      Subtarget(*TM.getSubtargetImpl()) { }
+
+  // Override MachineFunctionPass.
+  const char *getPassName() const override {
+    return "RISCV DAG->DAG Pattern Instruction Selection";
+  }
+
+  // Override SelectionDAGISel.
+  virtual bool runOnMachineFunction(MachineFunction &MF);
+  SDNode *Select(SDNode *Node) override;
+  virtual void processFunctionAfterISel(MachineFunction &MF);
+  bool SelectInlineAsmMemoryOperand(const SDValue &Op, unsigned ConstraintID,
+                                    std::vector<SDValue> &OutOps) override;
+
+  // Include the pieces autogenerated from the target description.
+  #include "RISCVGenDAGISel.inc"
+};
+} // end anonymous namespace
+
+bool RISCVDAGToDAGISel::runOnMachineFunction(MachineFunction &MF) {
+  bool ret = SelectionDAGISel::runOnMachineFunction(MF);
+
+  processFunctionAfterISel(MF);
+
+  return ret;
+}
+
+FunctionPass *llvm::createRISCVISelDag(RISCVTargetMachine &TM,
+                                         CodeGenOpt::Level OptLevel) {
+  return new RISCVDAGToDAGISel(TM, OptLevel);
+}
+
+// Return true if Val should be selected as a displacement for an address
+// with range DR.  Here we're interested in the range of both the instruction
+// described by DR and of any pairing instruction.
+static bool selectOffset(RISCVAddressingMode::OffRange OffR, int64_t Val) {
+  switch (OffR) {
+  case RISCVAddressingMode::Off12Only:
+    return isInt<12>(Val);
+  }
+  llvm_unreachable("Unhandled offset range");
+}
+
+// The base or index of AM is equivalent to Op0 + Op1, where IsBase selects
+// between the base and index.  Try to fold Op1 into AM's displacement.
+static bool expandOffset(RISCVAddressingMode &AM, bool IsBase,
+                       SDValue Op0, ConstantSDNode *Op1) {
+  // First try adjusting the displacement.
+  int64_t TestOffset = AM.Offset + Op1->getSExtValue();
+  if (selectOffset(AM.OffR, TestOffset)) {
+    //changeComponent(AM, IsBase, Op0);
+    AM.Base = Op0;
+    AM.Offset = TestOffset;
+    return true;
+  }
+
+  // We could consider forcing the displacement into a register and
+  // using it as an index, but it would need to be carefully tuned.
+  return false;
+}
+
+bool RISCVDAGToDAGISel::expandAddress(RISCVAddressingMode &AM,
+                                        bool IsBase) {
+  //SDValue N = IsBase ? AM.Base : AM.Index;
+  SDValue N = AM.Base;
+  unsigned Opcode = N.getOpcode();
+  if (Opcode == ISD::TRUNCATE) {
+    N = N.getOperand(0);
+    Opcode = N.getOpcode();
+  }
+  if (Opcode == ISD::ADD || CurDAG->isBaseWithConstantOffset(N)) {
+    SDValue Op0 = N.getOperand(0);
+    SDValue Op1 = N.getOperand(1);
+
+    unsigned Op0Code = Op0->getOpcode();
+    unsigned Op1Code = Op1->getOpcode();
+
+    if (Op0Code == ISD::Constant)
+      return expandOffset(AM, IsBase, Op1, cast<ConstantSDNode>(Op0));
+    if (Op1Code == ISD::Constant)
+      return expandOffset(AM, IsBase, Op0, cast<ConstantSDNode>(Op1));
+
+  }
+  return false;
+}
+
+// Return true if an instruction with displacement range DR should be
+// used for displacement value Val.  selectDisp(DR, Val) must already hold.
+static bool isValidOffset(RISCVAddressingMode::OffRange OffR, int64_t Val) {
+  assert(selectOffset(OffR, Val) && "Invalid displacement");
+  switch (OffR) {
+  case RISCVAddressingMode::Off12Only:
+    return true;
+  }
+  llvm_unreachable("Unhandled displacement range");
+}
+
+// Return true if Addr is suitable for AM, updating AM if so.
+bool RISCVDAGToDAGISel::selectAddress(SDValue Addr,
+                                        RISCVAddressingMode &AM) {
+  // Start out assuming that the address will need to be loaded separately,
+  // then try to extend it as much as we can.
+  AM.Base = Addr;
+
+  // First try treating the address as a constant.
+  if (Addr.getOpcode() == ISD::Constant &&
+      expandOffset(AM, true, SDValue(), cast<ConstantSDNode>(Addr)))
+  { }
+
+  // Reject cases where the other instruction in a pair should be used.
+  if (!isValidOffset(AM.OffR, AM.Offset))
+    return false;
+
+  DEBUG(AM.dump());
+  return true;
+}
+
+// Insert a node into the DAG at least before Pos.  This will reposition
+// the node as needed, and will assign it a node ID that is <= Pos's ID.
+// Note that this does *not* preserve the uniqueness of node IDs!
+// The selection DAG must no longer depend on their uniqueness when this
+// function is used.
+static void insertDAGNode(SelectionDAG *DAG, SDNode *Pos, SDValue N) {
+  if (N.getNode()->getNodeId() == -1 ||
+      N.getNode()->getNodeId() > Pos->getNodeId()) {
+    DAG->RepositionNode(Pos, N.getNode());
+    N.getNode()->setNodeId(Pos->getNodeId());
+  }
+}
+
+void RISCVDAGToDAGISel::getAddressOperands(const RISCVAddressingMode &AM,
+                                             EVT VT, SDValue &Base,
+                                             SDValue &Offset) {
+  Base = AM.Base;
+  if (!Base.getNode())
+    // Register 0 means "no base".  This is mostly useful for shifts.
+    Base = CurDAG->getRegister(0, VT);
+  else if (Base.getOpcode() == ISD::FrameIndex) {
+    // Lower a FrameIndex to a TargetFrameIndex.
+    int64_t FrameIndex = cast<FrameIndexSDNode>(Base)->getIndex();
+    Offset = CurDAG->getTargetFrameIndex(FrameIndex, VT);
+    Base = CurDAG->getTargetConstant(AM.Offset, SDLoc(Base), VT);
+    return;
+  } else if (Base.getValueType() != VT) {
+    // Truncate values from i64 to i32, for shifts.
+    assert(VT == MVT::i32 && Base.getValueType() == MVT::i64 &&
+           "Unexpected truncation");
+    SDLoc DL(Base);
+    SDValue Trunc = CurDAG->getNode(ISD::TRUNCATE, DL, VT, Base);
+    insertDAGNode(CurDAG, Base.getNode(), Trunc);
+    Base = Trunc;
+  }
+
+  // Lower the displacement to a TargetConstant.
+  Offset = CurDAG->getTargetConstant(AM.Offset, SDLoc(Base), VT);
+}
+
+
+SDNode *RISCVDAGToDAGISel::splitLargeImmediate(unsigned Opcode, SDNode *Node,
+                                                 SDValue Op0, uint64_t UpperVal,
+                                                 uint64_t LowerVal) {
+  EVT VT = Node->getValueType(0);
+  SDLoc DL(Node);
+  SDValue Upper = CurDAG->getConstant(UpperVal, DL, VT);
+  if (Op0.getNode())
+    Upper = CurDAG->getNode(Opcode, DL, VT, Op0, Upper);
+  Upper = SDValue(Select(Upper.getNode()), 0);
+
+  SDValue Lower = CurDAG->getConstant(LowerVal, DL, VT);
+  SDValue Or = CurDAG->getNode(Opcode, DL, VT, Upper, Lower);
+  return Or.getNode();
+}
+
+SDNode *RISCVDAGToDAGISel::Select(SDNode *Node) {
+  SDLoc DL(Node);
+  // Dump information about the Node being selected
+  DEBUG(errs() << "Selecting: "; Node->dump(CurDAG); errs() << "\n");
+
+  // If we have a custom node, we already have selected!
+  if (Node->isMachineOpcode()) {
+    DEBUG(errs() << "== "; Node->dump(CurDAG); errs() << "\n");
+    return 0;
+  }
+
+  unsigned Opcode = Node->getOpcode();
+  switch (Opcode) {
+  case ISD::FrameIndex: {
+    SDValue imm = CurDAG->getTargetConstant(0, DL, Subtarget.isRV64() ? MVT::i64 : MVT::i32);
+    int FI = cast<FrameIndexSDNode>(Node)->getIndex();
+    SDValue TFI =
+        CurDAG->getTargetFrameIndex(FI, getTargetLowering()->getPointerTy(CurDAG->getDataLayout()));
+    unsigned Opc = Subtarget.isRV64() ? RISCV::ADDI64 : RISCV::ADDI;
+    EVT VT = Subtarget.isRV64() ? MVT::i64 : MVT::i32;
+    
+    if(Node->hasOneUse()) //don't create a new node just morph this one
+      return CurDAG->SelectNodeTo(Node, Opc, VT, TFI, imm);
+    return CurDAG->getMachineNode(Opc, DL, VT, TFI, imm);
+  }
+  }//end special selections
+
+  // Select the default instruction
+  SDNode *ResNode = SelectCode(Node);
+
+  DEBUG(errs() << "=> ";
+        if (ResNode == NULL || ResNode == Node)
+          Node->dump(CurDAG);
+        else
+          ResNode->dump(CurDAG);
+        errs() << "\n";
+        );
+  return ResNode;
+}
+
+bool RISCVDAGToDAGISel::
+SelectInlineAsmMemoryOperand(const SDValue &Op,
+                             unsigned ConstraintID,
+                             std::vector<SDValue> &OutOps) {
+  switch(ConstraintID) {
+  default:
+    llvm_unreachable("Unexpected asm memory constraint");
+  case InlineAsm::Constraint_m:
+
+    SDValue Base, Offset;
+    selectMemRegAddr(Op, Base, Offset);
+    OutOps.push_back(Base);
+    OutOps.push_back(Offset);
+    return false;
+  }
+  return false;
+}
+
+void RISCVDAGToDAGISel::processFunctionAfterISel(MachineFunction &MF) {
+
+  for (auto &MBB: MF)
+    for (auto &I: MBB) {
+      //replaceUsesWithZeroReg(MRI, *I);
+    }
+}
diff --git a/lib/Target/RISCV/RISCVISelLowering.cpp b/lib/Target/RISCV/RISCVISelLowering.cpp
new file mode 100644
index 0000000..81e6509
--- /dev/null
+++ b/lib/Target/RISCV/RISCVISelLowering.cpp
@@ -0,0 +1,1512 @@
+//===-- RISCVISelLowering.cpp - RISCV DAG lowering implementation ---------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file implements the RISCVTargetLowering class.
+//
+//===----------------------------------------------------------------------===//
+
+#define DEBUG_TYPE "riscv-lower"
+
+#include "RISCVISelLowering.h"
+#include "RISCVCallingConv.h"
+#include "RISCVConstantPoolValue.h"
+#include "RISCVMachineFunctionInfo.h"
+#include "RISCVSubtarget.h"
+#include "RISCVTargetMachine.h"
+#include "llvm/CodeGen/CallingConvLower.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+
+using namespace llvm;
+
+static const MCPhysReg RV32IntRegs[8] = {
+  RISCV::a0, RISCV::a1, RISCV::a2, RISCV::a3,
+  RISCV::a4, RISCV::a5, RISCV::a6, RISCV::a7
+};
+
+static const MCPhysReg RV64IntRegs[8] = {
+  RISCV::a0_64, RISCV::a1_64, RISCV::a2_64, RISCV::a3_64,
+  RISCV::a4_64, RISCV::a5_64, RISCV::a6_64, RISCV::a7_64
+};
+
+static const MCPhysReg FPFRegs[8] = {
+  RISCV::fa0, RISCV::fa1, RISCV::fa2, RISCV::fa3,
+  RISCV::fa4, RISCV::fa5, RISCV::fa6, RISCV::fa7
+};
+
+static const MCPhysReg FPDRegs[8] = {
+  RISCV::fa0_64, RISCV::fa1_64, RISCV::fa2_64, RISCV::fa3_64,
+  RISCV::fa4_64, RISCV::fa5_64, RISCV::fa6_64, RISCV::fa7_64
+};
+
+void RISCVTargetObjectFile::Initialize(MCContext &Ctx, const TargetMachine &TM) {
+  TargetLoweringObjectFileELF::Initialize(Ctx, TM);
+  InitializeELF(TM.Options.UseInitArray);
+}
+
+RISCVTargetLowering::RISCVTargetLowering(const TargetMachine &tm, 
+                                         const RISCVSubtarget &STI)
+    : TargetLowering(tm), Subtarget(STI), IsRV32(Subtarget.isRV32()) {
+  MVT PtrVT = Subtarget.isRV64() ? MVT::i64 : MVT::i32;
+  // Set up the register classes.
+  addRegisterClass(MVT::i32,  &RISCV::GR32BitRegClass);
+  if(Subtarget.isRV64())
+    addRegisterClass(MVT::i64,  &RISCV::GR64BitRegClass);
+  if(Subtarget.hasD()){
+    addRegisterClass(MVT::f64,  &RISCV::FP64BitRegClass);
+    addRegisterClass(MVT::f32,  &RISCV::FP32BitRegClass);
+  }else if(Subtarget.hasF())
+    addRegisterClass(MVT::f32,  &RISCV::FP32BitRegClass);
+
+
+  // Set up special registers.
+  if(Subtarget.isRV64()) {
+    setExceptionPointerRegister(RISCV::epc_64);
+    setExceptionSelectorRegister(RISCV::evec_64);
+    setStackPointerRegisterToSaveRestore(RISCV::sp_64);
+  }else {
+    setExceptionPointerRegister(RISCV::epc);
+    setExceptionSelectorRegister(RISCV::evec);
+    setStackPointerRegisterToSaveRestore(RISCV::sp);
+  }
+
+  setSchedulingPreference(Sched::RegPressure);
+
+  //For i1 types all bits are zero except bit 0
+  setBooleanContents(ZeroOrOneBooleanContent);
+  setBooleanVectorContents(ZeroOrOneBooleanContent); //vectors of i1s are the same
+
+  // Used by legalize types to correctly generate the setcc result.
+  AddPromotedToType(ISD::SETCC, MVT::i1, MVT::i32);
+
+  // Instructions are strings of 2-byte aligned 2-byte values.
+  // align by log2(2) bytes?
+  setMinFunctionAlignment(2);
+
+  // Handle operations that are handled in a similar way for all types.
+  for (unsigned I = MVT::FIRST_INTEGER_VALUETYPE;
+       I <= MVT::LAST_FP_VALUETYPE;
+       ++I) {
+    MVT VT = MVT::SimpleValueType(I);
+    if (isTypeLegal(VT)) {
+      // Lower SELECT_CC and BR_CC into separate comparisons and branches.
+      setOperationAction(ISD::SELECT_CC, VT, Expand);
+      setOperationAction(ISD::BR_CC,     VT, Expand);
+
+    }
+  }
+  if(Subtarget.isRV64()){
+    setOperationAction(ISD::SETCC, MVT::i32, Legal);//only use 32bit setcc
+    setOperationAction(ISD::Constant, MVT::i32, Legal);
+    setOperationAction(ISD::Constant, MVT::i64, Legal);
+  }else {
+    setOperationAction(ISD::SETCC, MVT::i32, Legal);//folds into brcond
+    setOperationAction(ISD::SETCC, MVT::i64, Expand);//only use 32bit
+    setOperationAction(ISD::Constant, MVT::i32, Legal);
+    setOperationAction(ISD::Constant, MVT::i64, Legal);
+  }
+
+
+  // Expand jump table branches as address arithmetic followed by an
+  // indirect jump.
+  setOperationAction(ISD::BR_JT, MVT::Other, Expand);
+  //RISCV also does not have indirect branch so expand them
+  //TODO: don't we have one via JALR?
+  setOperationAction(ISD::BRIND, MVT::Other, Expand);
+
+  //make BRCOND legal, its actually only legal for a subset of conds
+  setOperationAction(ISD::BRCOND, MVT::Other, Legal);
+
+  //Custom Lower Overflow operators
+
+  // Handle integer types.
+  for (unsigned I = MVT::FIRST_INTEGER_VALUETYPE;
+       I <= MVT::LAST_INTEGER_VALUETYPE;
+       ++I) {
+    MVT VT = MVT::SimpleValueType(I);
+    if (isTypeLegal(VT)) {
+      if(Subtarget.hasM()) {
+        if(Subtarget.isRV64() && VT==MVT::i32)
+          setOperationAction(ISD::MUL  , VT, Promote);
+        if(Subtarget.isRV32() && VT==MVT::i64)
+          setOperationAction(ISD::MUL  , VT, Expand);
+        setOperationAction(ISD::MUL  , VT, Legal);
+        setOperationAction(ISD::MULHS, VT, Legal);
+        setOperationAction(ISD::MULHU, VT, Legal);
+        setOperationAction(ISD::SDIV , VT, Legal);
+        setOperationAction(ISD::UDIV , VT, Legal);
+        setOperationAction(ISD::SREM , VT, Legal);
+        setOperationAction(ISD::UREM , VT, Legal);
+      }else{
+        setOperationAction(ISD::MUL  , VT, Expand);
+        setOperationAction(ISD::MULHS, VT, Expand);
+        setOperationAction(ISD::MULHU, VT, Expand);
+        setOperationAction(ISD::SDIV , VT, Expand);
+        setOperationAction(ISD::UDIV , VT, Expand);
+        setOperationAction(ISD::SREM , VT, Expand);
+        setOperationAction(ISD::UREM , VT, Expand);
+      }
+      //No support at all
+      setOperationAction(ISD::SDIVREM, VT, Expand);
+      setOperationAction(ISD::UDIVREM, VT, Expand);
+      //RISCV doesn't support  [ADD,SUB][E,C]
+      setOperationAction(ISD::ADDE, VT, Expand);
+      setOperationAction(ISD::SUBE, VT, Expand);
+      setOperationAction(ISD::ADDC, VT, Expand);
+      setOperationAction(ISD::SUBC, VT, Expand);
+      //RISCV doesn't support s[hl,rl,ra]_parts
+      setOperationAction(ISD::SHL_PARTS, VT, Expand);
+      setOperationAction(ISD::SRL_PARTS, VT, Expand);
+      setOperationAction(ISD::SRA_PARTS, VT, Expand);
+      //RISCV doesn't support rotl
+      setOperationAction(ISD::ROTL, VT, Expand);
+      setOperationAction(ISD::ROTR, VT, Expand);
+
+      // Expand ATOMIC_LOAD and ATOMIC_STORE using ATOMIC_CMP_SWAP.
+      // FIXME: probably much too conservative.
+      setOperationAction(ISD::ATOMIC_LOAD,  VT, Expand);
+      setOperationAction(ISD::ATOMIC_STORE, VT, Expand);
+
+      // No special instructions for these.
+      setOperationAction(ISD::CTPOP,           VT, Expand);
+      setOperationAction(ISD::CTTZ,            VT, Expand);
+      setOperationAction(ISD::CTLZ,            VT, Expand);
+      setOperationAction(ISD::CTTZ_ZERO_UNDEF, VT, Expand);
+      setOperationAction(ISD::CTLZ_ZERO_UNDEF, VT, Expand);
+
+    }
+  }
+
+  //to have the best chance and doing something good with fences custom lower them
+  setOperationAction(ISD::ATOMIC_FENCE,      MVT::Other, Custom);
+  //Some Atmoic ops are legal
+  if(Subtarget.hasA()) {
+    if(Subtarget.isRV64()) {
+      //push 32 bits up to 64
+      setOperationAction(ISD::ATOMIC_SWAP,      MVT::i32, Promote);
+      setOperationAction(ISD::ATOMIC_LOAD_ADD,  MVT::i32, Promote);
+      setOperationAction(ISD::ATOMIC_LOAD_AND,  MVT::i32, Promote);
+      setOperationAction(ISD::ATOMIC_LOAD_OR,   MVT::i32, Promote);
+      setOperationAction(ISD::ATOMIC_LOAD_XOR,  MVT::i32, Promote);
+      setOperationAction(ISD::ATOMIC_LOAD_MIN,  MVT::i32, Promote);
+      setOperationAction(ISD::ATOMIC_LOAD_MAX,  MVT::i32, Promote);
+      setOperationAction(ISD::ATOMIC_LOAD_UMIN, MVT::i32, Promote);
+      setOperationAction(ISD::ATOMIC_LOAD_UMAX, MVT::i32, Promote);
+      //Legal in RV64A
+      setOperationAction(ISD::ATOMIC_SWAP,      MVT::i64, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_ADD,  MVT::i64, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_AND,  MVT::i64, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_OR,   MVT::i64, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_XOR,  MVT::i64, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_MIN,  MVT::i64, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_MAX,  MVT::i64, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_UMIN, MVT::i64, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_UMAX, MVT::i64, Legal);
+      //These are not native instructions
+      setOperationAction(ISD::ATOMIC_CMP_SWAP,  MVT::i32, Expand);
+      setOperationAction(ISD::ATOMIC_CMP_SWAP,  MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_NAND, MVT::i32, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_SUB,  MVT::i32, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_NAND, MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_SUB,  MVT::i64, Expand);
+    } else {
+      //Legal in RV32A
+      setOperationAction(ISD::ATOMIC_SWAP,      MVT::i32, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_ADD,  MVT::i32, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_AND,  MVT::i32, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_OR,   MVT::i32, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_XOR,  MVT::i32, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_MIN,  MVT::i32, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_MAX,  MVT::i32, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_UMIN, MVT::i32, Legal);
+      setOperationAction(ISD::ATOMIC_LOAD_UMAX, MVT::i32, Legal);
+      //Expand 64 bit into 32?
+      setOperationAction(ISD::ATOMIC_SWAP,      MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_ADD,  MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_AND,  MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_OR,   MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_XOR,  MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_MIN,  MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_MAX,  MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_UMIN, MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_UMAX, MVT::i64, Expand);
+      //These are not native instructions
+      setOperationAction(ISD::ATOMIC_CMP_SWAP,  MVT::i32, Expand);
+      setOperationAction(ISD::ATOMIC_CMP_SWAP,  MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_NAND, MVT::i32, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_SUB,  MVT::i32, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_NAND, MVT::i64, Expand);
+      setOperationAction(ISD::ATOMIC_LOAD_SUB,  MVT::i64, Expand);
+    }
+  } else {
+    //No atomic ops so expand all
+    setOperationAction(ISD::ATOMIC_SWAP,      MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_ADD,  MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_AND,  MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_OR,   MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_XOR,  MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_MIN,  MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_MAX,  MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_UMIN, MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_UMAX, MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_SWAP,      MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_ADD,  MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_AND,  MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_OR,   MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_XOR,  MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_MIN,  MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_MAX,  MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_UMIN, MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_UMAX, MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_CMP_SWAP,  MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_CMP_SWAP,  MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_NAND, MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_SUB,  MVT::i32, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_NAND, MVT::i64, Expand);
+    setOperationAction(ISD::ATOMIC_LOAD_SUB,  MVT::i64, Expand);
+  }
+
+  setOperationAction(ISD::SMUL_LOHI, MVT::i32, Expand);
+  setOperationAction(ISD::SMUL_LOHI, MVT::i64, Expand);
+  setOperationAction(ISD::UMUL_LOHI, MVT::i32, Expand);
+  setOperationAction(ISD::UMUL_LOHI, MVT::i64, Expand);
+
+  // No sign extend instructions for i1
+  for (MVT VT : MVT::integer_valuetypes()) {
+    setLoadExtAction(ISD::SEXTLOAD, VT, MVT::i1, Promote);
+    setLoadExtAction(ISD::ZEXTLOAD, VT, MVT::i1, Promote);
+    setLoadExtAction(ISD::EXTLOAD,  VT, MVT::i1, Promote);
+  }
+  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i1, Expand);
+  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i8, Expand);
+  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i16, Expand);
+  setOperationAction(ISD::SIGN_EXTEND_INREG, MVT::i32, Expand);
+
+  // Handle the various types of symbolic address.
+  setOperationAction(ISD::ConstantPool,     PtrVT, Custom);
+  setOperationAction(ISD::GlobalAddress,    PtrVT, Custom);
+  setOperationAction(ISD::GlobalTLSAddress, PtrVT, Custom);
+  setOperationAction(ISD::BlockAddress,     PtrVT, Custom);
+  setOperationAction(ISD::JumpTable,        PtrVT, Custom);
+
+  //Expand stack allocations
+  setOperationAction(ISD::DYNAMIC_STACKALLOC, PtrVT, Expand);
+
+  // Use custom expanders so that we can force the function to use
+  // a frame pointer.
+  // TODO: real comment
+  setOperationAction(ISD::STACKSAVE,    MVT::Other, Custom);
+  setOperationAction(ISD::STACKRESTORE, MVT::Other, Custom);
+  setOperationAction(ISD::FRAMEADDR,    MVT::Other, Custom);
+
+  // Handle floating-point types.
+  for (unsigned I = MVT::FIRST_FP_VALUETYPE;
+       I <= MVT::LAST_FP_VALUETYPE;
+       ++I) {
+    MVT VT = MVT::SimpleValueType(I);
+    if (isTypeLegal(VT)) {
+      // We can use FI for FRINT.
+      //setOperationAction(ISD::FRINT, VT, Legal);
+      setOperationAction(ISD::FADD, VT, Legal);
+      setOperationAction(ISD::FSUB, VT, Legal);
+      setOperationAction(ISD::FMUL, VT, Legal);
+      setOperationAction(ISD::FDIV, VT, Legal);
+      //TODO: once implemented in InstrInfo uncomment
+      setOperationAction(ISD::FSQRT, VT, Expand);
+
+      // No special instructions for these.
+      setOperationAction(ISD::FSIN, VT, Expand);
+      setOperationAction(ISD::FCOS, VT, Expand);
+      setOperationAction(ISD::FREM, VT, Expand);
+      setOperationAction(ISD::FABS, VT, Expand);
+    }
+  }
+
+  // Handle floating-point types.
+  if(Subtarget.hasF() || Subtarget.hasD()){
+    setOperationAction(ISD::FMA, MVT::f32,  Legal);
+    setOperationAction(ISD::BITCAST, MVT::i32, Legal);
+    setOperationAction(ISD::BITCAST, MVT::f32, Legal);
+    setOperationAction(ISD::UINT_TO_FP, MVT::i32, Legal);
+    setOperationAction(ISD::SINT_TO_FP, MVT::i32, Legal);
+    setOperationAction(ISD::FP_TO_UINT, MVT::i32, Legal);
+    setOperationAction(ISD::FP_TO_SINT, MVT::i32, Legal);
+    setOperationAction(ISD::FCOPYSIGN, MVT::f32, Legal);
+    if(Subtarget.isRV64()) {
+      setOperationAction(ISD::UINT_TO_FP, MVT::i64, Legal);
+      setOperationAction(ISD::SINT_TO_FP, MVT::i64, Legal);
+      setOperationAction(ISD::FP_TO_UINT, MVT::i64, Legal);
+      setOperationAction(ISD::FP_TO_SINT, MVT::i64, Legal);
+    }
+  }
+  else{
+    setOperationAction(ISD::FMA, MVT::f32,  Expand);
+    setOperationAction(ISD::SETCC, MVT::f32, Expand);
+    setOperationAction(ISD::BITCAST, MVT::i32, Expand);
+    setOperationAction(ISD::BITCAST, MVT::f32, Expand);
+    setOperationAction(ISD::UINT_TO_FP, MVT::i32, Expand);
+    setOperationAction(ISD::SINT_TO_FP, MVT::i32, Expand);
+    setOperationAction(ISD::FP_TO_UINT, MVT::i32, Expand);
+    setOperationAction(ISD::FP_TO_SINT, MVT::i32, Expand);
+    setOperationAction(ISD::UINT_TO_FP, MVT::i64, Expand);
+    setOperationAction(ISD::SINT_TO_FP, MVT::i64, Expand);
+    setOperationAction(ISD::FP_TO_UINT, MVT::i64, Expand);
+    setOperationAction(ISD::FP_TO_SINT, MVT::i64, Expand);
+  }
+  if(Subtarget.hasD()){
+    setOperationAction(ISD::FMA, MVT::f64,  Legal);
+    setOperationAction(ISD::BITCAST, MVT::i64, Legal);
+    setOperationAction(ISD::BITCAST, MVT::f64, Legal);
+    setOperationAction(ISD::FCOPYSIGN, MVT::f64, Legal);
+    setOperationAction(ISD::FP_ROUND, MVT::f64, Legal);
+    setOperationAction(ISD::FP_EXTEND, MVT::f64, Legal);
+  }
+  else {
+    setOperationAction(ISD::FMA, MVT::f64,  Expand);
+    setOperationAction(ISD::SETCC, MVT::f64, Expand);
+    setOperationAction(ISD::BITCAST, MVT::i64, Expand);
+    setOperationAction(ISD::BITCAST, MVT::f64, Expand);
+  }
+  setOperationAction(ISD::FMA, MVT::f128, Expand);
+
+  // Needed so that we don't try to implement f128 constant loads using
+  // a load-and-extend of a f80 constant (in cases where the constant
+  // would fit in an f80).
+  for (MVT VT : MVT::fp_valuetypes())
+    setLoadExtAction(ISD::EXTLOAD, VT, MVT::f80, Expand);
+
+  // Floating-point truncation and stores need to be done separately.
+  setTruncStoreAction(MVT::f64,  MVT::f32, Expand);
+  setTruncStoreAction(MVT::f128, MVT::f32, Expand);
+  setTruncStoreAction(MVT::f128, MVT::f64, Expand);
+
+  // We have 64-bit FPR<->GPR moves, but need special handling for
+  // 32-bit forms.
+
+  // VASTART and VACOPY need to deal with the RISCV-specific varargs
+  // structure, but VAEND is a no-op.
+  setOperationAction(ISD::VASTART, MVT::Other, Custom);
+  //we always write var args with word boundary so we have to customize this
+  setOperationAction(ISD::VAARG  , MVT::Other, Custom);
+  setOperationAction(ISD::VACOPY , MVT::Other, Expand);
+  setOperationAction(ISD::VAEND  , MVT::Other, Expand);
+
+
+  // Compute derived properties from the register classes
+  computeRegisterProperties(STI.getRegisterInfo());
+}
+
+
+bool RISCVTargetLowering::isOffsetFoldingLegal(const GlobalAddressSDNode *GA) const {
+  // The RISCV target isn't yet aware of offsets.
+  return false;
+}
+
+bool RISCVTargetLowering::isFPImmLegal(const APFloat &Imm, EVT VT) const {
+  // We can load zero using LZ?R and negative zero using LZ?R;LC?BR.
+  return Imm.isPosZero();
+}
+
+//===----------------------------------------------------------------------===//
+// Inline asm support
+//===----------------------------------------------------------------------===//
+
+TargetLowering::ConstraintType
+RISCVTargetLowering::getConstraintType(StringRef Constraint) const {
+  if (Constraint.size() == 1) {
+    switch (Constraint[0]) {
+    case 'a': // Address register
+    case 'd': // Data register (equivalent to 'r')
+    case 'f': // Floating-point register
+    case 'r': // General-purpose register
+      return C_RegisterClass;
+
+    case 'Q': // Memory with base and unsigned 12-bit displacement
+    case 'R': // Likewise, plus an index
+    case 'S': // Memory with base and signed 20-bit displacement
+    case 'T': // Likewise, plus an index
+    case 'm': // Equivalent to 'T'.
+      return C_Memory;
+
+    case 'I': // Unsigned 8-bit constant
+    case 'J': // Unsigned 12-bit constant
+    case 'K': // Signed 16-bit constant
+    case 'L': // Signed 20-bit displacement (on all targets we support)
+    case 'M': // 0x7fffffff
+      return C_Other;
+
+    default:
+      break;
+    }
+  }
+  return TargetLowering::getConstraintType(Constraint);
+}
+
+TargetLowering::ConstraintWeight RISCVTargetLowering::
+getSingleConstraintMatchWeight(AsmOperandInfo &info,
+                               const char *constraint) const {
+  ConstraintWeight weight = CW_Invalid;
+  Value *CallOperandVal = info.CallOperandVal;
+  // If we don't have a value, we can't do a match,
+  // but allow it at the lowest weight.
+  if (CallOperandVal == NULL)
+    return CW_Default;
+  Type *type = CallOperandVal->getType();
+  // Look at the constraint type.
+  switch (*constraint) {
+  default:
+    weight = TargetLowering::getSingleConstraintMatchWeight(info, constraint);
+    break;
+
+  case 'a': // Address register
+  case 'd': // Data register (equivalent to 'r')
+  case 'r': // General-purpose register
+    if (CallOperandVal->getType()->isIntegerTy())
+      weight = CW_Register;
+    break;
+
+  case 'f': // Floating-point register
+    if (type->isFloatingPointTy())
+      weight = CW_Register;
+    break;
+
+  case 'I': // Unsigned 8-bit constant
+    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal))
+      if (isUInt<8>(C->getZExtValue()))
+        weight = CW_Constant;
+    break;
+
+  case 'J': // Unsigned 12-bit constant
+    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal))
+      if (isUInt<12>(C->getZExtValue()))
+        weight = CW_Constant;
+    break;
+
+  case 'K': // Signed 16-bit constant
+    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal))
+      if (isInt<16>(C->getSExtValue()))
+        weight = CW_Constant;
+    break;
+
+  case 'L': // Signed 20-bit displacement (on all targets we support)
+    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal))
+      if (isInt<20>(C->getSExtValue()))
+        weight = CW_Constant;
+    break;
+
+  case 'M': // 0x7fffffff
+    if (ConstantInt *C = dyn_cast<ConstantInt>(CallOperandVal))
+      if (C->getZExtValue() == 0x7fffffff)
+        weight = CW_Constant;
+    break;
+  }
+  return weight;
+}
+
+std::pair<unsigned, const TargetRegisterClass *> RISCVTargetLowering::
+getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI, StringRef Constraint, MVT VT) const {
+  if (Constraint.size() == 1) {
+    // GCC Constraint Letters
+    switch (Constraint[0]) {
+    default: break;
+    case 'd': // Data register (equivalent to 'r')
+    case 'r': // General-purpose register
+      if(Subtarget.isRV64())
+        return std::make_pair(0U, &RISCV::GR64BitRegClass);
+      return std::make_pair(0U, &RISCV::GR32BitRegClass);
+
+    case 'f': // Floating-point register
+      if(Subtarget.hasD())
+        return std::make_pair(0U, &RISCV::FP64BitRegClass);
+      else if(Subtarget.hasF())
+        return std::make_pair(0U, &RISCV::FP32BitRegClass);
+      else if(Subtarget.isRV64())
+        return std::make_pair(0U, &RISCV::GR64BitRegClass);
+      return std::make_pair(0U, &RISCV::GR32BitRegClass);
+    }
+  }
+  return TargetLowering::getRegForInlineAsmConstraint(TRI, Constraint, VT);
+}
+
+void RISCVTargetLowering::
+LowerAsmOperandForConstraint(SDValue Op, std::string &Constraint,
+                             std::vector<SDValue> &Ops,
+                             SelectionDAG &DAG) const {
+  // Only support length 1 constraints for now.
+  if (Constraint.length() == 1) {
+    switch (Constraint[0]) {
+    case 'I': // Unsigned 8-bit constant
+      if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op))
+        if (isUInt<8>(C->getZExtValue()))
+          Ops.push_back(DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),
+                                              Op.getValueType()));
+      return;
+
+    case 'J': // Unsigned 12-bit constant
+      if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op))
+        if (isUInt<12>(C->getZExtValue()))
+          Ops.push_back(DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),
+                                              Op.getValueType()));
+      return;
+
+    case 'K': // Signed 16-bit constant
+      if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op))
+        if (isInt<16>(C->getSExtValue()))
+          Ops.push_back(DAG.getTargetConstant(C->getSExtValue(), SDLoc(Op),
+                                              Op.getValueType()));
+      return;
+
+    case 'L': // Signed 20-bit displacement (on all targets we support)
+      if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op))
+        if (isInt<20>(C->getSExtValue()))
+          Ops.push_back(DAG.getTargetConstant(C->getSExtValue(), SDLoc(Op),
+                                              Op.getValueType()));
+      return;
+
+    case 'M': // 0x7fffffff
+      if (ConstantSDNode *C = dyn_cast<ConstantSDNode>(Op))
+        if (C->getZExtValue() == 0x7fffffff)
+          Ops.push_back(DAG.getTargetConstant(C->getZExtValue(), SDLoc(Op),
+                                              Op.getValueType()));
+      return;
+    }
+  }
+  TargetLowering::LowerAsmOperandForConstraint(Op, Constraint, Ops, DAG);
+}
+
+//===----------------------------------------------------------------------===//
+//  Lower helper functions
+//===----------------------------------------------------------------------===//
+
+// addLiveIn - This helper function adds the specified physical register to the
+// MachineFunction as a live in value.  It also creates a corresponding
+// virtual register for it.
+static unsigned
+addLiveIn(MachineFunction &MF, unsigned PReg, const TargetRegisterClass *RC)
+{
+  unsigned VReg = MF.getRegInfo().createVirtualRegister(RC);
+  MF.getRegInfo().addLiveIn(PReg, VReg);
+  return VReg;
+}
+
+//===----------------------------------------------------------------------===//
+// Calling conventions
+//===----------------------------------------------------------------------===//
+
+#include "RISCVGenCallingConv.inc"
+
+// Value is a value that has been passed to us in the location described by VA
+// (and so has type VA.getLocVT()).  Convert Value to VA.getValVT(), chaining
+// any loads onto Chain.
+static SDValue convertLocVTToValVT(SelectionDAG &DAG, SDLoc DL, CCValAssign &VA,
+                                   SDValue Chain, SDValue Value) {
+  // If the argument has been promoted from a smaller type, insert an
+  // assertion to capture this.
+  if (VA.getLocInfo() == CCValAssign::SExt)
+    Value = DAG.getNode(ISD::AssertSext, DL, VA.getLocVT(), Value,
+                        DAG.getValueType(VA.getValVT()));
+  else if (VA.getLocInfo() == CCValAssign::ZExt)
+    Value = DAG.getNode(ISD::AssertZext, DL, VA.getLocVT(), Value,
+                        DAG.getValueType(VA.getValVT()));
+
+  if (VA.isExtInLoc())
+    Value = DAG.getNode(ISD::TRUNCATE, DL, VA.getValVT(), Value);
+  else if (VA.getLocInfo() == CCValAssign::Indirect)
+    Value = DAG.getLoad(VA.getValVT(), DL, Chain, Value,
+                        MachinePointerInfo(), false, false, false, 0);
+  else
+    assert(VA.getLocInfo() == CCValAssign::Full && "Unsupported getLocInfo");
+  return Value;
+}
+
+// Value is a value of type VA.getValVT() that we need to copy into
+// the location described by VA.  Return a copy of Value converted to
+// VA.getValVT().  The caller is responsible for handling indirect values.
+static SDValue convertValVTToLocVT(SelectionDAG &DAG, SDLoc DL, CCValAssign &VA,
+                                   SDValue Value) {
+  switch (VA.getLocInfo()) {
+  case CCValAssign::SExt:
+    return DAG.getNode(ISD::SIGN_EXTEND, DL, VA.getLocVT(), Value);
+  case CCValAssign::ZExt:
+    return DAG.getNode(ISD::ZERO_EXTEND, DL, VA.getLocVT(), Value);
+  case CCValAssign::AExt:
+    return DAG.getNode(ISD::ANY_EXTEND, DL, VA.getLocVT(), Value);
+  case CCValAssign::BCvt:
+    return DAG.getNode(ISD::BITCAST, DL, VA.getLocVT(), Value);
+  case CCValAssign::Full:
+    return Value;
+  default:
+    llvm_unreachable("Unhandled getLocInfo()");
+  }
+}
+
+SDValue RISCVTargetLowering::
+LowerFormalArguments(SDValue Chain, CallingConv::ID CallConv, bool IsVarArg,
+                     const SmallVectorImpl<ISD::InputArg> &Ins,
+                     SDLoc DL, SelectionDAG &DAG,
+                     SmallVectorImpl<SDValue> &InVals) const {
+  MachineFunction &MF = DAG.getMachineFunction();
+  MachineFrameInfo *MFI = MF.getFrameInfo();
+  RISCVFunctionInfo *RISCVFI = MF.getInfo<RISCVFunctionInfo>();
+
+  RISCVFI->setVarArgsFrameIndex(0);
+
+  // Used with vargs to acumulate store chains.
+  std::vector<SDValue> OutChains;
+
+  // Assign locations to all of the incoming arguments.
+  SmallVector<CCValAssign, 16> ArgLocs;
+  CCState CCInfo(CallConv, IsVarArg, DAG.getMachineFunction(), ArgLocs,
+		 *DAG.getContext());
+
+  CCInfo.AnalyzeFormalArguments(Ins,
+    IsRV32 ? IsVarArg ? CC_RISCV32_VAR : CC_RISCV32 :
+    IsVarArg ? CC_RISCV64_VAR : CC_RISCV64);
+  
+  for (unsigned i = 0, e = ArgLocs.size(); i != e; ++i) {
+    CCValAssign &VA = ArgLocs[i];
+    // Arguments stored on registers
+    if (VA.isRegLoc()) {
+      EVT RegVT = VA.getLocVT();
+      const TargetRegisterClass *RC;
+
+      if (RegVT == MVT::i32) {
+        RC = &RISCV::GR32BitRegClass;
+        if(Subtarget.isRV64())
+          RC = &RISCV::GR64BitRegClass; //promoted
+      } else if (RegVT == MVT::i64){
+        if(Subtarget.isRV32()){
+          //for RV32 store in pair of two GR32
+          RC = &RISCV::PairGR64BitRegClass;
+        } else {
+          RC = &RISCV::GR64BitRegClass;
+        }
+      } else if (RegVT == MVT::f32) {
+          if(Subtarget.hasD())
+            RC = &RISCV::FP64BitRegClass;
+          else if(Subtarget.hasF())
+            RC = &RISCV::FP32BitRegClass;
+          else 
+            RC = &RISCV::GR32BitRegClass;
+      } else if (RegVT == MVT::f64) {
+          if(Subtarget.hasD())
+            RC = &RISCV::FP64BitRegClass;
+          else if(Subtarget.hasF())
+            RC = &RISCV::PairFP64BitRegClass;
+          else if(Subtarget.isRV64())
+            RC = &RISCV::GR64BitRegClass;
+          else
+            RC = &RISCV::PairGR64BitRegClass;
+      } else
+        llvm_unreachable("RegVT not supported by FormalArguments Lowering");
+
+      // Transform the arguments stored on
+      // physical registers into virtual ones
+      unsigned Reg = MF.addLiveIn(VA.getLocReg(), RC);
+      SDValue ArgValue = DAG.getCopyFromReg(Chain, DL, Reg, RegVT);
+
+      // If this is an 8 or 16-bit value, it has been passed promoted
+      // to 32 bits.  Insert an assert[sz]ext to capture this, then
+      // truncate to the right size.
+      if (VA.getLocInfo() != CCValAssign::Full) {
+        unsigned Opcode = 0;
+        if (VA.getLocInfo() == CCValAssign::SExt)
+          Opcode = ISD::AssertSext;
+        else if (VA.getLocInfo() == CCValAssign::ZExt)
+          Opcode = ISD::AssertZext;
+        if (Opcode)
+          ArgValue = DAG.getNode(Opcode, DL, RegVT, ArgValue,
+                                 DAG.getValueType(VA.getValVT()));
+        ArgValue = DAG.getNode(ISD::TRUNCATE, DL, VA.getValVT(), ArgValue);
+      }
+
+      InVals.push_back(ArgValue);
+    } else { // !VA.isRegLoc()
+
+      // sanity check
+      assert(VA.isMemLoc());
+
+      EVT ValVT = VA.getValVT();
+
+      // The stack pointer offset is relative to the caller stack frame.
+      int FI = MFI->CreateFixedObject(ValVT.getSizeInBits()/8,
+                                      VA.getLocMemOffset(), true);
+
+      // Create load nodes to retrieve arguments from the stack
+      SDValue FIN = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));
+      InVals.push_back(DAG.getLoad(ValVT, DL, Chain, FIN,
+                                   MachinePointerInfo::getFixedStack(FI),
+                                   false, false, false, 0));
+    }
+  }
+
+  //TODO: handle ByVal
+
+  if (IsVarArg){
+    auto ArgRegs = IsRV32 ? RV32IntRegs : RV64IntRegs;
+    unsigned NumRegs = llvm::RISCV::NumArgGPRs;
+    unsigned Idx = CCInfo.getFirstUnallocated(ArrayRef<MCPhysReg>(ArgRegs, 8));
+    unsigned RegSize = IsRV32 ? 4 : 8;
+    MVT RegTy = MVT::getIntegerVT(RegSize * 8);
+    const TargetRegisterClass *RC = getRegClassFor(RegTy);
+
+    // Offset of the first variable argument from stack pointer.
+    int VaArgOffset;
+
+    if (NumRegs == Idx)
+      VaArgOffset = RoundUpToAlignment(CCInfo.getNextStackOffset(), RegSize);
+    else
+      VaArgOffset = -(int)(RegSize * (NumRegs - Idx));
+
+    // Record the frame index of the first variable argument
+    // which is a value necessary to VASTART.
+    int FI = MFI->CreateFixedObject(RegSize, VaArgOffset, true);
+    RISCVFI->setVarArgsFrameIndex(FI);
+
+    // Copy the integer registers that have not been used for argument passing
+    // to the argument register save area. 
+    for (unsigned I = Idx; I < NumRegs; ++I, VaArgOffset += RegSize) {
+      unsigned Reg = addLiveIn(MF, ArgRegs[I], RC);
+      SDValue ArgValue = DAG.getCopyFromReg(Chain, DL, Reg, RegTy);
+      FI = MFI->CreateFixedObject(RegSize, VaArgOffset, true);
+      SDValue PtrOff = DAG.getFrameIndex(FI, getPointerTy(DAG.getDataLayout()));
+      SDValue Store = DAG.getStore(Chain, DL, ArgValue, PtrOff,
+                                   MachinePointerInfo(), false, false, 0);
+      cast<StoreSDNode>(Store.getNode())
+          ->getMemOperand()
+          ->setValue((Value *)nullptr);
+      OutChains.push_back(Store);
+    }
+  }
+
+  // All stores are grouped in one node to allow the matching between
+  // the size of Ins and InVals. This only happens when on varg functions
+  if (!OutChains.empty()) {
+    OutChains.push_back(Chain);
+    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, OutChains);
+  }
+
+  return Chain;
+}
+
+SDValue RISCVTargetLowering::getTargetNode(SDValue Op, SelectionDAG &DAG, unsigned Flag) const {
+  EVT Ty = getPointerTy(DAG.getDataLayout());
+
+  if (GlobalAddressSDNode *N = dyn_cast<GlobalAddressSDNode>(Op))
+    return DAG.getTargetGlobalAddress(N->getGlobal(), SDLoc(Op), Ty, 0, Flag);
+  if (ExternalSymbolSDNode *N = dyn_cast<ExternalSymbolSDNode>(Op))
+    return DAG.getTargetExternalSymbol(N->getSymbol(), Ty, Flag);
+  if (BlockAddressSDNode *N = dyn_cast<BlockAddressSDNode>(Op))
+    return DAG.getTargetBlockAddress(N->getBlockAddress(), Ty, 0, Flag);
+  if (JumpTableSDNode *N = dyn_cast<JumpTableSDNode>(Op))
+    return DAG.getTargetJumpTable(N->getIndex(), Ty, Flag);
+  if (ConstantPoolSDNode *N = dyn_cast<ConstantPoolSDNode>(Op))
+    return DAG.getTargetConstantPool(N->getConstVal(), Ty, N->getAlignment(),
+                                     N->getOffset(), Flag);
+
+  llvm_unreachable("Unexpected node type.");
+  return SDValue();
+}
+
+
+SDValue RISCVTargetLowering::getAddrNonPIC(SDValue Op, SelectionDAG &DAG) const {
+  SDLoc DL(Op);
+  EVT Ty = getPointerTy(DAG.getDataLayout());
+  SDValue Hi = getTargetNode(Op, DAG, RISCVII::MO_ABS_HI);
+  SDValue Lo = getTargetNode(Op, DAG, RISCVII::MO_ABS_LO);
+  SDValue ResHi = DAG.getNode(RISCVISD::Hi, DL, Ty, Hi);
+  SDValue ResLo = DAG.getNode(RISCVISD::Lo, DL, Ty, Lo);
+  return DAG.getNode(ISD::ADD, DL, Ty, ResHi, ResLo);
+}
+
+SDValue RISCVTargetLowering::getAddrPIC(SDValue Op, SelectionDAG &DAG) const {
+  SDLoc DL(Op);
+  EVT Ty = Op.getValueType();
+  return DAG.getNode(RISCVISD::PCREL_WRAPPER, DL, Ty, Op);
+}
+
+SDValue
+RISCVTargetLowering::LowerCall(CallLoweringInfo &CLI,
+                                 SmallVectorImpl<SDValue> &InVals) const {
+  SelectionDAG &DAG = CLI.DAG;
+  SDLoc &DL = CLI.DL;
+  SmallVector<ISD::OutputArg, 32> &Outs = CLI.Outs;
+  SmallVector<SDValue, 32> &OutVals = CLI.OutVals;
+  SmallVector<ISD::InputArg, 32> &Ins = CLI.Ins;
+  SDValue Chain = CLI.Chain;
+  SDValue Callee = CLI.Callee;
+  bool &isTailCall = CLI.IsTailCall;
+  CallingConv::ID CallConv = CLI.CallConv;
+  bool IsVarArg = CLI.IsVarArg;
+  MachineFunction &MF = DAG.getMachineFunction();
+  EVT PtrVT = getPointerTy(DAG.getDataLayout());
+
+  // RISCV target does not yet support tail call optimization.
+  isTailCall = false;
+
+  // Analyze the operands of the call, assigning locations to each operand.
+  SmallVector<CCValAssign, 16> ArgLocs;
+  CCState CCInfo(CallConv, IsVarArg, MF, ArgLocs, *DAG.getContext());
+
+  CCAssignFn *CC = IsRV32 ? IsVarArg ? CC_RISCV32_VAR : CC_RISCV32 :
+                           IsVarArg ? CC_RISCV64_VAR : CC_RISCV64;
+  CCInfo.AnalyzeCallOperands(Outs, CC);
+  //
+  // Get a count of how many bytes are to be pushed on the stack.
+  unsigned NumBytes = CCInfo.getNextStackOffset();
+
+  // Mark the start of the call.
+  Chain = DAG.getCALLSEQ_START(Chain, DAG.getConstant(NumBytes, DL, PtrVT, true),
+                               DL);
+
+  // Copy argument values to their designated locations.
+  std::deque< std::pair<unsigned, SDValue> > RegsToPass;
+  SmallVector<SDValue, 8> MemOpChains;
+  SDValue StackPtr;
+  for (unsigned I = 0, E = ArgLocs.size(); I != E; ++I) {
+    CCValAssign &VA = ArgLocs[I];
+    SDValue ArgValue = OutVals[I];
+    ISD::ArgFlagsTy Flags = Outs[I].Flags;
+
+    ArgValue = convertValVTToLocVT(DAG, DL, VA, ArgValue);
+
+    if (VA.isRegLoc())
+      // Queue up the argument copies and emit them at the end.
+      RegsToPass.push_back(std::make_pair(VA.getLocReg(), ArgValue));
+    else if (Flags.isByVal()) {
+      assert(VA.isMemLoc());
+      assert(Flags.getByValSize() &&
+             "ByVal args of size 0 should have been ignored by front-end.");
+      assert(!isTailCall &&
+             "Do not tail-call optimize if there is a byval argument.");
+
+      // True if this byval aggregate will be split between registers
+      // and memory.
+      unsigned ByValArgsCount = CCInfo.getInRegsParamsCount();
+      unsigned CurByValIdx = CCInfo.getInRegsParamsProcessed();
+      if (CurByValIdx < ByValArgsCount) {
+        unsigned RegBegin, RegEnd;
+        CCInfo.getInRegsParamInfo(CurByValIdx, RegBegin, RegEnd);
+
+        EVT PtrVT = DAG.getTargetLoweringInfo().getPointerTy(DAG.getDataLayout());
+        unsigned int i, j;
+        for (i = 0, j = RegBegin; j < RegEnd; i++, j++) {
+          SDValue Const = DAG.getConstant(4*i, DL, MVT::i32);//TODO:should this i32 be ptrTy
+          SDValue AddArg = DAG.getNode(ISD::ADD, DL, PtrVT, ArgValue, Const);
+          SDValue Load = DAG.getLoad(PtrVT, DL, Chain, AddArg,
+                                     MachinePointerInfo(),
+                                     false, false, false,
+                                     DAG.InferPtrAlignment(AddArg));
+          MemOpChains.push_back(Load.getValue(1));
+          RegsToPass.push_back(std::make_pair(j, Load));
+        }
+
+        CCInfo.nextInRegsParam();
+      }
+
+      // TODO: Handle byvals partially or entirely not in registers
+
+    }
+    else {
+      assert(VA.isMemLoc() && "Argument not register or memory");
+
+      // Work out the address of the stack slot.  Unpromoted ints and
+      // floats are passed as right-justified 8-byte values.
+      if (!StackPtr.getNode())
+        StackPtr = DAG.getCopyFromReg(Chain, DL, Subtarget.isRV64() ? RISCV::sp_64 : RISCV::sp, PtrVT);
+      unsigned Offset = VA.getLocMemOffset();
+      SDValue Address = DAG.getNode(ISD::ADD, DL, PtrVT, StackPtr,
+                                    DAG.getIntPtrConstant(Offset, DL));
+
+      // Emit the store.
+      MemOpChains.push_back(DAG.getStore(Chain, DL, ArgValue, Address,
+                                         MachinePointerInfo(),
+                                         false, false, 0));
+    }
+  }
+
+  // Join the stores, which are independent of one another.
+  if (!MemOpChains.empty())
+    Chain = DAG.getNode(ISD::TokenFactor, DL, MVT::Other, MemOpChains);
+
+  // Build a sequence of copy-to-reg nodes, chained and glued together.
+  SDValue Glue;
+  for (unsigned I = 0, E = RegsToPass.size(); I != E; ++I) {
+    Chain = DAG.getCopyToReg(Chain, DL, RegsToPass[I].first,
+                             RegsToPass[I].second, Glue);
+    Glue = Chain.getValue(1);
+  }
+
+  // Accept direct calls by converting symbolic call addresses to the
+  // associated Target* opcodes.
+  if (ExternalSymbolSDNode *E = dyn_cast<ExternalSymbolSDNode>(Callee)) {
+    if (DAG.getTarget().getRelocationModel() == Reloc::PIC_) {
+      Callee = getAddrPIC(DAG.getTargetExternalSymbol(E->getSymbol(), PtrVT), DAG);
+    } else
+      Callee = DAG.getTargetExternalSymbol(E->getSymbol(), PtrVT);
+  }
+
+  // The first call operand is the chain and the second is the target address.
+  SmallVector<SDValue, 8> Ops;
+  Ops.push_back(Chain);
+  Ops.push_back(Callee);
+
+  // Add argument registers to the end of the list so that they are
+  // known live into the call.
+  for (unsigned I = 0, E = RegsToPass.size(); I != E; ++I)
+    Ops.push_back(DAG.getRegister(RegsToPass[I].first,
+                                  RegsToPass[I].second.getValueType()));
+
+  // Glue the call to the argument copies, if any.
+  if (Glue.getNode())
+    Ops.push_back(Glue);
+
+  SDVTList NodeTys = DAG.getVTList(MVT::Other, MVT::Glue);
+  Chain = DAG.getNode(RISCVISD::CALL, DL, NodeTys, Ops);
+  Glue = Chain.getValue(1);
+
+  // Mark the end of the call, which is glued to the call itself.
+  Chain = DAG.getCALLSEQ_END(Chain,
+                             DAG.getConstant(NumBytes, DL, PtrVT, true),
+                             DAG.getConstant(0, DL, PtrVT, true),
+                             Glue, DL);
+  Glue = Chain.getValue(1);
+
+  // Assign locations to each value returned by this call.
+  SmallVector<CCValAssign, 16> RetLocs;
+  CCState RetCCInfo(CallConv, IsVarArg, MF, RetLocs, *DAG.getContext());
+  if(Subtarget.isRV64())
+    RetCCInfo.AnalyzeCallResult(Ins, RetCC_RISCV64);
+  else
+    RetCCInfo.AnalyzeCallResult(Ins, RetCC_RISCV32);
+
+  // Copy all of the result registers out of their specified physreg.
+  for (unsigned I = 0, E = RetLocs.size(); I != E; ++I) {
+    CCValAssign &VA = RetLocs[I];
+
+    // Copy the value out, gluing the copy to the end of the call sequence.
+    SDValue RetValue = DAG.getCopyFromReg(Chain, DL, VA.getLocReg(),
+                                          VA.getLocVT(), Glue);
+    Chain = RetValue.getValue(1);
+    Glue = RetValue.getValue(2);
+
+    // Convert the value of the return register into the value that's
+    // being returned.
+    InVals.push_back(convertLocVTToValVT(DAG, DL, VA, Chain, RetValue));
+  }
+
+
+
+  return Chain;
+}
+
+/// This hook should be implemented to check whether the return values
+/// described by the Outs array can fit into the return registers.  If false
+/// is returned, an sret-demotion is performed.
+bool
+RISCVTargetLowering::CanLowerReturn(CallingConv::ID CallConv,
+                                   MachineFunction &MF, bool IsVarArg,
+                                   const SmallVectorImpl<ISD::OutputArg> &Outs,
+                                   LLVMContext &Context) const {
+  SmallVector<CCValAssign, 16> RVLocs;
+  CCState CCInfo(CallConv, IsVarArg, MF, RVLocs, Context);
+  return CCInfo.CheckReturn(Outs, Subtarget.isRV64() ? RetCC_RISCV64 : RetCC_RISCV32);
+}
+
+SDValue
+RISCVTargetLowering::LowerReturn(SDValue Chain,
+                                   CallingConv::ID CallConv, bool IsVarArg,
+                                   const SmallVectorImpl<ISD::OutputArg> &Outs,
+                                   const SmallVectorImpl<SDValue> &OutVals,
+                                   SDLoc DL, SelectionDAG &DAG) const {
+  MachineFunction &MF = DAG.getMachineFunction();
+
+  // Assign locations to each returned value.
+  SmallVector<CCValAssign, 16> RetLocs;
+  CCState RetCCInfo(CallConv, IsVarArg, MF, RetLocs, *DAG.getContext());
+  if(Subtarget.isRV64())
+    RetCCInfo.AnalyzeReturn(Outs, RetCC_RISCV64);
+  else
+    RetCCInfo.AnalyzeReturn(Outs, RetCC_RISCV32);
+
+  SDValue Glue;
+  // Quick exit for void returns
+  if (RetLocs.empty())
+    return DAG.getNode(RISCVISD::RET_FLAG, DL, MVT::Other, Chain);
+
+
+  // Copy the result values into the output registers.
+  SmallVector<SDValue, 4> RetOps;
+  RetOps.push_back(Chain);
+  for (unsigned I = 0, E = RetLocs.size(); I != E; ++I) {
+    CCValAssign &VA = RetLocs[I];
+    SDValue RetValue = OutVals[I];
+
+    // Make the return register live on exit.
+    assert(VA.isRegLoc() && "Can only return in registers!");
+
+    // Promote the value as required.
+    RetValue = convertValVTToLocVT(DAG, DL, VA, RetValue);
+
+    // Chain and glue the copies together.
+    unsigned Reg = VA.getLocReg();
+    Chain = DAG.getCopyToReg(Chain, DL, Reg, RetValue, Glue);
+    Glue = Chain.getValue(1);
+    RetOps.push_back(DAG.getRegister(Reg, VA.getLocVT()));
+  }
+
+  // Update chain and glue.
+  RetOps[0] = Chain;
+  if (Glue.getNode())
+    RetOps.push_back(Glue);
+
+  return DAG.getNode(RISCVISD::RET_FLAG, DL, MVT::Other, RetOps);
+}
+
+SDValue RISCVTargetLowering::
+lowerSELECT_CC(SDValue Op, SelectionDAG &DAG) const
+{
+  SDLoc DL(Op);
+  EVT Ty = Op.getOperand(0).getValueType();
+  SDValue Cond = DAG.getNode(ISD::SETCC, DL,
+                             getSetCCResultType(DAG.getDataLayout(),*DAG.getContext(), Ty),
+                             Op.getOperand(0), Op.getOperand(1),
+                             Op.getOperand(4));
+
+  return DAG.getNode(ISD::SELECT, DL, Op.getValueType(), Cond, Op.getOperand(2),
+                     Op.getOperand(3));
+}
+
+SDValue RISCVTargetLowering::lowerRETURNADDR(SDValue Op, SelectionDAG &DAG) const {
+  // check the depth
+  //TODO: riscv-gcc can handle this, by navigating through the stack, we should be able to do this too
+  assert((cast<ConstantSDNode>(Op.getOperand(0))->getZExtValue() == 0) &&
+    "Return address can be determined only for current frame.");
+      
+  MachineFunction &MF = DAG.getMachineFunction();
+  MachineFrameInfo *MFI = MF.getFrameInfo();
+  MVT VT = Op.getSimpleValueType();
+  unsigned RA = Subtarget.isRV64() ? RISCV::ra_64 : RISCV::ra;
+  MFI->setReturnAddressIsTaken(true);
+
+  // Return RA, which contains the return address. Mark it an implicit live-in.
+  unsigned Reg = MF.addLiveIn(RA, getRegClassFor(VT));
+  return DAG.getCopyFromReg(DAG.getEntryNode(), SDLoc(Op), Reg, VT);
+}
+
+SDValue RISCVTargetLowering::lowerGlobalAddress(SDValue Op,
+                                                  SelectionDAG &DAG) const {
+  Reloc::Model RM = DAG.getTarget().getRelocationModel();
+
+  if(RM != Reloc::PIC_) {
+      //%hi/%lo relocation
+      return getAddrNonPIC(Op,DAG);
+  }
+  if (GlobalAddressSDNode *G = dyn_cast<GlobalAddressSDNode>(Op)) {
+    Op = DAG.getTargetGlobalAddress(G->getGlobal(), SDLoc(Op), getPointerTy(DAG.getDataLayout()));
+    return getAddrPIC(Op,DAG);
+  }
+  llvm_unreachable("invalid global addresses to lower");
+}
+
+SDValue RISCVTargetLowering::lowerGlobalTLSAddress(GlobalAddressSDNode *GA,
+						     SelectionDAG &DAG) const {
+  // If the relocation model is PIC, use the General Dynamic TLS Model or
+  // Local Dynamic TLS model, otherwise use the Initial Exec or
+  // Local Exec TLS Model.
+
+  SDLoc DL(GA);
+  const GlobalValue *GV = GA->getGlobal();
+  EVT PtrVT = getPointerTy(DAG.getDataLayout());
+
+  TLSModel::Model model = getTargetMachine().getTLSModel(GV);
+
+  SDValue Offset;
+  if (model == TLSModel::LocalExec) {
+    // Local Exec TLS Model
+    assert(model == TLSModel::LocalExec);
+    SDValue TGAHi = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0,
+                                               RISCVII::MO_TPREL_HI);
+    SDValue TGALo = DAG.getTargetGlobalAddress(GV, DL, PtrVT, 0,
+                                               RISCVII::MO_TPREL_LO);
+    SDValue Hi = DAG.getNode(RISCVISD::Hi, DL, PtrVT, TGAHi);
+    SDValue Lo = DAG.getNode(RISCVISD::Lo, DL, PtrVT, TGALo);
+    Offset = DAG.getNode(ISD::ADD, DL, PtrVT, Hi, Lo); 
+  } else {
+    llvm_unreachable("only local-exec TLS mode supported");
+  }
+
+  //SDValue ThreadPointer = DAG.getNode(MipsISD::ThreadPointer, DL, PtrVT);
+  SDValue ThreadPointer = DAG.getRegister(
+      Subtarget.isRV64() ? RISCV::tp_64 : RISCV::tp, PtrVT);
+  
+  return DAG.getNode(ISD::ADD, DL, PtrVT, ThreadPointer, Offset);
+
+
+}
+
+SDValue RISCVTargetLowering::lowerBlockAddress(BlockAddressSDNode *Node,
+                                                 SelectionDAG &DAG) const {
+  const BlockAddress *BA = Node->getBlockAddress();
+  int64_t Offset = Node->getOffset();
+  EVT PtrVT = getPointerTy(DAG.getDataLayout());
+
+  SDValue Result = DAG.getTargetBlockAddress(BA, PtrVT, Offset);
+  return Result;
+}
+
+SDValue RISCVTargetLowering::lowerJumpTable(JumpTableSDNode *JT,
+                                              SelectionDAG &DAG) const {
+  SDLoc DL(JT);
+  EVT PtrVT = getPointerTy(DAG.getDataLayout());
+  SDValue Result = DAG.getTargetJumpTable(JT->getIndex(), PtrVT);
+
+  // Use LARL to load the address of the table.
+  return DAG.getNode(RISCVISD::PCREL_WRAPPER, DL, PtrVT, Result);
+}
+
+SDValue RISCVTargetLowering::lowerConstantPool(ConstantPoolSDNode *CP,
+                                                 SelectionDAG &DAG) const {
+  EVT PtrVT = getPointerTy(DAG.getDataLayout());
+
+  SDValue Result;
+  if (CP->isMachineConstantPoolEntry())
+    Result = DAG.getTargetConstantPool(CP->getMachineCPVal(), PtrVT,
+				       CP->getAlignment());
+  else
+    Result = DAG.getTargetConstantPool(CP->getConstVal(), PtrVT,
+				       CP->getAlignment(), CP->getOffset());
+
+  Reloc::Model RM = DAG.getTarget().getRelocationModel();
+
+  if(RM != Reloc::PIC_)
+    return getAddrNonPIC(Result, DAG);
+  return getAddrPIC(Result, DAG);
+}
+
+SDValue RISCVTargetLowering::lowerVASTART(SDValue Op,
+                                            SelectionDAG &DAG) const {
+  MachineFunction &MF = DAG.getMachineFunction();
+  RISCVFunctionInfo *FuncInfo =
+    MF.getInfo<RISCVFunctionInfo>();
+  EVT PtrVT = getPointerTy(DAG.getDataLayout());
+
+  SDValue Chain   = Op.getOperand(0);
+  SDValue Addr    = Op.getOperand(1);
+  const Value *SV = cast<SrcValueSDNode>(Op.getOperand(2))->getValue();
+  SDLoc DL(Op);
+  SDValue FI      = DAG.getFrameIndex(FuncInfo->getVarArgsFrameIndex(),
+                                 PtrVT);
+
+  // vastart just stores the address of the VarArgsFrameIndex slot into the
+  // memory location argument.
+  return DAG.getStore(Chain, DL, FI, Addr,
+                      MachinePointerInfo(SV), false, false, 0);
+}
+
+SDValue RISCVTargetLowering::lowerVAARG(SDValue Op, SelectionDAG &DAG) const{
+  SDNode *Node = Op.getNode();
+  EVT VT = Node->getValueType(0);
+  SDValue InChain = Node->getOperand(0);
+  SDValue VAListPtr = Node->getOperand(1);
+  EVT PtrVT = VAListPtr.getValueType();
+  const Value *SV = cast<SrcValueSDNode>(Node->getOperand(2))->getValue();
+  SDLoc DL(Node);
+  SDValue VAList = DAG.getLoad(PtrVT, DL, InChain, VAListPtr,
+                               MachinePointerInfo(SV), false, false, false, 0);
+  // Increment the pointer, VAList, to the next vaarg.
+  SDValue NextPtr = DAG.getNode(ISD::ADD, DL, PtrVT, VAList,
+                                DAG.getIntPtrConstant(VT.getSizeInBits()/8, DL));
+  // Store the incremented VAList to the legalized pointer.
+  InChain = DAG.getStore(VAList.getValue(1), DL, NextPtr,
+                         VAListPtr, MachinePointerInfo(SV), false, false, 0);
+  // Load the actual argument out of the pointer VAList.
+  // We can't count on greater alignment than the word size.
+  return DAG.getLoad(VT, DL, InChain, VAList, MachinePointerInfo(),
+                     false, false, false,
+                     std::min(PtrVT.getSizeInBits(), VT.getSizeInBits())/8);
+}
+
+SDValue RISCVTargetLowering::lowerATOMIC_FENCE(SDValue Op, SelectionDAG &DAG) const {
+  SDLoc DL(Op);
+  
+  unsigned PI,PO,PR,PW,SI,SO,SR,SW;
+  switch(Op.getConstantOperandVal(1)) {
+    case NotAtomic:
+    case Unordered:
+    case Monotonic:
+    case Acquire:
+    case Release:
+    case AcquireRelease:
+    case SequentiallyConsistent:
+      PI = 1 << 3;
+      PO = 1 << 2;
+      PR = 1 << 1;
+      PW = 1 << 0;
+  } 
+
+  switch(Op.getConstantOperandVal(2)) {
+    case SingleThread:
+    case CrossThread:
+      SI = 1 << 3;
+      SO = 1 << 2;
+      SR = 1 << 1;
+      SW = 1 << 0;
+  }
+
+  unsigned pred = PI | PO | PR | PW;
+  unsigned succ = SI | SO | SR | SW;
+  return DAG.getNode(RISCVISD::FENCE, DL, MVT::Other,Op.getOperand(0),
+                       DAG.getConstant(pred, DL, Subtarget.isRV64() ? MVT::i64 : MVT::i32),
+                       DAG.getConstant(succ, DL, Subtarget.isRV64() ? MVT::i64 : MVT::i32));
+}
+
+SDValue RISCVTargetLowering::lowerSTACKSAVE(SDValue Op,
+                                              SelectionDAG &DAG) const {
+  MachineFunction &MF = DAG.getMachineFunction();
+  MF.getInfo<RISCVFunctionInfo>()->setManipulatesSP(true);
+  unsigned sp = Subtarget.isRV64() ? RISCV::sp_64 : RISCV::sp;
+  return DAG.getCopyFromReg(Op.getOperand(0), SDLoc(Op), sp, Op.getValueType());
+}
+
+SDValue RISCVTargetLowering::lowerSTACKRESTORE(SDValue Op,
+                                                 SelectionDAG &DAG) const {
+  MachineFunction &MF = DAG.getMachineFunction();
+  MF.getInfo<RISCVFunctionInfo>()->setManipulatesSP(true);
+  unsigned sp = Subtarget.isRV64() ? RISCV::sp_64 : RISCV::sp;
+  return DAG.getCopyToReg(Op.getOperand(0), SDLoc(Op), sp, Op.getOperand(1));
+}
+
+SDValue RISCVTargetLowering::
+lowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const {
+  // check the depth
+  assert((cast<ConstantSDNode>(Op.getOperand(0))->getZExtValue() == 0) &&
+         "Frame address can only be determined for current frame.");
+
+  MachineFrameInfo *MFI = DAG.getMachineFunction().getFrameInfo();
+  MFI->setFrameAddressIsTaken(true);
+  EVT VT = Op.getValueType();
+  SDLoc DL(Op);
+  SDValue FrameAddr = DAG.getCopyFromReg(DAG.getEntryNode(), DL,
+                                         Subtarget.isRV64() ? RISCV::fp_64 : RISCV::fp, VT);
+  return FrameAddr;
+}
+
+SDValue RISCVTargetLowering::LowerOperation(SDValue Op,
+                                              SelectionDAG &DAG) const {
+  switch (Op.getOpcode()) {
+  case ISD::RETURNADDR:
+    return lowerRETURNADDR(Op, DAG);
+  case ISD::SELECT_CC:
+    return lowerSELECT_CC(Op, DAG);
+  case ISD::GlobalAddress:
+    return lowerGlobalAddress(Op, DAG);
+  case ISD::GlobalTLSAddress:
+    return lowerGlobalTLSAddress(cast<GlobalAddressSDNode>(Op), DAG);
+  case ISD::BlockAddress:
+    return lowerBlockAddress(cast<BlockAddressSDNode>(Op), DAG);
+  case ISD::JumpTable:
+    return lowerJumpTable(cast<JumpTableSDNode>(Op), DAG);
+  case ISD::ConstantPool:
+    return lowerConstantPool(cast<ConstantPoolSDNode>(Op), DAG);
+  case ISD::VASTART:
+    return lowerVASTART(Op, DAG);
+  case ISD::VAARG:
+    return lowerVAARG(Op, DAG);
+  case ISD::ATOMIC_FENCE:
+    return lowerATOMIC_FENCE(Op, DAG);
+  case ISD::STACKSAVE:
+    return lowerSTACKSAVE(Op, DAG);
+  case ISD::STACKRESTORE:
+    return lowerSTACKRESTORE(Op, DAG);
+  case ISD::FRAMEADDR:
+    return lowerFRAMEADDR(Op, DAG);
+  default:
+    llvm_unreachable("Unexpected node to lower");
+  }
+}
+
+const char *RISCVTargetLowering::getTargetNodeName(unsigned Opcode) const {
+#define OPCODE(NAME) case RISCVISD::NAME: return "RISCVISD::" #NAME
+  switch (Opcode) {
+    OPCODE(RET_FLAG);
+    OPCODE(CALL);
+    OPCODE(PCREL_WRAPPER);
+    OPCODE(Hi);
+    OPCODE(Lo);
+    OPCODE(FENCE);
+    OPCODE(SELECT_CC);
+  }
+  return NULL;
+#undef OPCODE
+}
+
+//===----------------------------------------------------------------------===//
+// Custom insertion
+//===----------------------------------------------------------------------===//
+
+
+// Call pseduo ops for ABI compliant calls (output is always ra)
+MachineBasicBlock *RISCVTargetLowering::
+emitCALL(MachineInstr *MI, MachineBasicBlock *BB) const {
+  const TargetInstrInfo *TII = BB->getParent()->getSubtarget().getInstrInfo();
+  DebugLoc DL = MI->getDebugLoc();
+
+  unsigned jump;
+  unsigned RA;
+  switch(MI->getOpcode()) {
+  case RISCV::CALL:
+    jump = RISCV::JAL; RA = RISCV::ra; break;
+  case RISCV::CALLREG:
+    jump = RISCV::JALR; RA = RISCV::ra; break;
+  case RISCV::CALL64:
+    jump = RISCV::JAL64; RA = RISCV::ra_64; break;
+  case RISCV::CALLREG64:
+    jump = RISCV::JALR64; RA = RISCV::ra_64; break;
+  default:
+    llvm_unreachable("Unexpected call instr type to insert");
+  }
+  
+  MachineInstrBuilder jumpMI = BuildMI(*BB, MI, DL, TII->get(jump), RA);
+
+  //copy over other operands
+  for(unsigned i = 0; i < MI->getNumOperands(); i++){
+    jumpMI.addOperand(MI->getOperand(i));
+  }
+  MI->eraseFromParent();
+
+  return BB;
+}
+
+MachineBasicBlock *RISCVTargetLowering::
+emitSelectCC(MachineInstr *MI, MachineBasicBlock *BB) const {
+
+  const TargetInstrInfo *TII = BB->getParent()->getSubtarget().getInstrInfo();
+  DebugLoc DL = MI->getDebugLoc();
+
+  // To "insert" a SELECT_CC instruction, we actually have to insert the
+  // diamond control-flow pattern.  The incoming instruction knows the
+  // destination vreg to set, the condition code register to branch on, the
+  // true/false values to select between, and a branch opcode to use.
+  const BasicBlock *LLVM_BB = BB->getBasicBlock();
+  MachineFunction::iterator It = BB;
+  ++It;
+
+  //  thisMBB:
+  //  ...
+  //   TrueVal = ...
+  //   setcc r1, r2, r3
+  //   bNE   r1, r0, copy1MBB
+  //   fallthrough --> copy0MBB
+  MachineBasicBlock *thisMBB  = BB;
+  MachineFunction *F = BB->getParent();
+  MachineBasicBlock *copy0MBB = F->CreateMachineBasicBlock(LLVM_BB);
+  MachineBasicBlock *sinkMBB  = F->CreateMachineBasicBlock(LLVM_BB);
+  F->insert(It, copy0MBB);
+  F->insert(It, sinkMBB);
+
+  // Transfer the remainder of BB and its successor edges to sinkMBB.
+  sinkMBB->splice(sinkMBB->begin(), BB,
+                  std::next(MachineBasicBlock::iterator(MI)),
+                  BB->end());
+  sinkMBB->transferSuccessorsAndUpdatePHIs(BB);
+
+  // Next, add the true and fallthrough blocks as its successors.
+  BB->addSuccessor(copy0MBB);
+  BB->addSuccessor(sinkMBB);
+
+  const TargetRegisterInfo *TRI = getTargetMachine().getSubtargetImpl(*F->getFunction())->getRegisterInfo();
+  const TargetRegisterClass *RC = MI->getRegClassConstraint(1, TII, TRI);
+  unsigned bne = RC == &RISCV::GR64BitRegClass ? RISCV::BNE64 : RISCV::BNE;
+  unsigned zero = RC == &RISCV::GR64BitRegClass ? RISCV::zero_64 : RISCV::zero;
+  BuildMI(BB, DL, TII->get(bne)).addMBB(sinkMBB).addReg(zero).addReg(MI->getOperand(1).getReg());
+
+  //  copy0MBB:
+  //   %FalseValue = ...
+  //   # fallthrough to sinkMBB
+  BB = copy0MBB;
+
+  // Update machine-CFG edges
+  BB->addSuccessor(sinkMBB);
+
+  //  sinkMBB:
+  //   %Result = phi [ %TrueValue, thisMBB ], [ %FalseValue, copy0MBB ]
+  //  ...
+  BB = sinkMBB;
+
+  //assume there is only one use of zero
+  if(MI->getOperand(0).getReg() == RISCV::zero ||
+     MI->getOperand(0).getReg() == RISCV::zero_64){
+    //Create a virtual register for zero and make a copy into it
+    const TargetRegisterClass *RC = MI->getOperand(0).getReg() == RISCV::zero_64 ? &RISCV::GR64BitRegClass : &RISCV::GR32BitRegClass;
+    unsigned VReg = F->getRegInfo().createVirtualRegister(RC);
+    BuildMI(*copy0MBB, copy0MBB->begin(), DL, TII->get(TargetOpcode::COPY), VReg)
+      .addReg(MI->getOperand(0).getReg());
+    //Do the actual phi using the virtual reg now
+    BuildMI(*BB, BB->begin(), DL,
+            TII->get(RISCV::PHI), VReg)
+      .addReg(MI->getOperand(3).getReg()).addMBB(copy0MBB)
+      .addReg(MI->getOperand(2).getReg()).addMBB(thisMBB);
+  }else if(MI->getOperand(2).getReg() == RISCV::zero || 
+           MI->getOperand(2).getReg() == RISCV::zero_64){
+    //Create a virtual register for zero and make a copy into it
+    const TargetRegisterClass *RC = MI->getOperand(2).getReg() == RISCV::zero_64 ? &RISCV::GR64BitRegClass : &RISCV::GR32BitRegClass;
+    unsigned VReg = F->getRegInfo().createVirtualRegister(RC);
+    BuildMI(*copy0MBB, copy0MBB->begin(), DL, TII->get(TargetOpcode::COPY), VReg)
+      .addReg(MI->getOperand(2).getReg());
+    //Do the actual phi using the virtual reg now
+    BuildMI(*BB, BB->begin(), DL,
+            TII->get(RISCV::PHI), MI->getOperand(0).getReg())
+      .addReg(MI->getOperand(3).getReg()).addMBB(thisMBB)
+      .addReg(VReg).addMBB(copy0MBB);
+  }else if(MI->getOperand(3).getReg() == RISCV::zero ||
+           MI->getOperand(3).getReg() == RISCV::zero_64){
+    //Create a virtual register for zero and make a copy into it
+    const TargetRegisterClass *RC = MI->getOperand(3).getReg() == RISCV::zero_64 ? &RISCV::GR64BitRegClass : &RISCV::GR32BitRegClass;
+    unsigned VReg = F->getRegInfo().createVirtualRegister(RC);
+    BuildMI(*copy0MBB, copy0MBB->begin(), DL, TII->get(TargetOpcode::COPY), VReg)
+      .addReg(MI->getOperand(3).getReg());
+    //Do the actual phi using the virtual reg now
+    BuildMI(*BB, BB->begin(), DL,
+            TII->get(RISCV::PHI), MI->getOperand(0).getReg())
+      .addReg(VReg).addMBB(copy0MBB)
+      .addReg(MI->getOperand(2).getReg()).addMBB(thisMBB);
+  }else{
+    //None of the registers is zero so everything is already a virt reg
+    BuildMI(*BB, BB->begin(), DL,
+            TII->get(RISCV::PHI), MI->getOperand(0).getReg())
+      .addReg(MI->getOperand(3).getReg()).addMBB(copy0MBB)
+      .addReg(MI->getOperand(2).getReg()).addMBB(thisMBB);
+  }
+
+  MI->eraseFromParent();   // The pseudo instruction is gone now.
+  return BB;
+}
+
+MachineBasicBlock *RISCVTargetLowering::
+EmitInstrWithCustomInserter(MachineInstr *MI, MachineBasicBlock *MBB) const {
+  switch (MI->getOpcode()) {
+  case RISCV::SELECT_CC:
+  case RISCV::SELECT_CC64:
+  case RISCV::FSELECT_CC_F:
+  case RISCV::FSELECT_CC_D:
+      return emitSelectCC(MI, MBB);
+  case RISCV::CALL:
+  case RISCV::CALLREG:
+  case RISCV::CALL64:
+  case RISCV::CALLREG64:
+      return emitCALL(MI, MBB);
+  default:
+    llvm_unreachable("Unexpected instr type to insert");
+  }
+}
diff --git a/lib/Target/RISCV/RISCVISelLowering.h b/lib/Target/RISCV/RISCVISelLowering.h
new file mode 100644
index 0000000..626a6c0
--- /dev/null
+++ b/lib/Target/RISCV/RISCVISelLowering.h
@@ -0,0 +1,215 @@
+//===-- RISCVISelLowering.h - RISCV DAG lowering interface ------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file defines the interfaces that RISCV uses to lower LLVM code into a
+// selection DAG.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_ISELLOWERING_H
+#define LLVM_LIB_TARGET_RISCV_ISELLOWERING_H
+
+#include "RISCV.h"
+#include "llvm/CodeGen/CallingConvLower.h"
+#include "llvm/CodeGen/SelectionDAG.h"
+#include "llvm/IR/Function.h"
+#include "llvm/Target/TargetLowering.h"
+#include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+#include <string>
+#include <deque>
+
+namespace llvm {
+namespace RISCVISD {
+  enum {
+    FIRST_NUMBER = ISD::BUILTIN_OP_END,
+
+    // Return with a flag operand.  Operand 0 is the chain operand.
+    RET_FLAG,
+
+    // Calls a function.  Operand 0 is the chain operand and operand 1
+    // is the target address.  The arguments start at operand 2.
+    // There is an optional glue operand at the end.
+    CALL,
+
+    // Jump and link to Operand 0 is the chain operand and operand 1
+    // is the register to store the return address. Operand 2 is the target address
+    JAL,
+
+    // Wraps a TargetGlobalAddress that should be loaded using PC-relative
+    // accesses (AUIPC).  Operand 0 is the address.
+    PCREL_WRAPPER,
+
+    // Get the Higher 20 bits from a 32-bit immediate
+    // No relation with Mips Hi register
+    Hi,
+
+    // Get the Lower 12 bits from a 32-bit immediate
+    // No relation with Mips Lo register
+    Lo,
+
+    // TprelHi and TprelLo nodes are used to handle Local Exec TLS
+    TprelHi,
+    TprelLo,
+
+    // Branches if a condition is true.  Operand 0 is the chain operand;
+    // operand 1 is the 4-bit condition-code mask, with bit N in
+    // big-endian order meaning "branch if CC=N"; operand 2 is the
+    // target block and operand 3 is the flag operand.
+    BRCOND,
+
+    // Selects between operand 0 and operand 1.  Operand 2 is the
+    // mask of condition-code values for which operand 0 should be
+    // chosen over operand 1; it has the same form as BR_CCMASK.
+    // Operand 3 is the flag operand.
+    SELECT_CC,
+
+    FENCE,
+
+    // Wrappers around the inner loop of an 8- or 16-bit ATOMIC_SWAP or
+    // ATOMIC_LOAD_<op>.
+    //
+    // Operand 0: the address of the containing 32-bit-aligned field
+    // Operand 1: the second operand of <op>, in the high bits of an i32
+    //            for everything except ATOMIC_SWAPW
+    // Operand 2: how many bits to rotate the i32 left to bring the first
+    //            operand into the high bits
+    // Operand 3: the negative of operand 2, for rotating the other way
+    // Operand 4: the width of the field in bits (8 or 16)
+    ATOMIC_SWAPW = ISD::FIRST_TARGET_MEMORY_OPCODE,
+    ATOMIC_LOADW_ADD,
+    ATOMIC_LOADW_SUB,
+    ATOMIC_LOADW_AND,
+    ATOMIC_LOADW_OR,
+    ATOMIC_LOADW_XOR,
+    ATOMIC_LOADW_NAND,
+    ATOMIC_LOADW_MIN,
+    ATOMIC_LOADW_MAX,
+    ATOMIC_LOADW_UMIN,
+    ATOMIC_LOADW_UMAX
+  };
+}
+
+class RISCVSubtarget;
+
+class RISCVTargetLowering : public TargetLowering {
+public:
+  explicit RISCVTargetLowering(const TargetMachine &TM, const RISCVSubtarget &STI);
+
+  // Override TargetLowering.
+  MVT getScalarShiftAmountTy(const DataLayout &, EVT LHSTy) const override {
+    return LHSTy.getSizeInBits() <= 32 ? MVT::i32 : MVT::i64;
+  }
+  EVT getSetCCResultType(const DataLayout &, LLVMContext &, EVT VT) const override {
+    return MVT::i32;
+  }
+  bool isFMAFasterThanFMulAndFAdd(EVT) const override {
+    return true;
+  }
+  bool isOffsetFoldingLegal(const GlobalAddressSDNode *GA) const;
+  bool isFPImmLegal(const APFloat &Imm, EVT VT) const override;
+  const char *getTargetNodeName(unsigned Opcode) const override;
+  std::pair<unsigned, const TargetRegisterClass *>
+  getRegForInlineAsmConstraint(const TargetRegisterInfo *TRI,
+                               StringRef Constraint,
+                               MVT VT) const override;
+  TargetLowering::ConstraintType
+  getConstraintType(StringRef Constraint) const override;
+  TargetLowering::ConstraintWeight
+  getSingleConstraintMatchWeight(AsmOperandInfo &info,
+                                 const char *constraint) const override;
+  void LowerAsmOperandForConstraint(SDValue Op, std::string &Constraint,
+                                    std::vector<SDValue> &Ops,
+                                    SelectionDAG &DAG) const override;
+  MachineBasicBlock *
+  EmitInstrWithCustomInserter(MachineInstr *MI,
+                              MachineBasicBlock *BB) const override;
+  SDValue LowerOperation(SDValue Op, SelectionDAG &DAG) const override;
+  SDValue LowerFormalArguments(SDValue Chain, CallingConv::ID CallConv,
+                               bool isVarArg,
+                               const SmallVectorImpl<ISD::InputArg> &Ins,
+                               SDLoc DL, SelectionDAG &DAG,
+                               SmallVectorImpl<SDValue> &InVals) const override;
+  SDValue LowerCall(CallLoweringInfo &CLI,
+                    SmallVectorImpl<SDValue> &InVals) const override;
+
+  virtual bool
+    CanLowerReturn(CallingConv::ID CallConv, MachineFunction &MF,
+                   bool isVarArg,
+                   const SmallVectorImpl<ISD::OutputArg> &Outs,
+                   LLVMContext &Context) const;
+
+  SDValue LowerReturn(SDValue Chain, CallingConv::ID CallConv, bool IsVarArg,
+                      const SmallVectorImpl<ISD::OutputArg> &Outs,
+                      const SmallVectorImpl<SDValue> &OutVals, SDLoc DL,
+                      SelectionDAG &DAG) const override;
+
+    struct LTStr {
+      bool operator()(const char *S1, const char *S2) const {
+        return strcmp(S1, S2) < 0;
+      }
+    };
+
+    /// ByValArgInfo - Byval argument information.
+    struct ByValArgInfo {
+      unsigned FirstIdx; // Index of the first register used.
+      unsigned NumRegs;  // Number of registers used for this argument.
+      unsigned Address;  // Offset of the stack area used to pass this argument.
+
+      ByValArgInfo() : FirstIdx(0), NumRegs(0), Address(0) {}
+    };
+
+private:
+  const RISCVSubtarget &Subtarget;
+public:
+  bool IsRV32;
+private:
+
+  // Implement LowerOperation for individual opcodes.
+  SDValue lowerSELECT_CC(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerRETURNADDR(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerGlobalAddress(SDValue Op,
+                             SelectionDAG &DAG) const;
+  SDValue lowerGlobalTLSAddress(GlobalAddressSDNode *Node,
+                                SelectionDAG &DAG) const;
+  SDValue lowerBlockAddress(BlockAddressSDNode *Node,
+                            SelectionDAG &DAG) const;
+  SDValue lowerJumpTable(JumpTableSDNode *JT, SelectionDAG &DAG) const;
+  SDValue lowerConstantPool(ConstantPoolSDNode *CP, SelectionDAG &DAG) const;
+  SDValue lowerVASTART(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerVAARG(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerDYNAMIC_STACKALLOC(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerBITCAST(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerOR(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerATOMIC_FENCE(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerATOMIC_LOAD(SDValue Op, SelectionDAG &DAG,
+                           unsigned Opcode) const;
+  SDValue lowerSTACKSAVE(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerSTACKRESTORE(SDValue Op, SelectionDAG &DAG) const;
+  SDValue lowerFRAMEADDR(SDValue Op, SelectionDAG &DAG) const;
+
+  // Helper functions for above
+  SDValue getTargetNode(SDValue Op, SelectionDAG &DAG, unsigned Flag) const;
+  SDValue getAddrNonPIC(SDValue Op, SelectionDAG &DAG) const;
+  SDValue getAddrPIC(SDValue Op, SelectionDAG &DAG) const;
+
+  // Implement EmitInstrWithCustomInserter for individual operation types.
+  MachineBasicBlock *emitCALL(MachineInstr *MI,
+                                MachineBasicBlock *BB) const;
+  MachineBasicBlock *emitSelectCC(MachineInstr *MI,
+                                MachineBasicBlock *BB) const;
+
+};
+
+class RISCVTargetObjectFile : public TargetLoweringObjectFileELF {
+  void Initialize(MCContext &Ctx, const TargetMachine &TM);
+};
+
+} // end namespace llvm
+
+#endif
diff --git a/lib/Target/RISCV/RISCVInstrBuilder.h b/lib/Target/RISCV/RISCVInstrBuilder.h
new file mode 100644
index 0000000..2d1e318
--- /dev/null
+++ b/lib/Target/RISCV/RISCVInstrBuilder.h
@@ -0,0 +1,47 @@
+//===-- RISCVInstrBuilder.h - Functions to aid building insts ---*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file exposes functions that may be used with BuildMI from the
+// MachineInstrBuilder.h file to handle RISCV'isms in a clean way.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVINSTRBUILDER_H
+#define LLVM_LIB_TARGET_RISCV_RISCVINSTRBUILDER_H
+
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineMemOperand.h"
+#include "llvm/CodeGen/PseudoSourceValue.h"
+
+namespace llvm {
+
+/// Add a BDX memory reference for frame object FI to MIB.
+static inline const MachineInstrBuilder &
+addFrameReference(const MachineInstrBuilder &MIB, int FI) {
+  MachineInstr *MI = MIB;
+  MachineFunction &MF = *MI->getParent()->getParent();
+  MachineFrameInfo *MFFrame = MF.getFrameInfo();
+  const MCInstrDesc &MCID = MI->getDesc();
+  unsigned Flags = 0;
+  if (MCID.mayLoad())
+    Flags |= MachineMemOperand::MOLoad;
+  if (MCID.mayStore())
+    Flags |= MachineMemOperand::MOStore;
+  int64_t Offset = 0;
+  MachineMemOperand *MMO =
+    MF.getMachineMemOperand(MachinePointerInfo::getFixedStack(FI, Offset),
+                            Flags, MFFrame->getObjectSize(FI),
+                            MFFrame->getObjectAlignment(FI));
+  return MIB.addImm(Offset).addFrameIndex(FI).addMemOperand(MMO);
+}
+
+} // End llvm namespace
+
+#endif
diff --git a/lib/Target/RISCV/RISCVInstrFormats.td b/lib/Target/RISCV/RISCVInstrFormats.td
new file mode 100644
index 0000000..7242d73
--- /dev/null
+++ b/lib/Target/RISCV/RISCVInstrFormats.td
@@ -0,0 +1,270 @@
+//===- RISCVInstrFormats.td - RISCV Instruction Formats ----*- tablegen -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// Basic RISCV instruction definition
+//===----------------------------------------------------------------------===//
+
+class InstRISCV<int size, dag outs, dag ins, string asmstr,
+                  list<dag> pattern> : Instruction {
+  let Namespace = "RISCV";
+
+  dag OutOperandList = outs;
+  dag InOperandList = ins;
+  let Size = size;
+  let Pattern = pattern;
+  let AsmString = asmstr;
+
+  let AddedComplexity = 1;
+
+  // Used to identify a group of related instructions, such as ST and STY.
+  string Function = "";
+
+  // "12" for an instruction that has a ...Y equivalent, "20" for that
+  // ...Y equivalent.
+  string PairType = "none";
+
+  // True if this instruction is a simple load of a register
+  // (with no sign or zero extension).
+  bit SimpleLoad = 0;
+
+  // True if this instruction is a simple store of a register
+  // (with no truncation).
+  bit SimpleStore = 0;
+
+  let TSFlags{0} = SimpleLoad;
+  let TSFlags{1} = SimpleStore;
+}
+
+/***************
+*RISCV Instruction Formats
+*/
+
+//R-Type
+class InstR<string mnemonic, bits<7> op, bits<7> funct7, bits<3> funct3,
+            SDPatternOperator operator, RegisterOperand cls1, 
+            RegisterOperand cls2>
+  : InstRISCV<4, (outs cls1:$dst), (ins cls2:$src1, cls2:$src2),
+                mnemonic#"\t$dst, $src1, $src2", 
+                [(set cls1:$dst, (operator cls2:$src1, cls2:$src2))]> {
+  field bits<32> Inst;
+
+  bits<5> RD;
+  bits<5> RS1;
+  bits<5> RS2;
+
+  let Inst{31-25} = funct7;
+  let Inst{24-20} = RS2;
+  let Inst{19-15} = RS1;
+  let Inst{14-12} = funct3;
+  let Inst{11- 7} = RD;
+  let Inst{6 - 0} = op;
+}
+
+//LR/SC
+class InstLR<string mnemonic, bits<3> funct3,
+             RegisterOperand cls1, Operand cls2>
+  : InstRISCV<4, (outs cls1:$dst), (ins cls2:$src2), 
+                mnemonic#"\t$dst, $src2", 
+                []> {
+  field bits<32> Inst;
+
+  bits<5> RD;
+  bits<5> RS1;
+
+  let Inst{31-27} = 0b00010;
+  let Inst{26} = 0;//aq
+  let Inst{25} = 0;//rl
+  let Inst{24-20} = 0b00000;
+  let Inst{19-15} = RS1;
+  let Inst{14-12} = funct3;
+  let Inst{11- 7} = RD;
+  let Inst{6 - 0} = 0b0101111;
+}
+
+class InstSC<string mnemonic, bits<3> funct3,
+             RegisterOperand reg, Operand memOp>
+  : InstRISCV<4, (outs reg:$dst), (ins reg:$src2, memOp:$src1), 
+                mnemonic#"\t$dst, $src2, $src1", 
+                []> {
+  field bits<32> Inst;
+
+  bits<5> RD;
+  bits<5> RS1;
+  bits<5> RS2;
+
+  let Inst{31-27} = 0b00011;
+  let Inst{26} = 0;//aq
+  let Inst{25} = 0;//rl
+  let Inst{24-20} = RS2;
+  let Inst{19-15} = RS1;
+  let Inst{14-12} = funct3;
+  let Inst{11- 7} = RD;
+  let Inst{6 - 0} = 0b0101111;
+}
+
+//A-Type
+class InstA<string mnemonic, bits<7> op, bits<5> funct5, bits<3> funct3,
+            SDPatternOperator operator, RegisterOperand cls1, 
+            Operand cls2>
+  : InstRISCV<4, (outs cls1:$dst), (ins cls1:$src1, cls2:$src2), 
+                mnemonic#"\t$dst, $src1, $src2", 
+                [(set cls1:$dst, (operator regaddr:$src2, cls1:$src1))]> {
+  field bits<32> Inst;
+
+  bits<5> RD;
+  bits<5> RS1;
+  bits<5> RS2;
+
+  let Inst{31-27} = funct5;
+  let Inst{26} = 0;//aq
+  let Inst{25} = 0;//rl
+  let Inst{24-20} = RS2;
+  let Inst{19-15} = RS1;
+  let Inst{14-12} = funct3;
+  let Inst{11- 7} = RD;
+  let Inst{6 - 0} = op;
+}
+
+//Load-Type
+class InstLoad<string mnemonic, bits<7> op, bits<3> funct3,
+            SDPatternOperator opNode,
+            RegisterOperand cls1,
+            Operand memOp>
+  : InstRISCV<4, (outs cls1:$dst), (ins memOp:$addr), 
+                mnemonic#"\t$dst, $addr", 
+                [(set cls1:$dst, (opNode addr:$addr))]> {
+  field bits<32> Inst;
+
+  bits<5> RD;
+  bits<5> RS1;
+  bits<12> IMM;
+
+  let Inst{31-27} = RD;
+  let Inst{26-22} = RS1;
+  let Inst{21-17} = IMM{11-7};
+  let Inst{16-10} = IMM{6 -0};
+  let Inst{9 - 7} = funct3;
+  let Inst{6 - 0} = op;
+}
+
+class InstStore<string mnemonic, bits<7> op, bits<3> funct3,
+                SDPatternOperator opNode,
+                RegisterOperand cls1,
+                Operand memOp>
+  : InstRISCV<4, (outs), (ins cls1:$src, memOp:$addr),
+              mnemonic#"\t$src, $addr", 
+              [(opNode cls1:$src, addr:$addr)]> {
+  field bits<32> Inst;
+
+  bits<5> RS2;
+  bits<5> RS1;
+  bits<12> IMM;
+
+  let Inst{31-27} = IMM{11-7};
+  let Inst{26-22} = RS1;
+  let Inst{21-17} = RS2;
+  let Inst{16-10} = IMM{6 -0};
+  let Inst{9 - 7} = funct3;
+  let Inst{6 - 0} = op;
+}
+
+//I-Type
+class InstI<string mnemonic, bits<7> op, bits<3> funct3,
+            SDPatternOperator operator, RegisterOperand cls1, RegisterOperand cls2,
+            Immediate imm>
+  : InstRISCV<4, (outs cls1:$dst), (ins cls2:$src1, imm:$src2), 
+                mnemonic#"\t$dst, $src1, $src2", 
+                [(set cls1:$dst, (operator cls2:$src1, imm:$src2))]> {
+  field bits<32> Inst;
+
+  bits<5> RD;
+  bits<5> RS1;
+  bits<12> IMM;
+
+  let Inst{31-20} = IMM;
+  let Inst{19-15} = RS1;
+  let Inst{14-12} = funct3;
+  let Inst{11- 7} = RD;
+  let Inst{6 - 0} = op;
+}
+
+//ISYS-Type
+class InstISYS<string mnemonic, bits<12> funct12,
+             RegisterOperand cls1>
+  : InstRISCV<4, (outs cls1:$dst), (ins), 
+                mnemonic#"\t$dst", 
+                []> {
+  field bits<32> Inst;
+
+  bits<5> RD;
+
+  let Inst{31-20} = funct12;
+  let Inst{19-15} = 0b00000;
+  let Inst{14-12} = 0b010;
+  let Inst{11- 7} = RD;
+  let Inst{6 - 0} = 0b1110011;
+}
+
+//B-Type, too different to consolidate further
+class InstB<bits<7> op, bits<3> funct3, dag outs, dag ins, string asmstr, list<dag> pattern>
+  : InstRISCV<4, outs, ins, asmstr, pattern> {
+  field bits<32> Inst;
+
+  bits<12> IMM;
+  bits<5> RS1;
+  bits<5> RS2;
+
+  let Inst{31-27} = IMM{11-7};
+  let Inst{26-22} = RS1;
+  let Inst{21-17} = RS2;
+  let Inst{16-10} = IMM{6 -0};
+  let Inst{9 - 7} = funct3;
+  let Inst{6 - 0} = op;
+}
+
+//U-Type, only two instructions fit here so no further condensation
+class InstU<bits<7> op, dag outs, dag ins, string asmstr, list<dag> pattern>
+  : InstRISCV<4, outs, ins, asmstr, pattern> {
+  field bits<32> Inst;
+
+  bits<5> RD;
+  bits<20> IMM;
+
+  let Inst{31-12} = IMM{19-0};
+  let Inst{11- 7} = RD;
+  let Inst{6 - 0} = op;
+}
+
+//J-Type, only 2 instructions no further consolidation
+class InstJ<bits<7> op, dag outs, dag ins, string asmstr, list<dag> pattern>
+  : InstRISCV<4, outs, ins, asmstr, pattern> {
+  field bits<32> Inst;
+
+  bits<25> IMM;
+
+  let Inst{31- 7} = IMM{24-0};
+  let Inst{6 - 0} = op;
+}
+
+//===----------------------------------------------------------------------===//
+// Pseudo instructions
+//===----------------------------------------------------------------------===//
+//
+// Convenience instructions that get lowered to real instructions
+// by either RISCVTargetLowering::EmitInstrWithCustomInserter()
+// or RISCVInstrInfo::expandPostRAPseudo().
+//
+//===----------------------------------------------------------------------===//
+
+class Pseudo<dag outs, dag ins, list<dag> pattern>
+  : InstRISCV<0, outs, ins, "", pattern> {
+  let isPseudo = 1;
+  let isCodeGenOnly = 1;
+}
diff --git a/lib/Target/RISCV/RISCVInstrInfo.cpp b/lib/Target/RISCV/RISCVInstrInfo.cpp
new file mode 100644
index 0000000..942b444
--- /dev/null
+++ b/lib/Target/RISCV/RISCVInstrInfo.cpp
@@ -0,0 +1,629 @@
+//===-- RISCVInstrInfo.cpp - RISCV instruction information ------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the RISCV implementation of the TargetInstrInfo class.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVInstrInfo.h"
+#include "RISCVInstrBuilder.h"
+#include "RISCVTargetMachine.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+
+#define GET_INSTRINFO_CTOR_DTOR
+#define GET_INSTRMAP_INFO
+#include "RISCVGenInstrInfo.inc"
+
+using namespace llvm;
+
+RISCVInstrInfo::RISCVInstrInfo(RISCVSubtarget &sti)
+  : RISCVGenInstrInfo(RISCV::ADJCALLSTACKDOWN, RISCV::ADJCALLSTACKUP),
+    RI(sti), STI(sti) {
+}
+
+// If MI is a simple load or store for a frame object, return the register
+// it loads or stores and set FrameIndex to the index of the frame object.
+// Return 0 otherwise.
+//
+// Flag is SimpleLoad for loads and SimpleStore for stores.
+static int isSimpleMove(const MachineInstr *MI, int &FrameIndex, int Flag) {
+  const MCInstrDesc &MCID = MI->getDesc();
+  if ((MCID.TSFlags & Flag) &&
+      MI->getOperand(1).isFI() &&
+      MI->getOperand(2).getImm() == 0 &&
+      MI->getOperand(3).getReg() == 0) {
+    FrameIndex = MI->getOperand(1).getIndex();
+    return MI->getOperand(0).getReg();
+  }
+  return 0;
+}
+
+unsigned RISCVInstrInfo::isLoadFromStackSlot(const MachineInstr *MI,
+                                               int &FrameIndex) const {
+  return isSimpleMove(MI, FrameIndex, RISCVII::SimpleLoad);
+}
+
+unsigned RISCVInstrInfo::isStoreToStackSlot(const MachineInstr *MI,
+                                              int &FrameIndex) const {
+  return isSimpleMove(MI, FrameIndex, RISCVII::SimpleStore);
+}
+
+/// Adjust SP by Amount bytes.
+void RISCVInstrInfo::adjustStackPtr(unsigned SP, int64_t Amount,
+                                     MachineBasicBlock &MBB,
+                                     MachineBasicBlock::iterator I) const {
+  DebugLoc DL = I != MBB.end() ? I->getDebugLoc() : DebugLoc();
+  unsigned ADD =  STI.isRV64() ? RISCV::ADD64 : RISCV::ADD;
+  unsigned ADDI = STI.isRV64() ? RISCV::ADDI64 : RISCV::ADDI;
+
+  if (isInt<12>(Amount))// addi sp, sp, amount
+    BuildMI(MBB, I, DL, get(ADDI), SP).addReg(SP).addImm(Amount);
+  else { // Expand immediate that doesn't fit in 12-bit.
+    unsigned Reg;
+    loadImmediate(MBB, I, &Reg, Amount);
+    BuildMI(MBB, I, DL, get(ADD), SP).addReg(SP).addReg(Reg, RegState::Kill);
+  }
+}
+
+unsigned RISCVInstrInfo::GetInstSizeInBytes(MachineInstr *I) const {
+  //Since we don't have variable length instructions this just looks at the subtarget
+  //TODO:check for C
+  return (STI.isRV64() || STI.isRV32()) ? 4 : 4;
+}
+
+bool RISCVInstrInfo::AnalyzeBranch(MachineBasicBlock &MBB,
+                                     MachineBasicBlock *&TBB,
+                                     MachineBasicBlock *&FBB,
+                                     SmallVectorImpl<MachineOperand> &Cond,
+                                     bool AllowModify) const {
+  // Most of the code and comments here are boilerplate.
+
+  // Start from the bottom of the block and work up, examining the
+  // terminator instructions.
+  MachineBasicBlock::iterator I = MBB.end();
+  while (I != MBB.begin()) {
+    --I;
+    if (I->isDebugValue())
+      continue;
+
+    // Working from the bottom, when we see a non-terminator instruction, we're
+    // done.
+    if (!isUnpredicatedTerminator(I))
+      break;
+
+    // A terminator that isn't a branch can't easily be handled by this
+    // analysis.
+    SmallVector<MachineOperand, 4> ThisCond;
+    ThisCond.push_back(MachineOperand::CreateImm(0));
+    const MachineOperand *ThisTarget;
+    if (!isBranch(I, ThisCond, ThisTarget))
+      return true;
+
+    // Can't handle indirect branches.
+    if (!ThisTarget->isMBB())
+      return true;
+
+    if (ThisCond[0].getImm() == RISCV::CCMASK_ANY) {
+      // Handle unconditional branches.
+      if (!AllowModify) {
+        TBB = ThisTarget->getMBB();
+        continue;
+      }
+
+      // If the block has any instructions after a JMP, delete them.
+      while (std::next(I) != MBB.end())
+        std::next(I)->eraseFromParent();
+
+      Cond.clear();
+      FBB = 0;
+
+      // Delete the JMP if it's equivalent to a fall-through.
+/*We can't do this now because the BBs can still be rearranged
+      if (MBB.isLayoutSuccessor(ThisTarget->getMBB())) {
+        TBB = 0;
+        I->eraseFromParent();
+        I = MBB.end();
+        continue;
+      }
+*/
+
+      // TBB is used to indicate the unconditinal destination.
+      TBB = ThisTarget->getMBB();
+      continue;
+    }
+
+    // Working from the bottom, handle the first conditional branch.
+    if (Cond.empty()) {
+      // FIXME: add X86-style branch swap
+      FBB = TBB;
+      TBB = ThisTarget->getMBB();
+      Cond.push_back(MachineOperand::CreateImm(ThisCond[0].getImm()));
+      //push remaining operands
+      for (unsigned int i=0; i<(I->getNumExplicitOperands()); i++)
+        Cond.push_back(I->getOperand(i));
+
+      continue;
+    }
+
+    // Handle subsequent conditional branches.
+    assert(Cond.size() <= 4);
+    assert(TBB);
+
+    // Only handle the case where all conditional branches branch to the same
+    // destination.
+    if (TBB != ThisTarget->getMBB())
+      return true;
+
+    // If the conditions are the same, we can leave them alone.
+    unsigned OldCond = Cond[0].getImm();
+    if (OldCond == ThisCond[0].getImm())
+      continue;
+
+    // FIXME: Try combining conditions like X86 does.
+  }
+
+  return false;
+}
+
+unsigned RISCVInstrInfo::RemoveBranch(MachineBasicBlock &MBB) const {
+  // Most of the code and comments here are boilerplate.
+  MachineBasicBlock::iterator I = MBB.end();
+  unsigned Count = 0;
+
+  while (I != MBB.begin()) {
+    --I;
+    if (I->isDebugValue())
+      continue;
+    SmallVector<MachineOperand, 4> Cond;
+    Cond.push_back(MachineOperand::CreateImm(0));
+    const MachineOperand *Target;
+    if (!isBranch(I, Cond, Target))
+      break;
+    if (!Target->isMBB())
+      break;
+    // Remove the branch.
+    I->eraseFromParent();
+    I = MBB.end();
+    ++Count;
+  }
+
+  return Count;
+}
+
+unsigned
+RISCVInstrInfo::InsertBranch(MachineBasicBlock &MBB, MachineBasicBlock *TBB,
+                               MachineBasicBlock *FBB,
+                               ArrayRef<MachineOperand> Cond,
+                               DebugLoc DL) const {
+  if (FBB) {
+    //Need to build two branches then
+    //one to branch to TBB on Cond
+    //and a second one immediately after to unconditionally jump to FBB
+    unsigned count = InsertBranchAtInst(MBB, MBB.end(), TBB, Cond, DL);
+    BuildMI(&MBB, DL, get(RISCV::J)).addMBB(FBB);
+    count++;
+    return count;
+  }
+  //This function inserts the branch at the end of the MBB
+  return InsertBranchAtInst(MBB, MBB.end(), TBB, Cond, DL);
+}
+unsigned
+RISCVInstrInfo::InsertConstBranchAtInst(MachineBasicBlock &MBB, MachineInstr *I, int64_t offset,
+                               ArrayRef<MachineOperand> Cond,
+                               DebugLoc DL) const {
+  // Shouldn't be a fall through.
+  assert(&MBB && "InsertBranch must not be told to insert a fallthrough");
+  assert(Cond.size() <= 4 &&
+         "RISCV branch conditions have less than four components!");
+
+  if (Cond.empty() || Cond[0].getImm() == RISCV::CCMASK_ANY) {
+    // Unconditional branch
+    BuildMI(MBB, I, DL, get(RISCV::J)).addImm(offset);
+    return 1;
+  }
+
+  // Conditional branch.
+  unsigned Count = 0;
+  unsigned CC = Cond[0].getImm();
+  switch(CC) {
+    case RISCV::CCMASK_CMP_EQ:
+      BuildMI(MBB, I, DL, get(RISCV::BEQ)).addImm(offset).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_NE:
+      BuildMI(MBB, I, DL, get(RISCV::BNE)).addImm(offset).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_LT:
+      BuildMI(MBB, I, DL, get(RISCV::BLT)).addImm(offset).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case (RISCV::CCMASK_CMP_LT | RISCV::CCMASK_CMP_UO):
+      BuildMI(MBB, I, DL, get(RISCV::BLTU)).addImm(offset).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_GE:
+      BuildMI(MBB, I, DL, get(RISCV::BGE)).addImm(offset).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case (RISCV::CCMASK_CMP_GE | RISCV::CCMASK_CMP_UO):
+      BuildMI(MBB, I, DL, get(RISCV::BGEU)).addImm(offset).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    //synth
+    case RISCV::CCMASK_CMP_GT:
+      BuildMI(MBB, I, DL, get(RISCV::BGT)).addImm(offset).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_LE:
+      BuildMI(MBB, I, DL, get(RISCV::BLE)).addImm(offset).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_GT | RISCV::CCMASK_CMP_UO:
+      BuildMI(MBB, I, DL, get(RISCV::BGTU)).addImm(offset).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_LE | RISCV::CCMASK_CMP_UO:
+      BuildMI(MBB, I, DL, get(RISCV::BLEU)).addImm(offset).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    default:
+      llvm_unreachable("Invalid branch condition code!");
+  }
+  ++Count;
+  return Count;
+}
+
+unsigned
+RISCVInstrInfo::InsertBranchAtInst(MachineBasicBlock &MBB, MachineInstr *I,
+                               MachineBasicBlock *TBB, ArrayRef<MachineOperand> Cond,
+                               DebugLoc DL) const {
+  // Shouldn't be a fall through.
+  assert(TBB && "InsertBranch must not be told to insert a fallthrough");
+  assert(Cond.size() <= 4 &&
+         "RISCV branch conditions have less than four components!");
+
+  if (Cond.empty() || Cond[0].getImm() == RISCV::CCMASK_ANY) {
+    // Unconditional branch
+    BuildMI(MBB, I, DL, get(RISCV::J)).addMBB(TBB);
+    return 1;
+  }
+
+  // Conditional branch.
+  unsigned Count = 0;
+  unsigned CC = Cond[0].getImm();
+  switch(CC) {
+    case RISCV::CCMASK_CMP_EQ:
+      BuildMI(MBB, I, DL, get(RISCV::BEQ)).addMBB(TBB).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_NE:
+      BuildMI(MBB, I, DL, get(RISCV::BNE)).addMBB(TBB).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_LT:
+      BuildMI(MBB, I, DL, get(RISCV::BLT)).addMBB(TBB).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case (RISCV::CCMASK_CMP_LT | RISCV::CCMASK_CMP_UO):
+      BuildMI(MBB, I, DL, get(RISCV::BLTU)).addMBB(TBB).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_GE:
+      BuildMI(MBB, I, DL, get(RISCV::BGE)).addMBB(TBB).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case (RISCV::CCMASK_CMP_GE | RISCV::CCMASK_CMP_UO):
+      BuildMI(MBB, I, DL, get(RISCV::BGEU)).addMBB(TBB).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    //synth
+    case RISCV::CCMASK_CMP_GT:
+      BuildMI(MBB, I, DL, get(RISCV::BGT)).addMBB(TBB).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_LE:
+      BuildMI(MBB, I, DL, get(RISCV::BLE)).addMBB(TBB).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_GT | RISCV::CCMASK_CMP_UO:
+      BuildMI(MBB, I, DL, get(RISCV::BGTU)).addMBB(TBB).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    case RISCV::CCMASK_CMP_LE | RISCV::CCMASK_CMP_UO:
+      BuildMI(MBB, I, DL, get(RISCV::BLEU)).addMBB(TBB).addReg(Cond[2].getReg())
+          .addReg(Cond[3].getReg());
+      break;
+    default:
+      llvm_unreachable("Invalid branch condition code!");
+  }
+  ++Count;
+
+  return Count;
+}
+
+void
+RISCVInstrInfo::copyPhysReg(MachineBasicBlock &MBB,
+			      MachineBasicBlock::iterator MBBI, DebugLoc DL,
+			      unsigned DestReg, unsigned SrcReg,
+			      bool KillSrc) const {
+
+  unsigned Opcode;
+  //when we are copying a phys reg we want the bits for fp
+  if (RISCV::GR32BitRegClass.contains(DestReg, SrcReg))
+    Opcode = STI.isRV64() ? RISCV::ADDIW : RISCV::ADDI;
+  else if (RISCV::GR64BitRegClass.contains(DestReg, SrcReg))
+    Opcode = RISCV::ADDI64;
+  else if (RISCV::FP32BitRegClass.contains(DestReg, SrcReg)){
+    Opcode = RISCV::FSGNJ_S;
+    BuildMI(MBB, MBBI, DL, get(Opcode), DestReg)
+      .addReg(SrcReg, getKillRegState(KillSrc))
+      .addReg(SrcReg, getKillRegState(KillSrc));
+    return;
+  }else if (RISCV::FP64BitRegClass.contains(DestReg, SrcReg)) {
+    Opcode = RISCV::FSGNJ_D;
+    BuildMI(MBB, MBBI, DL, get(Opcode), DestReg)
+      .addReg(SrcReg, getKillRegState(KillSrc))
+      .addReg(SrcReg, getKillRegState(KillSrc));
+    return;
+  }else if(RISCV::FP32BitRegClass.contains(SrcReg) &&
+           RISCV::GR32BitRegClass.contains(DestReg)){
+    Opcode = STI.isRV64() ? RISCV::FMV_X_S64 : RISCV::FMV_X_S;
+    BuildMI(MBB, MBBI, DL, get(Opcode), DestReg)
+      .addReg(SrcReg, getKillRegState(KillSrc));
+    return;
+  }else if(RISCV::FP32BitRegClass.contains(SrcReg) &&
+           RISCV::GR64BitRegClass.contains(DestReg)){
+    Opcode = RISCV::FMV_X_S64;
+    BuildMI(MBB, MBBI, DL, get(Opcode), DestReg)
+      .addReg(SrcReg, getKillRegState(KillSrc));
+    return;
+  }else if(RISCV::FP64BitRegClass.contains(SrcReg) &&
+           RISCV::GR64BitRegClass.contains(DestReg)){
+    Opcode = RISCV::FMV_X_D;
+    BuildMI(MBB, MBBI, DL, get(Opcode), DestReg)
+      .addReg(SrcReg, getKillRegState(KillSrc));
+    return;
+  }else if(RISCV::FP32BitRegClass.contains(DestReg) &&
+           RISCV::GR32BitRegClass.contains(SrcReg)){
+    Opcode = STI.isRV64() ? RISCV::FMV_S_X64 : RISCV::FMV_S_X;
+    BuildMI(MBB, MBBI, DL, get(Opcode), DestReg)
+      .addReg(SrcReg, getKillRegState(KillSrc));
+    return;
+  }else if(RISCV::FP64BitRegClass.contains(DestReg) &&
+           RISCV::GR64BitRegClass.contains(SrcReg)){
+    Opcode = RISCV::FMV_D_X;
+    BuildMI(MBB, MBBI, DL, get(Opcode), DestReg)
+      .addReg(SrcReg, getKillRegState(KillSrc));
+    return;
+  }else if(RISCV::FP64BitRegClass.contains(DestReg) &&
+           RISCV::FP32BitRegClass.contains(SrcReg)){
+    Opcode = RISCV::FCVT_D_S_RDY;
+    BuildMI(MBB, MBBI, DL, get(Opcode), DestReg)
+      .addReg(SrcReg, getKillRegState(KillSrc));
+    return;
+  }else if(RISCV::FP32BitRegClass.contains(DestReg) &&
+           RISCV::FP64BitRegClass.contains(SrcReg)){
+    Opcode = RISCV::FCVT_S_D_RDY;
+    BuildMI(MBB, MBBI, DL, get(Opcode), DestReg)
+      .addReg(SrcReg, getKillRegState(KillSrc));
+    return;
+  }else
+    llvm_unreachable("Impossible reg-to-reg copy");
+
+  BuildMI(MBB, MBBI, DL, get(Opcode), DestReg)
+    .addReg(SrcReg, getKillRegState(KillSrc))
+    .addImm(0);
+}
+
+void
+RISCVInstrInfo::storeRegToStackSlot(MachineBasicBlock &MBB,
+				      MachineBasicBlock::iterator MBBI,
+				      unsigned SrcReg, bool isKill,
+				      int FrameIdx,
+				      const TargetRegisterClass *RC,
+				      const TargetRegisterInfo *TRI) const {
+  DebugLoc DL = MBBI != MBB.end() ? MBBI->getDebugLoc() : DebugLoc();
+
+  // Callers may expect a single instruction, so keep 128-bit moves
+  // together for now and lower them after register allocation.
+  unsigned LoadOpcode, StoreOpcode;
+  getLoadStoreOpcodes(RC, LoadOpcode, StoreOpcode);
+  addFrameReference(BuildMI(MBB, MBBI, DL, get(StoreOpcode))
+		    .addReg(SrcReg, getKillRegState(isKill)), FrameIdx);
+}
+
+void
+RISCVInstrInfo::loadRegFromStackSlot(MachineBasicBlock &MBB,
+				       MachineBasicBlock::iterator MBBI,
+				       unsigned DestReg, int FrameIdx,
+				       const TargetRegisterClass *RC,
+				       const TargetRegisterInfo *TRI) const {
+  DebugLoc DL = MBBI != MBB.end() ? MBBI->getDebugLoc() : DebugLoc();
+
+  // Callers may expect a single instruction, so keep 128-bit moves
+  // together for now and lower them after register allocation.
+  unsigned LoadOpcode, StoreOpcode;
+  getLoadStoreOpcodes(RC, LoadOpcode, StoreOpcode);
+  addFrameReference(BuildMI(MBB, MBBI, DL, get(LoadOpcode), DestReg),
+                    FrameIdx);
+}
+
+bool
+RISCVInstrInfo::expandPostRAPseudo(MachineBasicBlock::iterator MI) const {
+  switch (MI->getOpcode()) {
+
+  default:
+    return false;
+  }
+}
+
+bool RISCVInstrInfo::
+ReverseBranchCondition(SmallVectorImpl<MachineOperand> &Cond) const {
+  assert(Cond.size() <= 4 && "Invalid branch condition!");
+  //Only need to switch the condition code, not the registers
+  switch (Cond[0].getImm()) {
+  case RISCV::CCMASK_CMP_EQ:
+    Cond[0].setImm(RISCV::CCMASK_CMP_NE);
+    return false;
+  case RISCV::CCMASK_CMP_NE:
+    Cond[0].setImm(RISCV::CCMASK_CMP_EQ);
+    return false;
+  case RISCV::CCMASK_CMP_LT:
+    Cond[0].setImm(RISCV::CCMASK_CMP_GE);
+    return false;
+  case RISCV::CCMASK_CMP_GE:
+    Cond[0].setImm(RISCV::CCMASK_CMP_LT);
+    return false;
+  case RISCV::CCMASK_CMP_LT | RISCV::CCMASK_CMP_UO:
+    Cond[0].setImm(RISCV::CCMASK_CMP_GE | RISCV::CCMASK_CMP_UO);
+    return false;
+  case RISCV::CCMASK_CMP_GE | RISCV::CCMASK_CMP_UO:
+    Cond[0].setImm(RISCV::CCMASK_CMP_LT | RISCV::CCMASK_CMP_UO);
+    return false;
+  //synth
+  case RISCV::CCMASK_CMP_GT:
+    Cond[0].setImm(RISCV::CCMASK_CMP_LE);
+    return false;
+  case RISCV::CCMASK_CMP_LE:
+    Cond[0].setImm(RISCV::CCMASK_CMP_GT);
+    return false;
+  case RISCV::CCMASK_CMP_GT | RISCV::CCMASK_CMP_UO:
+    Cond[0].setImm(RISCV::CCMASK_CMP_LE | RISCV::CCMASK_CMP_UO);
+    return false;
+  case RISCV::CCMASK_CMP_LE | RISCV::CCMASK_CMP_UO:
+    Cond[0].setImm(RISCV::CCMASK_CMP_GT | RISCV::CCMASK_CMP_UO);
+    return false;
+  default:
+    llvm_unreachable("Invalid branch condition!");
+  }
+}
+
+bool RISCVInstrInfo::isBranch(const MachineInstr *MI, SmallVectorImpl<MachineOperand> &Cond,
+                                const MachineOperand *&Target) const {
+  switch (MI->getOpcode()) {
+  case RISCV::J:
+  case RISCV::J64:
+  case RISCV::JAL:
+  case RISCV::JAL64:
+  case RISCV::JALR:
+  case RISCV::JALR64:
+    Cond[0].setImm(RISCV::CCMASK_ANY);
+    Target = &MI->getOperand(0);
+    return true;
+  case RISCV::BEQ:
+  case RISCV::BEQ64:
+    Cond[0].setImm(RISCV::CCMASK_CMP_EQ);
+    Target = &MI->getOperand(0);
+    return true;
+  case RISCV::BNE:
+  case RISCV::BNE64:
+    Cond[0].setImm(RISCV::CCMASK_CMP_NE);
+    Target = &MI->getOperand(0);
+    return true;
+  case RISCV::BLT:
+  case RISCV::BLT64:
+    Cond[0].setImm(RISCV::CCMASK_CMP_LT);
+    Target = &MI->getOperand(0);
+    return true;
+  case RISCV::BLTU:
+  case RISCV::BLTU64:
+    Cond[0].setImm(RISCV::CCMASK_CMP_LT | RISCV::CCMASK_CMP_UO);
+    Target = &MI->getOperand(0);
+    return true;
+  case RISCV::BGE:
+  case RISCV::BGE64:
+    Cond[0].setImm(RISCV::CCMASK_CMP_GE);
+    Target = &MI->getOperand(0);
+    return true;
+  case RISCV::BGEU:
+  case RISCV::BGEU64:
+    Cond[0].setImm(RISCV::CCMASK_CMP_GE | RISCV::CCMASK_CMP_UO);
+    Target = &MI->getOperand(0);
+    return true;
+//synth
+  case RISCV::BGT:
+  case RISCV::BGT64:
+    Cond[0].setImm(RISCV::CCMASK_CMP_GT);
+    Target = &MI->getOperand(0);
+    return true;
+  case RISCV::BGTU:
+  case RISCV::BGTU64:
+    Cond[0].setImm(RISCV::CCMASK_CMP_GT | RISCV::CCMASK_CMP_UO);
+    Target = &MI->getOperand(0);
+    return true;
+  case RISCV::BLE:
+  case RISCV::BLE64:
+    Cond[0].setImm(RISCV::CCMASK_CMP_LE);
+    Target = &MI->getOperand(0);
+    return true;
+  case RISCV::BLEU:
+  case RISCV::BLEU64:
+    Cond[0].setImm(RISCV::CCMASK_CMP_LE | RISCV::CCMASK_CMP_UO);
+    Target = &MI->getOperand(0);
+    return true;
+ 
+
+  default:
+    assert(!MI->getDesc().isBranch() && "Unknown branch opcode");
+    return false;
+  }
+}
+
+void RISCVInstrInfo::getLoadStoreOpcodes(const TargetRegisterClass *RC,
+                                           unsigned &LoadOpcode,
+                                           unsigned &StoreOpcode) const {
+  if (RC == &RISCV::GR32BitRegClass ){
+    LoadOpcode = STI.isRV64() ? RISCV::LW64_32 : RISCV::LW;
+    StoreOpcode = STI.isRV64() ? RISCV::SW64_32 : RISCV::SW;
+  } else if (RC == &RISCV::GR64BitRegClass) {
+    LoadOpcode = RISCV::LD;
+    StoreOpcode = RISCV::SD;
+  } else if (RC == &RISCV::FP32BitRegClass) {
+    LoadOpcode = STI.isRV64() ? RISCV::FLW64 : RISCV::FLW;
+    StoreOpcode = STI.isRV64() ? RISCV::FSW64 : RISCV::FSW;
+  } else if (RC == &RISCV::FP64BitRegClass) {
+    LoadOpcode = STI.isRV64() ? RISCV::FLD64 : RISCV::FLD;
+    StoreOpcode = STI.isRV64() ? RISCV::FSD64 : RISCV::FSD;
+  } else
+    llvm_unreachable("Unsupported regclass to load or store");
+}
+
+unsigned RISCVInstrInfo::getOpcodeForOffset(unsigned Opcode,
+                                              int64_t Offset) const {
+  int64_t Offset2 = Offset;
+  if (isInt<12>(Offset) && isInt<12>(Offset2)) {
+    return Opcode;
+  }
+  if (isInt<20>(Offset) && isInt<20>(Offset2)) {
+      return Opcode;
+  }
+  return 0;
+}
+
+void RISCVInstrInfo::loadImmediate(MachineBasicBlock &MBB,
+                                     MachineBasicBlock::iterator MBBI,
+                                     unsigned *Reg, int64_t Value) const {
+  DebugLoc DL = MBBI != MBB.end() ? MBBI->getDebugLoc() : DebugLoc();
+  unsigned Opcode;
+  MachineRegisterInfo &RegInfo = MBB.getParent()->getRegInfo();
+  const TargetRegisterClass *RC = STI.isRV64() ?
+    &RISCV::GR64BitRegClass : &RISCV::GR32BitRegClass;
+  unsigned ZERO = STI.isRV64() ? RISCV::zero_64 : RISCV::zero;
+
+  //create virtual reg to store immediate
+  *Reg = RegInfo.createVirtualRegister(RC);
+  if (isInt<12>(Value)){
+    Opcode = STI.isRV64() ? RISCV::ADDI64 : RISCV::ADDI;
+    BuildMI(MBB, MBBI, DL, get(Opcode), *Reg).addReg(ZERO).addImm(Value);
+  } else {
+  //use LI to let assembler load immediate best
+  BuildMI(MBB, MBBI, DL, get(RISCV::LI), *Reg).addImm(Value);
+  }
+}
diff --git a/lib/Target/RISCV/RISCVInstrInfo.h b/lib/Target/RISCV/RISCVInstrInfo.h
new file mode 100644
index 0000000..3cb0be5
--- /dev/null
+++ b/lib/Target/RISCV/RISCVInstrInfo.h
@@ -0,0 +1,132 @@
+//===-- RISCVInstrInfo.h - RISCV instruction information --------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file contains the RISCV implementation of the TargetInstrInfo class.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVINSTRINFO_H
+#define LLVM_LIB_TARGET_RISCV_RISCVINSTRINFO_H
+
+#include "RISCV.h"
+#include "RISCVRegisterInfo.h"
+#include "llvm/Target/TargetInstrInfo.h"
+
+#define GET_INSTRINFO_HEADER
+#include "RISCVGenInstrInfo.inc"
+
+namespace llvm {
+
+class RISCVTargetMachine;
+
+namespace RISCVII {
+  enum {
+    // See comments in RISCVInstrFormats.td.
+    SimpleLoad  = (1 << 0),
+    SimpleStore = (1 << 1)
+  };
+  // RISCV MachineOperand target flags.
+  enum {
+    // Masks out the bits for the access model.
+    MO_SYMBOL_MODIFIER = (1 << 0),
+
+    // @GOT (aka @GOTENT)
+    MO_GOT = (1 << 0),
+
+    MO_ABS_HI,
+    MO_ABS_LO,
+    MO_TPREL_HI,
+    MO_TPREL_LO
+  };
+}
+
+class RISCVSubtarget;
+class RISCVInstrInfo : public RISCVGenInstrInfo {
+  const RISCVRegisterInfo RI;
+  RISCVSubtarget &STI;
+
+  void splitMove(MachineBasicBlock::iterator MI, unsigned NewOpcode) const;
+  void splitAdjDynAlloc(MachineBasicBlock::iterator MI) const;
+
+public:
+  explicit RISCVInstrInfo(RISCVSubtarget &STI);
+
+  // Override TargetInstrInfo.
+  unsigned isLoadFromStackSlot(const MachineInstr *MI,
+                               int &FrameIndex) const override;
+  unsigned isStoreToStackSlot(const MachineInstr *MI,
+                              int &FrameIndex) const override;
+  void adjustStackPtr(unsigned SP, int64_t Amount,
+                                     MachineBasicBlock &MBB,
+                                     MachineBasicBlock::iterator I) const;
+  unsigned GetInstSizeInBytes(MachineInstr *I) const;
+  bool AnalyzeBranch(MachineBasicBlock &MBB, MachineBasicBlock *&TBB,
+                     MachineBasicBlock *&FBB,
+                     SmallVectorImpl<MachineOperand> &Cond,
+                     bool AllowModify) const override;
+  unsigned RemoveBranch(MachineBasicBlock &MBB) const override;
+  unsigned InsertBranch(MachineBasicBlock &MBB, MachineBasicBlock *TBB,
+                        MachineBasicBlock *FBB,
+                        ArrayRef<MachineOperand> Cond,
+                        DebugLoc DL) const override;
+  unsigned InsertBranchAtInst(MachineBasicBlock &MBB, MachineInstr *I,
+                              MachineBasicBlock *TBB,
+                              ArrayRef<MachineOperand> Cond,
+                              DebugLoc DL) const;
+  unsigned InsertConstBranchAtInst(MachineBasicBlock &MBB, MachineInstr *I,
+                                   int64_t offset,
+                                   ArrayRef<MachineOperand> Cond,
+                                   DebugLoc DL) const;
+  void copyPhysReg(MachineBasicBlock &MBB, MachineBasicBlock::iterator MBBI,
+                   DebugLoc DL, unsigned DestReg, unsigned SrcReg,
+                   bool KillSrc) const override;
+  void storeRegToStackSlot(MachineBasicBlock &MBB,
+                           MachineBasicBlock::iterator MBBI, unsigned SrcReg,
+                           bool isKill, int FrameIndex,
+                           const TargetRegisterClass *RC,
+                           const TargetRegisterInfo *TRI) const override;
+  void loadRegFromStackSlot(MachineBasicBlock &MBB,
+                            MachineBasicBlock::iterator MBBI, unsigned DestReg,
+                            int FrameIdx, const TargetRegisterClass *RC,
+                            const TargetRegisterInfo *TRI) const override;
+  bool expandPostRAPseudo(MachineBasicBlock::iterator MBBI) const override;
+  bool
+  ReverseBranchCondition(SmallVectorImpl<MachineOperand> &Cond) const override;
+
+  // Return the RISCVRegisterInfo, which this class owns.
+  const RISCVRegisterInfo &getRegisterInfo() const { return RI; }
+
+  // Return true if MI is a conditional or unconditional branch.
+  // When returning true, set Cond to the mask of condition-code
+  // values on which the instruction will branch, and set Target
+  // to the operand that contains the branch target.  This target
+  // can be a register or a basic block.
+  bool isBranch(const MachineInstr *MI, SmallVectorImpl<MachineOperand> &Cond,
+                const MachineOperand *&Target) const;
+
+  // Get the load and store opcodes for a given register class.
+  void getLoadStoreOpcodes(const TargetRegisterClass *RC,
+                           unsigned &LoadOpcode, unsigned &StoreOpcode) const;
+
+  // Opcode is the opcode of an instruction that has an address operand,
+  // and the caller wants to perform that instruction's operation on an
+  // address that has displacement Offset.  Return the opcode of a suitable
+  // instruction (which might be Opcode itself) or 0 if no such instruction
+  // exists.
+  unsigned getOpcodeForOffset(unsigned Opcode, int64_t Offset) const;
+
+  // Emit code before MBBI in MI to move immediate value Value into
+  // physical register Reg.
+  void loadImmediate(MachineBasicBlock &MBB,
+                     MachineBasicBlock::iterator MBBI,
+                     unsigned *Reg, int64_t Value) const;
+};
+} // end namespace llvm
+
+#endif
diff --git a/lib/Target/RISCV/RISCVInstrInfo.td b/lib/Target/RISCV/RISCVInstrInfo.td
new file mode 100644
index 0000000..b90fc56
--- /dev/null
+++ b/lib/Target/RISCV/RISCVInstrInfo.td
@@ -0,0 +1,430 @@
+//===-- RISCVInstrInfo.td - General RISCV instructions --------*- tblgen-*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+/*
+ * Predicates for Subtargets
+ */
+
+ def IsRV32 :    Predicate<"Subtarget.isRV32()">,
+                 AssemblerPredicate<"FeatureRV32">; 
+ def IsRV64 :    Predicate<"Subtarget.isRV64()">,
+                 AssemblerPredicate<"FeatureRV64">; 
+ def HasM   :    Predicate<"Subtarget.hasM()">,
+                 AssemblerPredicate<"FeatureM">; 
+ def HasF   :    Predicate<"Subtarget.hasF()">,
+                 AssemblerPredicate<"FeatureF">; 
+ def HasD   :    Predicate<"Subtarget.hasD()">,
+                 AssemblerPredicate<"FeatureD">; 
+ def HasA   :    Predicate<"Subtarget.hasA()">,
+                 AssemblerPredicate<"FeatureA">; 
+
+/*******************
+*RISCV Instructions
+********************/
+//Integer arithmetic register-register
+def ADD : InstR<"add" , 0b0110011, 0b0000000, 0b000, add   , GR32, GR32>, Requires<[IsRV32]>;
+def SUB : InstR<"sub" , 0b0110011, 0b0100000, 0b000, sub   , GR32, GR32>, Requires<[IsRV32]>;
+def SLL : InstR<"sll" , 0b0110011, 0b0000000, 0b001, shl   , GR32, GR32>, Requires<[IsRV32]>;
+def SLT : InstR<"slt" , 0b0110011, 0b0000000, 0b010, setlt , GR32, GR32>;
+def SLTU: InstR<"sltu", 0b0110011, 0b0000000, 0b011, setult, GR32, GR32>;
+def XOR : InstR<"xor" , 0b0110011, 0b0000000, 0b100, xor   , GR32, GR32>;
+def SRL : InstR<"srl" , 0b0110011, 0b0000000, 0b101, srl   , GR32, GR32>, Requires<[IsRV32]>;
+def SRA : InstR<"sra" , 0b0110011, 0b0100000, 0b101, sra   , GR32, GR32>, Requires<[IsRV32]>;
+def OR  : InstR<"or"  , 0b0110011, 0b0000000, 0b110, or    , GR32, GR32>;
+def AND : InstR<"and" , 0b0110011, 0b0000000, 0b111, and   , GR32, GR32>;
+//Integer arithmetic register-immediate
+def ADDI: InstI<"addi", 0b0010011, 0b000       , add, GR32, GR32, imm32sx12>, Requires<[IsRV32]>;
+def XORI: InstI<"xori", 0b0010011, 0b100       , xor, GR32, GR32, imm32sx12>;
+def ORI : InstI<"ori" , 0b0010011, 0b110       , or , GR32, GR32, imm32sx12>;
+def ANDI: InstI<"andi", 0b0010011, 0b111       , and, GR32, GR32, imm32sx12>;
+
+def NOP : InstAlias<"nop", (ADDI zero, zero, 0)>, Requires<[IsRV32]>;
+def MV  : InstAlias<"mv $dst, $src", (ADDI GR32:$dst, GR32:$src, 0)>, Requires<[IsRV32]>;
+def NOT : InstAlias<"not $dst, $src", (XORI GR32:$dst, GR32:$src, -1)>;
+
+//TODO: enforce constraints here or up on level?
+def SLLI: InstI<"slli", 0b0010011, 0b001       , shl, GR32, GR32, imm32sx12>, Requires<[IsRV32]>{
+  let IMM{11-6} = 0b000000; 
+  //trap if $imm{5}!=0 TODO:how to do this?
+}
+def SRLI: InstI<"srli", 0b0010011, 0b101       , srl, GR32, GR32, imm32sx12>, Requires<[IsRV32]>{
+  let IMM{11-6} = 0b000000; 
+  //trap if $src{5}!=0 TODO:how to do this?
+}
+def SRAI: InstI<"srai", 0b0010011, 0b101       , sra, GR32, GR32, imm32sx12>, Requires<[IsRV32]>{
+  let IMM{11-6} = 0b010000;
+  //trap if $src{5}!=0 TODO:how to do this?
+}
+def SLTI : InstI<"slti", 0b0010011, 0b010, setlt, GR32, GR32, imm32sx12>;
+def SLTIU: InstI<"sltiu",0b0010011, 0b011, setult,GR32, GR32, imm32sx12>;
+
+def SEQZ : InstAlias<"seqz $dst, $src", (SLTIU GR32:$dst, GR32:$src, 1)>;
+
+//Synthesized set operators
+//patterns to be used for 32 and 64bit
+multiclass SeteqPats<RegisterOperand RC, Instruction SLTiuOp, Instruction XOROp,
+                     Instruction SLTuOp, Register ZEROReg> {
+  def : Pat<(seteq RC:$lhs, RC:$rhs),
+            (SLTiuOp (XOROp RC:$lhs, RC:$rhs), 1)>;
+  def : Pat<(setne RC:$lhs, RC:$rhs),
+            (SLTuOp ZEROReg, (XOROp RC:$lhs, RC:$rhs))>;
+}
+
+multiclass SetlePats<RegisterOperand RC, Instruction SLTOp, Instruction SLTuOp> {
+  def : Pat<(setle RC:$lhs, RC:$rhs),
+            (XORI (SLTOp RC:$rhs, RC:$lhs), 1)>;
+  def : Pat<(setule RC:$lhs, RC:$rhs),
+            (XORI (SLTuOp RC:$rhs, RC:$lhs), 1)>;
+}
+
+multiclass SetgtPats<RegisterOperand RC, Instruction SLTOp, Instruction SLTuOp> {
+  def : Pat<(setgt RC:$lhs, RC:$rhs),
+            (SLTOp RC:$rhs, RC:$lhs)>;
+  def : Pat<(setugt RC:$lhs, RC:$rhs),
+            (SLTuOp RC:$rhs, RC:$lhs)>;
+}
+
+multiclass SetgePats<RegisterOperand RC, Instruction SLTOp, Instruction SLTuOp> {
+  def : Pat<(setge RC:$lhs, RC:$rhs),
+            (XORI (SLTOp RC:$lhs, RC:$rhs), 1)>;
+  def : Pat<(setuge RC:$lhs, RC:$rhs),
+            (XORI (SLTuOp RC:$lhs, RC:$rhs), 1)>;
+}
+
+defm : SeteqPats<GR32, SLTIU, XOR, SLTU, zero>;
+defm : SetlePats<GR32, SLT, SLTU>;
+defm : SetgtPats<GR32, SLT, SLTU>;
+defm : SetgePats<GR32, SLT, SLTU>;
+
+//Unconditional Jumps
+let isBranch = 1, isTerminator = 1, isBarrier = 1 in {
+  def J  : InstJ<0b1100111, (outs), (ins jumptarget:$target), "j\t$target", 
+          [(br bb:$target)]>, Requires<[IsRV32]>;
+}
+let isCall = 1, Defs = [ra, a0, a1, fa0, fa1, fa0_64, fa1_64] in { //after call return addr and values are defined
+    def JAL: InstJ<0b1101111, (outs GR32:$ret), (ins pcrel32call:$target),
+      "jal\t$ret, $target", 
+          [(set GR32:$ret, (r_jal pcrel32call:$target))]>, Requires<[IsRV32]>;
+}
+
+//call psuedo ops
+let isCall = 1, isCodeGenOnly = 1, usesCustomInserter = 1,
+  Defs = [ra, a0, a1, fa0, fa1] in {
+  def CALL : Pseudo<(outs), (ins pcrel32call:$target),
+                              [(r_call pcrel32call:$target)]>, Requires<[IsRV32]>;
+  def CALLREG : Pseudo<(outs), (ins jalrmem:$target),
+                              [(r_call addr:$target)]>, Requires<[IsRV32]>;
+}
+  //TODO: fix jalr and write test
+  //TODO: JALR can be implemented at brind in llvm since brind is unconditional
+  // JLEIDEL : possible fix in place; requires more testing
+let isCall = 1,  Defs = [ra, a0, a1, fa0, fa1, fa0_64, fa1_64] in { //after call return addr and values are defined
+
+    def JALR: InstRISCV<4, (outs GR32:$ret), (ins jalrmem:$target), 
+          "jalr\t$ret, $target", [(set GR32:$ret, (r_jal addr:$target))]>, Requires<[IsRV32]>{
+            field bits<32> Inst;
+
+            bits<5> RD;
+            bits<5> RS1;
+            bits<12> IMM;
+
+            let Inst{31-20} = IMM{11-0};
+            let Inst{19-15} = RS1;
+            let Inst{14-12} = 0b000;
+            let Inst{11- 7} = RD;
+            let Inst{6 - 0} = 0b1100111;
+          }
+}
+//simple brind pat
+def : Pat<(brind i32:$dst), (JALR 0, $dst)>, Requires<[IsRV32]>;
+ 
+
+//Conditional Branches
+//TODO:refactor to class
+let isBranch = 1, isTerminator = 1, isBarrier = 1 in {
+  def BEQ : InstB<0b1100011, 0b000, (outs), 
+              (ins brtarget:$target, GR32:$src1, GR32:$src2), 
+              "beq\t$src1, $src2, $target", 
+              [(brcond (i32 (seteq GR32:$src1,  GR32:$src2)), bb:$target)]>;
+  def BNE : InstB<0b1100011, 0b001, (outs), 
+              (ins brtarget:$target, GR32:$src1, GR32:$src2), 
+              "bne\t$src1, $src2, $target", 
+              [(brcond (i32 (setne GR32:$src1, GR32:$src2)), bb:$target)]>;
+  def BLT : InstB<0b1100011, 0b100, (outs), 
+              (ins brtarget:$target, GR32:$src1, GR32:$src2), 
+              "blt\t$src1, $src2, $target", 
+              [(brcond (i32 (setlt GR32:$src1, GR32:$src2)), bb:$target)]>;
+  def BGE : InstB<0b1100011, 0b101, (outs), 
+              (ins brtarget:$target, GR32:$src1, GR32:$src2), 
+              "bge\t$src1, $src2, $target", 
+              [(brcond (i32 (setge GR32:$src1, GR32:$src2)), bb:$target)]>;
+  def BLTU: InstB<0b1100011, 0b110, (outs), 
+              (ins brtarget:$target, GR32:$src1, GR32:$src2), 
+              "bltu\t$src1, $src2, $target", 
+              [(brcond (i32 (setult GR32:$src1, GR32:$src2)), bb:$target)]>;
+  def BGEU: InstB<0b1100011, 0b111, (outs), 
+              (ins brtarget:$target, GR32:$src1, GR32:$src2), 
+              "bgeu\t$src1, $src2, $target", 
+              [(brcond (i32 (setuge GR32:$src1, GR32:$src2)), bb:$target)]>;
+
+//Synthesize remaining condition codes by reverseing operands
+  def BGT : InstB<0b1100011, 0b100, (outs), 
+              (ins brtarget:$target, GR32:$src1, GR32:$src2), 
+              "blt\t$src2, $src1, $target", 
+              [(brcond (i32 (setgt GR32:$src1, GR32:$src2)), bb:$target)]>;
+  def BGTU: InstB<0b1100011, 0b110, (outs), 
+              (ins brtarget:$target, GR32:$src1, GR32:$src2), 
+              "bltu\t$src2, $src1, $target", 
+              [(brcond (i32 (setugt GR32:$src1, GR32:$src2)), bb:$target)]>;
+  def BLE : InstB<0b1100011, 0b101, (outs), 
+              (ins brtarget:$target, GR32:$src1, GR32:$src2), 
+              "bge\t$src2, $src1, $target", 
+              [(brcond (i32 (setle GR32:$src1, GR32:$src2)), bb:$target)]>;
+  def BLEU: InstB<0b1100011, 0b111, (outs), 
+              (ins brtarget:$target, GR32:$src1, GR32:$src2), 
+              "bgeu\t$src2, $src1, $target", 
+              [(brcond (i32 (setule GR32:$src1, GR32:$src2)), bb:$target)]>;
+}
+//constant branches (e.g. br 1 $label or br 0 $label)
+def : Pat<(brcond GR32Bit:$cond, bb:$target),
+          (BNE bb:$target, GR32Bit:$cond, zero)>;  
+def : Pat<(i32 0), (i32 zero)>;
+//Conditional moves
+// SELECT_CC_* - Used to implement the SELECT_CC DAG operation.  Expanded after
+// instruction selection into a branch sequence.
+let usesCustomInserter = 1 in {
+  def SELECT_CC : Pseudo<(outs GR32:$dst),
+                              (ins GR32:$cond, GR32:$T, GR32:$F),
+                              [(set GR32:$dst,
+                                 (select GR32:$cond, GR32:$T, GR32:$F))]>;
+}
+def : Pat<(select (i32 (setne GR32:$lhs, 0)), GR32:$T, GR32:$F),
+        (SELECT_CC GR32:$lhs, GR32:$T, GR32:$F)>;
+
+def : Pat<(select (i32 (seteq GR32:$lhs, 0)), GR32:$T, GR32:$F),
+        (SELECT_CC GR32:$lhs, GR32:$F, GR32:$T)>;
+
+//Load/Store Instructions
+let mayLoad = 1 in {
+  def LW : InstLoad <"lw" , 0b0000011, 0b010, load, GR32, mem>, Requires<[IsRV32]>; 
+  def LH : InstLoad <"lh" , 0b0000011, 0b001, sextloadi16, GR32, mem>, Requires<[IsRV32]>; 
+  def LHU: InstLoad <"lhu", 0b0000011, 0b101, zextloadi16, GR32, mem>, Requires<[IsRV32]>; 
+  def LB : InstLoad <"lb" , 0b0000011, 0b000, sextloadi8, GR32, mem>, Requires<[IsRV32]>; 
+  def LBU: InstLoad <"lbu", 0b0000011, 0b100, zextloadi8, GR32, mem>, Requires<[IsRV32]>; 
+}
+//extended loads
+def : Pat<(i32 (extloadi1  addr:$addr)), (LBU addr:$addr)>, Requires<[IsRV32]>;
+def : Pat<(i32 (extloadi8  addr:$addr)), (LBU addr:$addr)>, Requires<[IsRV32]>;
+def : Pat<(i32 (extloadi16 addr:$addr)), (LHU addr:$addr)>, Requires<[IsRV32]>;
+
+let mayStore = 1 in {
+  def SW : InstStore<"sw" , 0b0100011, 0b010, store        , GR32, mem>, Requires<[IsRV32]>;
+  def SH : InstStore<"sh" , 0b0100011, 0b001, truncstorei16, GR32, mem>, Requires<[IsRV32]>; 
+  def SB : InstStore<"sb" , 0b0100011, 0b000, truncstorei8 , GR32, mem>, Requires<[IsRV32]>; 
+}
+
+//Upper Immediate
+def LUI: InstU<0b0110111, (outs GR32:$dst), (ins imm32sxu20:$imm),
+               "lui\t$dst, $imm",
+               [(set GR32:$dst, (shl imm32sx20:$imm, (i32 12)))]>;
+
+def AUIPC: InstU<0b0010111, (outs GR32:$dst), (ins pcimm:$target),
+               "auipc\t$dst, $target",
+               [(set GR32:$dst, (r_pcrel_wrapper tglobaladdr:$target))]>;
+
+//simple immediate loading
+// Transformation Function - get the lower 12 bits.
+def LO12 : SDNodeXForm<imm, [{
+    return getImm(N, N->getZExtValue() & 0xFFF);
+}]>;
+
+// Transformation Function - get the higher 20 bits for large immediate loading
+def HI20 : SDNodeXForm<imm, [{
+    uint32_t value = N->getZExtValue() & 0x00000800 ? 
+                     (N->getZExtValue() >> 12):
+                     (N->getZExtValue() >> 12);
+    return getI32Imm(value & 0x000FFFFF, SDLoc(N));
+}]>;
+
+//psuedo load low imm instruction to print operands better
+def LLI : InstI<"addi", 0b0010011, 0b000       , add, GR32, GR32, imm32sx12>, Requires<[IsRV32]>;
+//def : Pat<(i32 imm32:$imm), (LLI (LUI (HI20 imm32:$imm)), (LO12 imm32:$imm))>;
+def LI : InstRISCV<4, (outs GR32:$dst), (ins imm32:$imm), "li\t$dst, $imm",
+  []>, Requires<[IsRV32]>{
+    let isPseudo = 1;
+}
+
+def LA : InstRISCV<4, (outs GR32:$dst), (ins imm32:$label), "la\t$dst, $label",
+  []>, Requires<[IsRV32]>{
+    let isPseudo = 1;
+}
+
+def : Pat<(i32 imm32sx12:$imm), (LI imm32sx12:$imm)>, Requires<[IsRV32]>; 
+def : Pat<(i32 imm32:$imm), (LI imm32:$imm)>;
+//global addr loading
+def : Pat<(i32 tglobaladdr:$g), (LLI (LUI (HI20 tglobaladdr:$g)), (LO12 tglobaladdr:$g))>, Requires<[IsRV32]>;
+//call
+def : Pat<(r_call (i32 texternalsym:$in)), (CALL texternalsym:$in)>, Requires<[IsRV32]>;
+def : Pat<(r_call (i32 tglobaladdr:$in)), (CALL tglobaladdr:$in)>, Requires<[IsRV32]>;
+//pcrel addr loading using LA
+def : Pat<(r_pcrel_wrapper tglobaladdr:$in), (LA tglobaladdr:$in)>, Requires<[IsRV32]>;
+def : Pat<(r_pcrel_wrapper tblockaddress:$in), (LA tblockaddress:$in)>, Requires<[IsRV32]>;
+def : Pat<(r_pcrel_wrapper tjumptable:$in), (LA tjumptable:$in)>, Requires<[IsRV32]>;
+def : Pat<(r_pcrel_wrapper tconstpool:$in), (LA tconstpool:$in)>, Requires<[IsRV32]>;
+def : Pat<(r_pcrel_wrapper tglobaltlsaddr:$in), (LA tglobaltlsaddr:$in)>, Requires<[IsRV32]>;
+def : Pat<(r_pcrel_wrapper texternalsym:$in), (LA texternalsym:$in)>, Requires<[IsRV32]>;
+//global addr loading
+def : Pat<(RISCVHi tglobaladdr:$in), (LUI tglobaladdr:$in)>, Requires<[IsRV32]>;
+def : Pat<(RISCVHi tblockaddress:$in), (LUI tblockaddress:$in)>, Requires<[IsRV32]>;
+def : Pat<(RISCVHi tjumptable:$in), (LUI tjumptable:$in)>, Requires<[IsRV32]>;
+def : Pat<(RISCVHi tconstpool:$in), (LUI tconstpool:$in)>, Requires<[IsRV32]>;
+def : Pat<(RISCVHi tglobaltlsaddr:$in), (LUI tglobaltlsaddr:$in)>, Requires<[IsRV32]>;
+def : Pat<(RISCVHi texternalsym:$in), (LUI texternalsym:$in)>, Requires<[IsRV32]>;
+
+def : Pat<(RISCVLo tglobaladdr:$in), (ADDI zero, tglobaladdr:$in)>, Requires<[IsRV32]>;
+def : Pat<(RISCVLo tblockaddress:$in), (ADDI zero, tblockaddress:$in)>, Requires<[IsRV32]>;
+def : Pat<(RISCVLo tjumptable:$in), (ADDI zero, tjumptable:$in)>, Requires<[IsRV32]>;
+def : Pat<(RISCVLo tconstpool:$in), (ADDI zero, tconstpool:$in)>, Requires<[IsRV32]>;
+def : Pat<(RISCVLo tglobaltlsaddr:$in),
+          (ADDI zero, tglobaltlsaddr:$in)>, Requires<[IsRV32]>;
+def : Pat<(RISCVLo texternalsym:$in), (ADDI zero, texternalsym:$in)>, Requires<[IsRV32]>;
+
+def : Pat<(add GR32:$hi, (RISCVLo tglobaladdr:$lo)),
+          (ADDI GR32:$hi, tglobaladdr:$lo)>, Requires<[IsRV32]>;
+def : Pat<(add GR32:$hi, (RISCVLo tblockaddress:$lo)),
+          (ADDI GR32:$hi, tblockaddress:$lo)>, Requires<[IsRV32]>;
+def : Pat<(add GR32:$hi, (RISCVLo tjumptable:$lo)),
+          (ADDI GR32:$hi, tjumptable:$lo)>, Requires<[IsRV32]>;
+def : Pat<(add GR32:$hi, (RISCVLo tconstpool:$lo)),
+          (ADDI GR32:$hi, tconstpool:$lo)>, Requires<[IsRV32]>;
+def : Pat<(add GR32:$hi, (RISCVLo tglobaltlsaddr:$lo)),
+          (ADDI GR32:$hi, tglobaltlsaddr:$lo)>, Requires<[IsRV32]>;
+
+//===----------------------------------------------------------------------===//
+// Stack allocation
+//===----------------------------------------------------------------------===//
+
+def ADJCALLSTACKDOWN : Pseudo<(outs), (ins i64imm:$amt),
+                              [(callseq_start timm:$amt)]>;
+def ADJCALLSTACKUP   : Pseudo<(outs), (ins i64imm:$amt1, i64imm:$amt2),
+                              [(callseq_end timm:$amt1, timm:$amt2)]>;
+
+//hardcoded JALR to be return
+let isReturn = 1, isTerminator = 1, isBarrier = 1, hasCtrlDep = 1,
+    isCodeGenOnly = 1, Defs = [a0, a1] in {
+  def RET : InstRISCV<4, (outs), (ins), "ret", 
+          []>{
+            field bits<32> Inst;
+            
+            let Inst{31-27} = 0;// destination zero
+            let Inst{26-22} = 1;// target ra
+            let Inst{21-17} = 0;// imm 0
+            let Inst{16-10} = 0;// imm 0
+            let Inst{9 - 7} = 0b000;
+            let Inst{6 - 0} = 0b1101011;
+          }
+   }
+  def : Pat<(r_retflag), (RET)>;
+
+//Fence
+def FENCE: InstRISCV<4, (outs), (ins fenceImm:$pred, fenceImm:$succ), "fence", 
+      [(r_fence fenceImm:$pred, fenceImm:$succ)]>{
+        field bits<32> Inst;
+
+        bits<4> pred;
+        bits<4> succ;
+
+        let Inst{31-28} = 0b0000;
+        let Inst{27   } = pred{3};//PI;
+        let Inst{26   } = pred{2};//PO;
+        let Inst{25   } = pred{1};//PR;
+        let Inst{24   } = pred{0};//PW;
+        let Inst{23   } = succ{3};//SI;
+        let Inst{22   } = succ{2};//SO;
+        let Inst{21   } = succ{1};//SR;
+        let Inst{20   } = succ{0};//SW;
+        let Inst{19-15} = 0b00000;
+        let Inst{14-12} = 0b000;
+        let Inst{11- 7} = 0b00000;
+        let Inst{6 - 0} = 0b0001111;
+      }
+
+//Fence.I
+def FENCE_I: InstRISCV<4, (outs), (ins fenceImm:$pred, fenceImm:$succ), "fence.i", 
+      [(r_fence fenceImm:$pred, fenceImm:$succ)]>{
+        field bits<32> Inst;
+
+        bits<4> pred;
+        bits<4> succ;
+
+        let Inst{31-28} = 0b0000;
+        let Inst{27   } = 0b0;
+        let Inst{26   } = 0b0;
+        let Inst{25   } = 0b0;
+        let Inst{24   } = 0b0;
+        let Inst{23   } = 0b0;
+        let Inst{22   } = 0b0;
+        let Inst{21   } = 0b0;
+        let Inst{20   } = 0b0;
+        let Inst{19-15} = 0b00000;
+        let Inst{14-12} = 0b001;
+        let Inst{11- 7} = 0b00000;
+        let Inst{6 - 0} = 0b0001111;
+      }
+
+
+//===----------------------------------------------------------------------===//
+// System Calls
+//===----------------------------------------------------------------------===//
+
+//scall
+def SCALL: InstRISCV<4, (outs), (ins), "scall", []>{
+        field bits<32> Inst;
+
+        let Inst{31-20} = 0b000000000000;
+        let Inst{19-15} = 0b00000;
+        let Inst{14-12} = 0b000;
+        let Inst{11- 7} = 0b00000;
+        let Inst{6 - 0} = 0b1110011;
+      }
+
+//sbreak
+def SBREAK: InstRISCV<4, (outs), (ins), "sbreak", []>{
+        field bits<32> Inst;
+
+        let Inst{31-20} = 0b000000000001;
+        let Inst{19-15} = 0b00000;
+        let Inst{14-12} = 0b000;
+        let Inst{11- 7} = 0b00000;
+        let Inst{6 - 0} = 0b1110011;
+      }
+
+//rdcycle Rd
+def RDCYCLE: InstISYS<"rdcycle", 0b110000000000, GR32>;
+//rdcycleh Rd
+def RDCYCLEH: InstISYS<"rdcycleh", 0b110010000000, GR32>;
+//rdtime Rd
+def RDTIME: InstISYS<"rdtime", 0b110000000001, GR32>;
+//rdtimeh Rd
+def RDTIMEH: InstISYS<"rdtimeh", 0b110010000001, GR32>;
+//rdinstret Rd
+def RDINSTRET: InstISYS<"rdinstret", 0b110000000010, GR32>;
+//rdinstreth Rd
+def RDINSTRETH: InstISYS<"rdinstreth", 0b110010000010, GR32>;
+
+//===----------------------------------------------------------------------===//
+// Subtarget features
+//===----------------------------------------------------------------------===//
+
+include "RISCVInstrInfoRV64.td"
+include "RISCVInstrInfoM.td"
+include "RISCVInstrInfoF.td"
+include "RISCVInstrInfoA.td"
+include "RISCVInstrInfoD.td"
+
diff --git a/lib/Target/RISCV/RISCVInstrInfoA.td b/lib/Target/RISCV/RISCVInstrInfoA.td
new file mode 100644
index 0000000..8bc150d
--- /dev/null
+++ b/lib/Target/RISCV/RISCVInstrInfoA.td
@@ -0,0 +1,52 @@
+//===- RISCVInstrInfoA.td - Floating-point RISCV instructions -*- tblgen-*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+//RV32
+//TODO: add LR/SC and acq/rel
+
+def AMOSWAP_W : InstA<"amoswap.w" , 0b0101111, 0b00000, 0b010, atomic_swap     , GR32, memreg>, Requires<[IsRV32, HasA]>;
+def AMOADD_W  : InstA<"amoadd.w"  , 0b0101111, 0b00001, 0b010, atomic_load_add , GR32, memreg>, Requires<[IsRV32, HasA]>;
+def AMOXOR_W  : InstA<"amoxor.w"  , 0b0101111, 0b00100, 0b010, atomic_load_xor , GR32, memreg>, Requires<[IsRV32, HasA]>;
+def AMOAND_W  : InstA<"amoand.w"  , 0b0101111, 0b01100, 0b010, atomic_load_and , GR32, memreg>, Requires<[IsRV32, HasA]>;
+def AMOOR_W   : InstA<"amoor.w"   , 0b0101111, 0b01000, 0b010, atomic_load_or  , GR32, memreg>, Requires<[IsRV32, HasA]>;
+def AMOMIN_W  : InstA<"amomin.w"  , 0b0101111, 0b10000, 0b010, atomic_load_min , GR32, memreg>, Requires<[IsRV32, HasA]>;
+def AMOMAX_W  : InstA<"amomax.w"  , 0b0101111, 0b10100, 0b010, atomic_load_max , GR32, memreg>, Requires<[IsRV32, HasA]>;
+def AMOMINU_W : InstA<"amominu.w" , 0b0101111, 0b11000, 0b010, atomic_load_umin, GR32, memreg>, Requires<[IsRV32, HasA]>;
+def AMOMAXU_W : InstA<"amomaxu.w" , 0b0101111, 0b11100, 0b010, atomic_load_umax, GR32, memreg>, Requires<[IsRV32, HasA]>;
+
+def LR_W : InstLR<"lr.w", 0b010, GR32, memreg>, Requires<[HasA]>;
+def SC_W : InstSC<"sc.w", 0b010, GR32, memreg>, Requires<[HasA]>;
+
+//RV64A
+//TODO: add LR/SC and acq/rel
+
+//TODO add gr32 operations
+def AMOSWAP_D   : InstA<"amoswap.d" , 0b0101111, 0b00000, 0b011, atomic_swap     , GR64, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOADD_D    : InstA<"amoadd.D"  , 0b0101111, 0b00001, 0b011, atomic_load_add , GR64, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOXOR_D    : InstA<"amoxor.d"  , 0b0101111, 0b00100, 0b011, atomic_load_xor , GR64, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOAND_D    : InstA<"amoand.d"  , 0b0101111, 0b01100, 0b011, atomic_load_and , GR64, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOOR_D     : InstA<"amoor.d"   , 0b0101111, 0b01000, 0b011, atomic_load_or  , GR64, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOMIN_D    : InstA<"amomin.d"  , 0b0101111, 0b10000, 0b011, atomic_load_min , GR64, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOMAX_D    : InstA<"amomax.d"  , 0b0101111, 0b10100, 0b011, atomic_load_max , GR64, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOMINU_D   : InstA<"amominu.d" , 0b0101111, 0b11000, 0b011, atomic_load_umin, GR64, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOMAXU_D   : InstA<"amomaxu.d" , 0b0101111, 0b11100, 0b011, atomic_load_umax, GR64, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOSWAP_W64 : InstA<"amoswap.w" , 0b0101111, 0b00000, 0b010, atomic_swap     , GR32, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOADD_W64  : InstA<"amoadd.w"  , 0b0101111, 0b00001, 0b010, atomic_load_add , GR32, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOXOR_W64  : InstA<"amoxor.w"  , 0b0101111, 0b00100, 0b010, atomic_load_xor , GR32, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOAND_W64  : InstA<"amoand.w"  , 0b0101111, 0b01100, 0b010, atomic_load_and , GR32, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOOR_W64   : InstA<"amoor.w"   , 0b0101111, 0b01000, 0b010, atomic_load_or  , GR32, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOMIN_W64  : InstA<"amomin.w"  , 0b0101111, 0b10000, 0b010, atomic_load_min , GR32, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOMAX_W64  : InstA<"amomax.w"  , 0b0101111, 0b10100, 0b010, atomic_load_max , GR32, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOMINU_W64 : InstA<"amominu.w" , 0b0101111, 0b11000, 0b010, atomic_load_umin, GR32, memreg64>, Requires<[IsRV64, HasA]>;
+def AMOMAXU_W64 : InstA<"amomaxu.w" , 0b0101111, 0b11100, 0b010, atomic_load_umax, GR32, memreg64>, Requires<[IsRV64, HasA]>;
+
+def LR_W64 : InstLR<"lr.w", 0b010, GR32, memreg64>, Requires<[IsRV64, HasA]>;
+def SC_W64 : InstSC<"sc.w", 0b010, GR32, memreg64>, Requires<[IsRV64, HasA]>;
+def LR_D   : InstLR<"lr.d", 0b011, GR64, memreg64>, Requires<[IsRV64, HasA]>;
+def SC_D   : InstSC<"sc.d", 0b011, GR64, memreg64>, Requires<[IsRV64, HasA]>;
diff --git a/lib/Target/RISCV/RISCVInstrInfoD.td b/lib/Target/RISCV/RISCVInstrInfoD.td
new file mode 100644
index 0000000..24f8347
--- /dev/null
+++ b/lib/Target/RISCV/RISCVInstrInfoD.td
@@ -0,0 +1,120 @@
+//===- RISCVInstrFP.td - Floating-point RISCV instructions ----*- tblgen-*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+let mayLoad = 1 in {
+  def FLD : InstLoad <"fld" , 0b0000111, 0b011, loadf64,  FP64, mem>, Requires<[HasD,IsRV32]>; 
+  def FLD64 : InstLoad <"fld" , 0b0000111, 0b011, loadf64,  FP64, mem64>, Requires<[HasD,IsRV64]>; 
+}
+
+let mayStore = 1 in {
+  def FSD : InstStore <"fsd" , 0b0100111, 0b011, store, FP64, mem>, Requires<[HasD,IsRV32]>; 
+  def FSD64 : InstStore <"fsd" , 0b0100111, 0b011, store, FP64, mem64>, Requires<[HasD,IsRV64]>; 
+}
+
+multiclass  FPBinOps64<string name, SDPatternOperator op1, bits<5> funct5, bits<2> fmt> {
+  def _RDY : InstR<name, 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {1,1,1}, op1, FP64, FP64>;
+  let isAsmParserOnly = 1 in { //only use the dynamic version during instruction selection
+    def _RNE : InstR<name#".rne", 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {0,0,0}, op1, FP64, FP64>;
+    def _RTZ : InstR<name#".rtz", 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {0,0,1}, op1, FP64, FP64>;
+    def _RDN : InstR<name#".rdn", 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {0,1,0}, op1, FP64, FP64>;
+    def _RUP : InstR<name#".rup", 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {0,1,1}, op1, FP64, FP64>;
+    def _RMM : InstR<name#".rmm", 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {1,0,0}, op1, FP64, FP64>;
+  }
+}
+//Single precision arithmetic
+defm FADD_D : FPBinOps64<"fadd.d", fadd, 0b00000, 0b01>, Requires<[HasD]>;
+defm FSUB_D : FPBinOps64<"fsub.d", fsub, 0b00001, 0b01>, Requires<[HasD]>;
+defm FMUL_D : FPBinOps64<"fmul.d", fmul, 0b00010, 0b01>, Requires<[HasD]>;
+defm FDIV_D : FPBinOps64<"fdiv.d", fdiv, 0b00011, 0b01>, Requires<[HasD]>;
+//let RS2 = 0b00000 in {
+  //defm FSQRT_S : FPOps<"fsqrt.s", fsqrt, 0b00100, 0b00>, Requires<[HasD]>;}
+
+//TODO: implement min/max
+//defm FMIN_S : FPOps<"fmin.s", fmin, 0b11000, 0b00>, Requires<[HasD]>;
+//defm FMAX_S : FPOps<"fmax.s", fmax, 0b11001, 0b00>, Requires<[HasD]>;
+
+//TODO: implement fma
+
+//Move and Conversions
+//The float to int conversions do nothing because fp_to_uint means RTZ specifically
+defm FCVT_W_D  : FPConvOps<"fcvt.w.d",  null_frag, GR32, FP64, 0b01010, 0b01>, Requires<[HasD]>;
+defm FCVT_WU_D : FPConvOps<"fcvt.wu.d", null_frag, GR32, FP64, 0b01011, 0b01>, Requires<[HasD]>;
+defm FCVT_D_W  : FPConvOps<"fcvt.d.w",  sint_to_fp, FP64, GR32, 0b01110, 0b01>, Requires<[HasD]>;
+defm FCVT_D_WU : FPConvOps<"fcvt.d.wu", uint_to_fp, FP64, GR32, 0b01111, 0b01>, Requires<[HasD]>;
+//make sure we get the right rounding mode
+def :Pat<(i32 (fp_to_uint FP64:$src)), (FCVT_WU_D_RTZ FP64:$src)>;
+def :Pat<(i32 (fp_to_sint FP64:$src)), (FCVT_W_D_RTZ FP64:$src)>;
+//RV64F
+defm FCVT_L_D  : FPConvOps<"fcvt.l.d",  null_frag, GR64, FP64, 0b01000, 0b01>, Requires<[HasD,IsRV64]>;
+defm FCVT_LU_D : FPConvOps<"fcvt.lu.d", null_frag, GR64, FP64, 0b01001, 0b01>, Requires<[HasD,IsRV64]>;
+defm FCVT_D_L  : FPConvOps<"fcvt.d.l",  sint_to_fp, FP64, GR64, 0b01100, 0b01>, Requires<[HasD,IsRV64]>;
+defm FCVT_D_LU : FPConvOps<"fcvt.d.lu", uint_to_fp, FP64, GR64, 0b01101, 0b01>, Requires<[HasD,IsRV64]>;
+//make sure we get the right rounding mode
+def :Pat<(i64 (fp_to_uint FP64:$src)), (FCVT_LU_D_RTZ FP64:$src)>;
+def :Pat<(i64 (fp_to_sint FP64:$src)), (FCVT_L_D_RTZ FP64:$src)>;
+//Single <-> Double
+defm FCVT_S_D  : FPConvOps<"fcvt.s.d",  fround , FP32, FP64, 0b10001, 0b00>, Requires<[HasD]>;
+defm FCVT_D_S  : FPConvOps<"fcvt.d.s",  fextend, FP64, FP32, 0b10000, 0b01>, Requires<[HasD]>;
+
+//Sign injection
+def FSGNJ_D : InstSign<"fsgnj.d", 0b1010011, 0b00101, 0b01, 0b000,
+                        fcopysign, FP64, FP64>, Requires<[HasD]>;
+def FSGNJN_D : InstSign<"fsgnjn.d", 0b1010011, 0b00110, 0b01, 0b000,
+                        fcopysign, FP64, FP64>, Requires<[HasD]> {
+                          let Pattern =
+                          [(set FP64:$dst, (fcopysign FP64:$src1, (fneg FP64:$src2)))];
+                        }
+//pattern is
+//if signs are equal copysign from abs(src2)
+//otherwise copysign from fabs( fneg (src2))
+def FSGNJX_D : InstSign<"fsgnjx.d", 0b1010011, 0b00111, 0b01, 0b000,
+    fcopysign, FP64, FP64>, Requires<[HasD]> {
+      let Pattern =
+      [(set FP64:$dst, (select 
+      (i32 (seteq (i32 (fgetsign FP64:$src1)), (i32 (fgetsign FP64:$src2)))),
+        (fcopysign FP64:$src1, (fabs FP64:$src2)),
+        (fcopysign FP64:$src1, (fneg (fabs FP64:$src2)))
+      ))];
+      }
+
+//llvm cant select fneq itsels so help it out
+def : Pat<(fneg FP64:$src), (FSGNJN_D FP64:$src, FP64:$src)>, Requires<[HasD]>;
+def : Pat<(fabs FP64:$src), (FSGNJX_D FP64:$src, FP64:$src)>, Requires<[HasD]>;
+//llvm also can't select f32 = fcopysign f32,f64
+def : Pat<(fcopysign FP32:$src1, FP64:$src2), (FSGNJ_S FP32:$src1, (FCVT_S_D_RDY FP64:$src2))>;
+
+//Move instruction (bitcasts)
+def FMV_X_D : InstConv<"fmv.x.d", "", 0b1010011, 0b11100, 0b01, 0b000, bitconvert, GR64, FP64>, Requires<[HasD, IsRV64]>;
+def FMV_D_X : InstConv<"fmv.d.x", "", 0b1010011, 0b11110, 0b01, 0b000, bitconvert, FP64, GR64>, Requires<[HasD, IsRV64]>;
+
+//Floating point comparisons
+def FEQ_D : InstSign<"feq.d", 0b1010011, 0b10101, 0b01, 0b000, setoeq, GR32, FP64>, Requires<[HasD]>;
+def FLT_D : InstSign<"flt.d", 0b1010011, 0b10110, 0b01, 0b000, setolt, GR32, FP64>, Requires<[HasD]>;
+def FLE_D : InstSign<"fle.d", 0b1010011, 0b10111, 0b01, 0b000, setole, GR32, FP64>, Requires<[HasD]>;
+def FUEQ_D : InstSign<"feq.d", 0b1010011, 0b10101, 0b01, 0b000, setueq, GR32, FP64>, Requires<[HasD]>;
+def FULT_D : InstSign<"flt.d", 0b1010011, 0b10110, 0b01, 0b000, setult, GR32, FP64>, Requires<[HasD]>;
+def FULE_D : InstSign<"fle.d", 0b1010011, 0b10111, 0b01, 0b000, setule, GR32, FP64>, Requires<[HasD]>;
+//synthesized set operators
+
+defm : FPCmpPats<FP64, FEQ_D, FUEQ_D, FLT_D, FULT_D, FLE_D, FULE_D>;
+
+//extloads
+def : Pat<(extloadf32 addr:$addr), (FCVT_D_S_RDY (FLW addr:$addr))>, Requires<[HasD,IsRV32]>;
+def : Pat<(extloadf32 addr:$addr), (FCVT_D_S_RDY (FLW64 addr:$addr))>, Requires<[HasD,IsRV64]>;
+
+//fp imm
+def :Pat<(fpimm0), (FCVT_D_W_RDY zero)>;
+
+//fp select
+let usesCustomInserter = 1 in {
+  def FSELECT_CC_D : Pseudo<(outs FP64:$dst),
+                              (ins GR32:$cond, FP64:$T, FP64:$F),
+                              [(set FP64:$dst,
+                                 (select GR32:$cond, FP64:$T, FP64:$F))]>, Requires<[HasD]>;
+}
diff --git a/lib/Target/RISCV/RISCVInstrInfoF.td b/lib/Target/RISCV/RISCVInstrInfoF.td
new file mode 100644
index 0000000..0683a45
--- /dev/null
+++ b/lib/Target/RISCV/RISCVInstrInfoF.td
@@ -0,0 +1,177 @@
+//===- RISCVInstrFP.td - Floating-point RISCV instructions ----*- tblgen-*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+let mayLoad = 1 in {
+  def FLW : InstLoad <"flw" , 0b0000111, 0b010, loadf32,  FP32, mem>, Requires<[HasF,IsRV32]>; 
+  def FLW64 : InstLoad <"flw" , 0b0000111, 0b010, loadf32,  FP32, mem64>, Requires<[HasF,IsRV64]>; 
+}
+
+let mayStore = 1 in {
+  def FSW : InstStore <"fsw" , 0b0100111, 0b010, store, FP32, mem>, Requires<[HasF,IsRV32]>; 
+  def FSW64 : InstStore <"fsw" , 0b0100111, 0b010, store, FP32, mem64>, Requires<[HasF,IsRV64]>; 
+}
+
+multiclass  FPBinOps<string name, SDPatternOperator op1, bits<5> funct5, bits<2> fmt> {
+  def _RDY : InstR<name, 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {1,1,1}, op1, FP32, FP32>;
+  let isAsmParserOnly = 1 in { //only use the dynamic version during instruction selection
+    def _RNE : InstR<name#".rne", 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {0,0,0}, op1, FP32, FP32>;
+    def _RTZ : InstR<name#".rtz", 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {0,0,1}, op1, FP32, FP32>;
+    def _RDN : InstR<name#".rdn", 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {0,1,0}, op1, FP32, FP32>;
+    def _RUP : InstR<name#".rup", 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {0,1,1}, op1, FP32, FP32>;
+    def _RMM : InstR<name#".rmm", 0b1010011, {funct5{4},funct5{3},funct5{2},funct5{1},funct5{0},fmt{1},fmt{0}}, {1,0,0}, op1, FP32, FP32>;
+  }
+}
+//Single precision arithmetic
+defm FADD_S : FPBinOps<"fadd.s", fadd, 0b00000, 0b00>, Requires<[HasF]>;
+defm FSUB_S : FPBinOps<"fsub.s", fsub, 0b00001, 0b00>, Requires<[HasF]>;
+defm FMUL_S : FPBinOps<"fmul.s", fmul, 0b00010, 0b00>, Requires<[HasF]>;
+defm FDIV_S : FPBinOps<"fdiv.s", fdiv, 0b00011, 0b00>, Requires<[HasF]>;
+//let RS2 = 0b00000 in {
+  //defm FSQRT_S : FPOps<"fsqrt.s", fsqrt, 0b00100, 0b00>, Requires<[HasF]>;}
+
+//TODO: implement min/max
+//defm FMIN_S : FPOps<"fmin.s", fmin, 0b11000, 0b00>, Requires<[HasF]>;
+//defm FMAX_S : FPOps<"fmax.s", fmax, 0b11001, 0b00>, Requires<[HasF]>;
+
+//TODO: implement fma
+
+//Move and Conversions
+class InstConv<string mnemonic, string rmstr, bits<7> op, bits<5> funct5, bits<2> fmt, bits<3> rm,
+               SDPatternOperator operator, RegisterOperand cls1, 
+               RegisterOperand cls2>
+  : InstRISCV<4, (outs cls1:$dst), (ins cls2:$src1), 
+                mnemonic#"\t$dst, $src1"#rmstr, 
+                [(set cls1:$dst, (operator cls2:$src1))]> {
+  field bits<32> Inst;
+
+  bits<5> RD;
+  bits<5> RS1;
+
+  let Inst{31-27} = funct5;
+  let Inst{26-25} = fmt;
+  let Inst{24-20} = 0b00000;
+  let Inst{19-15} = RS1;
+  let Inst{14-12} = rm;
+  let Inst{11- 7} = RD;
+  let Inst{6 - 0} = op;
+}
+
+multiclass  FPConvOps<string name, SDPatternOperator op1, RegisterOperand outCls, RegisterOperand inCls, bits<5> funct5, bits<2> fmt> {
+  def _RDY : InstConv<name,"", 0b1010011, funct5, fmt, 0b111, op1, outCls, inCls>;
+  let isAsmParserOnly = 1 in { //only use the dynamic version during instruction selection
+    def _RNE : InstConv<name,",rne", 0b1010011, funct5, fmt, 0b000, op1, outCls, inCls>;
+    def _RTZ : InstConv<name,",rtz", 0b1010011, funct5, fmt, 0b001, op1, outCls, inCls>;
+    def _RDN : InstConv<name,",rdn", 0b1010011, funct5, fmt, 0b010, op1, outCls, inCls>;
+    def _RUP : InstConv<name,",rup", 0b1010011, funct5, fmt, 0b011, op1, outCls, inCls>;
+    def _RMM : InstConv<name,",rmm", 0b1010011, funct5, fmt, 0b100, op1, outCls, inCls>;
+  }
+}
+
+defm FCVT_W_S  : FPConvOps<"fcvt.w.s",  null_frag, GR32, FP32, 0b01010, 0b00>, Requires<[HasF]>;
+defm FCVT_WU_S : FPConvOps<"fcvt.wu.s", null_frag, GR32, FP32, 0b01011, 0b00>, Requires<[HasF]>;
+defm FCVT_S_W  : FPConvOps<"fcvt.s.w",  sint_to_fp, FP32, GR32, 0b01110, 0b00>, Requires<[HasF]>;
+defm FCVT_S_WU : FPConvOps<"fcvt.s.wu", uint_to_fp, FP32, GR32, 0b01111, 0b00>, Requires<[HasF]>;
+//make sure we get the right rounding mode
+def :Pat<(i32 (fp_to_uint FP32:$src)), (FCVT_WU_S_RTZ FP32:$src)>;
+def :Pat<(i32 (fp_to_sint FP32:$src)), (FCVT_W_S_RTZ FP32:$src)>;
+
+//RV64F
+defm FCVT_L_S  : FPConvOps<"fcvt.l.s",  null_frag, GR64, FP32, 0b01000, 0b00>, Requires<[HasF,IsRV64]>;
+defm FCVT_LU_S : FPConvOps<"fcvt.lu.s", null_frag, GR64, FP32, 0b01001, 0b00>, Requires<[HasF,IsRV64]>;
+defm FCVT_S_L  : FPConvOps<"fcvt.s.l",  sint_to_fp, FP32, GR64, 0b01100, 0b00>, Requires<[HasF,IsRV64]>;
+defm FCVT_S_LU : FPConvOps<"fcvt.s.lu", uint_to_fp, FP32, GR64, 0b01101, 0b00>, Requires<[HasF,IsRV64]>;
+//make sure we get the right rounding mode
+def :Pat<(i64 (fp_to_uint FP32:$src)), (FCVT_LU_S_RTZ FP32:$src)>;
+def :Pat<(i64 (fp_to_sint FP32:$src)), (FCVT_L_S_RTZ FP32:$src)>;
+
+//Sign injection
+class InstSign<string mnemonic, bits<7> op, bits<5> funct5, bits<2> fmt, bits<3> rm,
+               SDPatternOperator operator, RegisterOperand cls1, 
+               RegisterOperand cls2>
+  : InstRISCV<4, (outs cls1:$dst), (ins cls2:$src2, cls2:$src1), 
+                mnemonic#"\t$dst, $src1, $src2", 
+                [(set cls1:$dst, (operator cls2:$src1, cls2:$src2))]> {
+  field bits<32> Inst;
+
+  bits<5> RD;
+  bits<5> RS1;
+  bits<5> RS2;
+
+  let Inst{31-27} = funct5;
+  let Inst{26-25} = fmt;
+  let Inst{24-20} = RS2;
+  let Inst{19-15} = RS1;
+  let Inst{14-12} = rm;
+  let Inst{11- 7} = RD;
+  let Inst{6 - 0} = op;
+}
+def FSGNJ_S : InstSign<"fsgnj.s", 0b1010011, 0b00101, 0b00, 0b000,
+                        fcopysign, FP32, FP32>, Requires<[HasF]>;
+def FSGNJN_S : InstSign<"fsgnjn.s", 0b1010011, 0b00110, 0b00, 0b000,
+                        fcopysign, FP32, FP32>, Requires<[HasF]> {
+                          let Pattern =
+                          [(set FP32:$dst, (fcopysign FP32:$src1, (fneg FP32:$src2)))];
+                        }
+//pattern is
+//if signs are equal copysign from abs(src2)
+//otherwise copysign from fabs( fneg (src2))
+def FSGNJX_S : InstSign<"fsgnjx.s", 0b1010011, 0b00111, 0b00, 0b000,
+    fcopysign, FP32, FP32>, Requires<[HasF]> {
+      let Pattern =
+      [(set FP32:$dst, (select 
+      (i32 (seteq (i32 (fgetsign FP32:$src1)), (i32 (fgetsign FP32:$src2)))),
+        (fcopysign FP32:$src1, (fabs FP32:$src2)),
+        (fcopysign FP32:$src1, (fneg (fabs FP32:$src2)))
+      ))];
+      }
+
+//llvm cant select fneg itsels so help it out
+def : Pat<(fneg FP32:$src), (FSGNJN_S FP32:$src, FP32:$src)>, Requires<[HasF]>;
+def : Pat<(fabs FP32:$src), (FSGNJX_S FP32:$src, FP32:$src)>, Requires<[HasF]>;
+
+//Move instruction (bitcasts)
+def FMV_X_S : InstConv<"fmv.x.s", "", 0b1010011, 0b11100, 0b00, 0b000, bitconvert, GR32, FP32>, Requires<[HasF]>;
+def FMV_S_X : InstConv<"fmv.s.x", "", 0b1010011, 0b11110, 0b00, 0b000, bitconvert, FP32, GR32>, Requires<[HasF]>;
+def FMV_X_S64 : InstConv<"fmv.x.s", "", 0b1010011, 0b11100, 0b00, 0b000, bitconvert, GR64, FP32>, Requires<[HasF, IsRV64]>;
+def FMV_S_X64 : InstConv<"fmv.s.x", "", 0b1010011, 0b11110, 0b00, 0b000, bitconvert, FP32, GR64>, Requires<[HasF, IsRV64]>;
+
+//Floating point comparisons
+def FEQ_S : InstSign<"feq.s", 0b1010011, 0b10101, 0b00, 0b000, setoeq, GR32, FP32>, Requires<[HasF]>;
+def FLT_S : InstSign<"flt.s", 0b1010011, 0b10110, 0b00, 0b000, setolt, GR32, FP32>, Requires<[HasF]>;
+def FLE_S : InstSign<"fle.s", 0b1010011, 0b10111, 0b00, 0b000, setole, GR32, FP32>, Requires<[HasF]>;
+def FUEQ_S : InstSign<"feq.s", 0b1010011, 0b10101, 0b00, 0b000, setueq, GR32, FP32>, Requires<[HasF]>;
+def FULT_S : InstSign<"flt.s", 0b1010011, 0b10110, 0b00, 0b000, setult, GR32, FP32>, Requires<[HasF]>;
+def FULE_S : InstSign<"fle.s", 0b1010011, 0b10111, 0b00, 0b000, setule, GR32, FP32>, Requires<[HasF]>;
+//synthesized set operators
+multiclass FPCmpPats<RegisterOperand RC, Instruction FEQOp, Instruction FEQUOp,
+                     Instruction FLTOp, Instruction FLTUOp,
+                     Instruction FLEOp, Instruction FLEUOp> {
+                //RISC-V operands go in reverse order
+  //lhs > rhs ==> rhs < lhs
+  def : Pat<(setogt RC:$lhs, RC:$rhs), (FLTOp RC:$lhs, RC:$rhs)>;
+  def : Pat<(setugt RC:$lhs, RC:$rhs), (FLTUOp RC:$lhs, RC:$rhs)>;
+  //lhs >= rhs ==> rhs =< lhs
+  def : Pat<(setoge RC:$lhs, RC:$rhs), (FLEOp RC:$lhs, RC:$rhs)>;
+  def : Pat<(setuge RC:$lhs, RC:$rhs), (FLEUOp RC:$lhs, RC:$rhs)>;
+  //lhs != rhs ==> !(lhs == rhs) using seqz 
+  def : Pat<(setone RC:$lhs, RC:$rhs), (SLTIU (FEQOp RC:$rhs, RC:$lhs), 1)>;
+  def : Pat<(setune RC:$lhs, RC:$rhs), (SLTIU (FEQUOp RC:$rhs, RC:$lhs), 1)>;
+}
+defm : FPCmpPats<FP32, FEQ_S, FUEQ_S, FLT_S, FULT_S, FLE_S, FULE_S>;
+
+//fp imm
+def :Pat<(fpimm0), (FCVT_S_W_RDY zero)>;
+
+//fp select
+let usesCustomInserter = 1 in {
+  def FSELECT_CC_F : Pseudo<(outs FP32:$dst),
+                              (ins GR32:$cond, FP32:$T, FP32:$F),
+                              [(set FP32:$dst,
+                                 (select GR32:$cond, FP32:$T, FP32:$F))]>, Requires<[HasF]>;
+}
diff --git a/lib/Target/RISCV/RISCVInstrInfoM.td b/lib/Target/RISCV/RISCVInstrInfoM.td
new file mode 100644
index 0000000..0023387
--- /dev/null
+++ b/lib/Target/RISCV/RISCVInstrInfoM.td
@@ -0,0 +1,39 @@
+//===- RISCVInstrM.td - Multiply Divide RISCV instructions ----*- tblgen-*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+//RV32
+def MUL   : InstR<"mul"  , 0b0110011, 0b0000001, 0b000, mul   , GR32, GR32>, Requires<[IsRV32, HasM]>;
+def MULH  : InstR<"mulh" , 0b0110011, 0b0000001, 0b001, mulhs , GR32, GR32>, Requires<[HasM]>;
+//TODO: no corresponding llvm ir instruction
+//def MULHSU: InstR<"mulh", 0b0110011, 0b0000001, 0b010, mulhs , GR32, GR32>, Requires<[HasM]>;
+def MULHU : InstR<"mulhu", 0b0110011, 0b0000001, 0b011, mulhu , GR32, GR32>, Requires<[HasM]>;
+def DIV   : InstR<"div"  , 0b0110011, 0b0000001, 0b100, sdiv  , GR32, GR32>, Requires<[IsRV32, HasM]>;
+def DIVU  : InstR<"divu" , 0b0110011, 0b0000001, 0b101, udiv  , GR32, GR32>, Requires<[IsRV32, HasM]>;
+def REM   : InstR<"rem"  , 0b0110011, 0b0000001, 0b110, srem  , GR32, GR32>, Requires<[IsRV32, HasM]>;
+def REMU  : InstR<"remu" , 0b0110011, 0b0000001, 0b111, urem  , GR32, GR32>, Requires<[IsRV32, HasM]>;
+
+//RV64
+//standard M instructions on 64bit values
+def MUL64   : InstR<"mul"  , 0b0110011, 0b0000001, 0b000, mul   , GR64, GR64>, Requires<[IsRV64, HasM]>;
+def MULH64  : InstR<"mulh" , 0b0110011, 0b0000001, 0b001, mulhs , GR64, GR64>, Requires<[IsRV64, HasM]>;
+//TODO: no corresponding llvm ir instruction
+ //def MULHSU: InstR<"mulh", 0b0110011, 0b0000001, 0b010, mulhs , GR64, GR64>, Requires<[IsRV64, HasM]>;
+def MULHU64 : InstR<"mulhu", 0b0110011, 0b0000001, 0b011, mulhu , GR64, GR64>, Requires<[IsRV64, HasM]>;
+def DIV64   : InstR<"div"  , 0b0110011, 0b0000001, 0b100, sdiv  , GR64, GR64>, Requires<[IsRV64, HasM]>;
+def DIVU64  : InstR<"divu" , 0b0110011, 0b0000001, 0b101, udiv  , GR64, GR64>, Requires<[IsRV64, HasM]>;
+def REM64   : InstR<"rem"  , 0b0110011, 0b0000001, 0b110, srem  , GR64, GR64>, Requires<[IsRV64, HasM]>;
+def REMU64  : InstR<"remu" , 0b0110011, 0b0000001, 0b111, urem  , GR64, GR64>, Requires<[IsRV64, HasM]>;
+
+//special rv64 instructions
+//TODO:llvm mul won't sign extend
+def MULW    : InstR<"mulw" , 0b0111011, 0b0000001, 0b000, mul   , GR32, GR32>, Requires<[IsRV64, HasM]>;
+def DIVW    : InstR<"divw" , 0b0111011, 0b0000001, 0b100, sdiv  , GR32, GR32>, Requires<[IsRV64, HasM]>;
+def DIVUW   : InstR<"divuw", 0b0111011, 0b0000001, 0b101, udiv  , GR32, GR32>, Requires<[IsRV64, HasM]>;
+def REMW    : InstR<"remw" , 0b0111011, 0b0000001, 0b110, srem  , GR32, GR32>, Requires<[IsRV64, HasM]>;
+def REMUW   : InstR<"remuw", 0b0111011, 0b0000001, 0b111, urem  , GR32, GR32>, Requires<[IsRV64, HasM]>;
diff --git a/lib/Target/RISCV/RISCVInstrInfoRV64.td b/lib/Target/RISCV/RISCVInstrInfoRV64.td
new file mode 100644
index 0000000..a0eb0b0
--- /dev/null
+++ b/lib/Target/RISCV/RISCVInstrInfoRV64.td
@@ -0,0 +1,396 @@
+//===- RISCVInstrRV64.td - RISCV RV64I instructions -----------*- tblgen-*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+//special 64bit instructions
+def ADDW : InstR<"addw" , 0b0111011, 0b0000000, 0b000, add   , GR32, GR32>, Requires<[IsRV64]>;
+def SUBW : InstR<"subw" , 0b0111011, 0b0100000, 0b000, sub   , GR32, GR32>, Requires<[IsRV64]>;
+def SLLW : InstR<"sllw" , 0b0111011, 0b0000000, 0b001, shl   , GR32, GR32>, Requires<[IsRV64]>;
+def SRLW : InstR<"srlw" , 0b0111011, 0b0000000, 0b101, srl   , GR32, GR32>, Requires<[IsRV64]>;
+def SRAW : InstR<"sraw" , 0b0111011, 0b0100000, 0b101, sra   , GR32, GR32>, Requires<[IsRV64]>;
+
+//Integer arithmetic register-immediate
+def ADDIW:  InstI<"addiw",   0b0011011, 0b000       , add, GR32, GR32, imm32sx12>, Requires<[IsRV64]>;
+
+def SEXT_W  : InstAlias<"sext.w $dst, $src", (ADDIW GR32:$dst, GR32:$src, 0)>, Requires<[IsRV64]>;
+
+//TODO: enforce constraints here or up on level?
+def SLLIW: InstI<"slliw", 0b0011011, 0b001       , shl, GR32, GR32, imm32sx12>, Requires<[IsRV64]> {
+  let IMM{11-5} = 0b0000000; 
+  //trap if $imm{5}!=0 TODO:how to do this?
+}
+def SLLIW64: InstI<"slliw", 0b0011011, 0b001       , shl, GR32, GR32, imm64sx12>, Requires<[IsRV64]> {
+  let IMM{11-5} = 0b0000000; 
+  //trap if $imm{5}!=0 TODO:how to do this?
+}
+def SRLIW: InstI<"srliw", 0b0011011, 0b101       , srl, GR32, GR32, imm32sx12>, Requires<[IsRV64]> {
+  let IMM{11-5} = 0b0000000; 
+  //trap if $src{5}!=0 TODO:how to do this?
+}
+def SRLIW64: InstI<"srliw", 0b0011011, 0b101       , srl, GR32, GR32, imm64sx12>, Requires<[IsRV64]> {
+  let IMM{11-5} = 0b0000000; 
+  //trap if $src{5}!=0 TODO:how to do this?
+}
+def SRAIW: InstI<"sraiw", 0b0011011, 0b101       , sra, GR32, GR32, imm32sx12>, Requires<[IsRV64]> {
+  let IMM{11-6} = 0b010000;
+  //trap if $src{5}!=0 TODO:how to do this?
+}
+def SRAIW64: InstI<"sraiw", 0b0011011, 0b101       , sra, GR32, GR32, imm64sx12>, Requires<[IsRV64]> {
+  let IMM{11-6} = 0b010000;
+  //trap if $src{5}!=0 TODO:how to do this?
+}
+
+//Load/Store Instructions
+let mayLoad = 1 in {
+  def LWU : InstLoad <"lwu" , 0b0000011, 0b110, zextloadi32,  GR64, mem64>, Requires<[IsRV64]>; 
+  //def LWU64 : InstLoad <"lwu" , 0b0000011, 0b110, load,  GR32, mem64>, Requires<[IsRV64]>; 
+  def LD  : InstLoad <"ld"  , 0b0000011, 0b011, load,  GR64, mem64>, Requires<[IsRV64]>; 
+}
+
+let mayStore = 1 in {
+  def SD : InstStore <"sd"  , 0b0100011, 0b011, store, GR64, mem64>, Requires<[IsRV64]>;
+}
+
+//Standard instructions operating on 64bit values
+//Integer arithmetic register-register
+def ADD64 : InstR<"add" , 0b0110011, 0b0000000, 0b000, add   , GR64, GR64>, Requires<[IsRV64]>;
+def SUB64 : InstR<"sub" , 0b0110011, 0b0100000, 0b000, sub   , GR64, GR64>, Requires<[IsRV64]>;
+def SLL64 : InstR<"sll" , 0b0110011, 0b0000000, 0b001, shl   , GR64, GR64>, Requires<[IsRV64]>;
+def SLT64 : InstR<"slt" , 0b0110011, 0b0000000, 0b010, setlt , GR32, GR64>, Requires<[IsRV64]>;
+def SLTU64: InstR<"sltu", 0b0110011, 0b0000000, 0b011, setult, GR32, GR64>, Requires<[IsRV64]>;
+def XOR64 : InstR<"xor" , 0b0110011, 0b0000000, 0b100, xor   , GR64, GR64>, Requires<[IsRV64]>;
+def SRL64 : InstR<"srl" , 0b0110011, 0b0000000, 0b101, srl   , GR64, GR64>, Requires<[IsRV64]>;
+def SRA64 : InstR<"sra" , 0b0110011, 0b0100000, 0b101, sra   , GR64, GR64>, Requires<[IsRV64]>;
+def OR64  : InstR<"or"  , 0b0110011, 0b0000000, 0b110, or    , GR64, GR64>, Requires<[IsRV64]>;
+def AND64 : InstR<"and" , 0b0110011, 0b0000000, 0b111, and   , GR64, GR64>, Requires<[IsRV64]>;
+//Integer arithmetic register-immediate
+def ADDI64: InstI<"addi", 0b0010011, 0b000       , add, GR64, GR64, imm64sx12>, Requires<[IsRV64]>;
+def XORI64: InstI<"xori", 0b0010011, 0b100       , xor, GR64, GR64, imm64sx12>, Requires<[IsRV64]>;
+def ORI64 : InstI<"ori" , 0b0010011, 0b110       , or , GR64, GR64, imm64sx12>, Requires<[IsRV64]>;
+def ANDI64: InstI<"andi", 0b0010011, 0b111       , and, GR64, GR64, imm64sx12>, Requires<[IsRV64]>;
+
+def NOP64 : InstAlias<"nop", (ADDI64 zero_64, zero_64, 0)>, Requires<[IsRV64]>;
+def MV64  : InstAlias<"mv $dst, $src", (ADDI64 GR64:$dst, GR64:$src, 0)>, Requires<[IsRV64]>;
+def NOT64 : InstAlias<"not $dst, $src", (XORI64 GR64:$dst, GR64:$src, -1)>, Requires<[IsRV64]>;
+
+//TODO: check 64bit shifr constraints
+//TODO: enforce constraints here or up on level?
+def SLLI64: InstI<"slli", 0b0010011, 0b001       , shl, GR64, GR64, imm64sx12>, Requires<[IsRV64]> {
+  let IMM{11-6} = 0b000000; 
+  //trap if $imm{5}!=0 TODO:how to do this?
+}
+def SRLI64: InstI<"srli", 0b0010011, 0b101       , srl, GR64, GR64, imm64sx12>, Requires<[IsRV64]> {
+  let IMM{11-6} = 0b000000; 
+  //trap if $src{5}!=0 TODO:how to do this?
+}
+def SRAI64: InstI<"srai", 0b0010011, 0b101       , sra, GR64, GR64, imm64sx12>, Requires<[IsRV64]> {
+  let IMM{11-6} = 0b010000;
+  //trap if $src{5}!=0 TODO:how to do this?
+}
+def SLTI64 : InstI<"slti", 0b0010011, 0b010, setlt, GR32, GR64, imm64sx12>, Requires<[IsRV64]>;
+def SLTIU64: InstI<"sltiu",0b0010011, 0b011, setult,GR32, GR64, imm64sx12>, Requires<[IsRV64]>;
+
+def SEQZ64 : InstAlias<"seqz $dst, $src", (SLTIU64 GR32:$dst, GR64:$src, 1)>, Requires<[IsRV64]>;
+
+//Synthesized set operators
+defm : SeteqPats<GR64, SLTIU64, XOR64, SLTU64, zero_64>, Requires<[IsRV64]>;
+defm : SetlePats<GR64, SLT64, SLTU64>, Requires<[IsRV64]>;
+defm : SetgtPats<GR64, SLT64, SLTU64>, Requires<[IsRV64]>;
+defm : SetgePats<GR64, SLT64, SLTU64>, Requires<[IsRV64]>;
+
+//Unconditional Jumps
+let isBranch = 1, isTerminator = 1, isBarrier = 1 in {
+  def J64  : InstJ<0b1100111, (outs), (ins jumptarget:$target), "j\t$target", 
+          [(br bb:$target)]>, Requires<[IsRV64]>;
+}
+let isCall = 1, Defs = [ra_64, a0_64, a1_64, fa0, fa1, fa0_64, fa1_64] in {
+    def JAL64: InstJ<0b1101111, (outs GR64:$ret), (ins pcrel64call:$target),
+      "jal\t$ret, $target", 
+          [(set GR64:$ret, (r_jal pcrel64call:$target))]>, Requires<[IsRV64]>;
+}
+
+//call psuedo ops
+let isCall = 1, isCodeGenOnly = 1, usesCustomInserter = 1,
+  Defs = [ra_64, a0_64, a1_64, fa0, fa1, fa0_64, fa1_64] in {
+  def CALL64 : Pseudo<(outs), (ins pcrel64call:$target),
+                              [(r_call pcrel64call:$target)]>, Requires<[IsRV64]>;
+  def CALLREG64 : Pseudo<(outs), (ins jalrmem64:$target),
+                              [(r_call addr:$target)]>, Requires<[IsRV64]>;
+}
+
+let isCall = 1, Defs = [ra_64, a0_64, a1_64, fa0, fa1, fa0_64, fa1_64] in {
+    def JALR64: InstRISCV<4, (outs GR64:$ret), (ins jalrmem64:$target),
+          "jalr\t$ret, $target",
+          [(set GR64:$ret, (r_jal addr:$target))]>, Requires<[IsRV64]>{
+            field bits<32> Inst;
+
+            bits<5> RD;
+            bits<5> RS1;
+            bits<12> IMM;
+
+            let Inst{31-20} = IMM{11-0};
+            let Inst{19-15} = RS1;
+            let Inst{14-12} = 0b000;
+            let Inst{11- 7} = RD;
+            let Inst{6 - 0} = 0b1100111;
+          }
+}
+//simple brind pat
+def : Pat<(brind i64:$dst), (JALR64 0, $dst)>, Requires<[IsRV64]>;
+ 
+
+//Conditional Branches
+//TODO:refactor to class
+let isBranch = 1, isTerminator = 1, isBarrier = 1 in {
+  def BEQ64 : InstB<0b1100011, 0b000, (outs), 
+              (ins brtarget:$target, GR64:$src1, GR64:$src2), 
+              "beq\t$src1, $src2, $target", 
+              [(brcond (i32 (seteq GR64:$src1,  GR64:$src2)), bb:$target)]>, Requires<[IsRV64]>;
+  def BNE64 : InstB<0b1100011, 0b001, (outs), 
+              (ins brtarget:$target, GR64:$src1, GR64:$src2), 
+              "bne\t$src1, $src2, $target", 
+              [(brcond (i32 (setne GR64:$src1, GR64:$src2)), bb:$target)]>, Requires<[IsRV64]>;
+  def BLT64 : InstB<0b1100011, 0b100, (outs), 
+              (ins brtarget:$target, GR64:$src1, GR64:$src2), 
+              "blt\t$src1, $src2, $target", 
+              [(brcond (i32 (setlt GR64:$src1, GR64:$src2)), bb:$target)]>, Requires<[IsRV64]>;
+  def BGE64 : InstB<0b1100011, 0b101, (outs), 
+              (ins brtarget:$target, GR64:$src1, GR64:$src2), 
+              "bge\t$src1, $src2, $target", 
+              [(brcond (i32 (setge GR64:$src1, GR64:$src2)), bb:$target)]>, Requires<[IsRV64]>;
+  def BLTU64: InstB<0b1100011, 0b110, (outs), 
+              (ins brtarget:$target, GR64:$src1, GR64:$src2), 
+              "bltu\t$src1, $src2, $target", 
+              [(brcond (i32 (setult GR64:$src1, GR64:$src2)), bb:$target)]>, Requires<[IsRV64]>;
+  def BGEU64: InstB<0b1100011, 0b111, (outs), 
+              (ins brtarget:$target, GR64:$src1, GR64:$src2), 
+              "bgeu\t$src1, $src2, $target", 
+              [(brcond (i32 (setuge GR64:$src1, GR64:$src2)), bb:$target)]>, Requires<[IsRV64]>;
+
+//Synthesize remaining condition codes by reverseing operands
+  def BGT64 : InstB<0b1100011, 0b100, (outs), 
+              (ins brtarget:$target, GR64:$src1, GR64:$src2), 
+              "blt\t$src2, $src1, $target", 
+              [(brcond (i32 (setgt GR64:$src1, GR64:$src2)), bb:$target)]>, Requires<[IsRV64]>;
+  def BGTU64: InstB<0b1100011, 0b110, (outs), 
+              (ins brtarget:$target, GR64:$src1, GR64:$src2), 
+              "bltu\t$src2, $src1, $target", 
+              [(brcond (i32 (setugt GR64:$src1, GR64:$src2)), bb:$target)]>, Requires<[IsRV64]>;
+  def BLE64 : InstB<0b1100011, 0b101, (outs), 
+              (ins brtarget:$target, GR64:$src1, GR64:$src2), 
+              "bge\t$src2, $src1, $target", 
+              [(brcond (i32 (setle GR64:$src1, GR64:$src2)), bb:$target)]>, Requires<[IsRV64]>;
+  def BLEU64: InstB<0b1100011, 0b111, (outs), 
+              (ins brtarget:$target, GR64:$src1, GR64:$src2), 
+              "bgeu\t$src2, $src1, $target", 
+              [(brcond (i32 (setule GR64:$src1, GR64:$src2)), bb:$target)]>, Requires<[IsRV64]>;
+}
+
+//constant branches (e.g. br 1 $label or br 0 $label)
+def : Pat<(brcond GR64Bit:$cond, bb:$target),
+          (BNE64 bb:$target, GR64Bit:$cond, zero_64)>;  
+
+def : Pat<(i64 0), (i64 zero_64)>;
+//Conditional moves
+// SELECT_CC_* - Used to implement the SELECT_CC DAG operation.  Expanded after
+// instruction selection into a branch sequence.
+let usesCustomInserter = 1 in {
+  def SELECT_CC64 : Pseudo<(outs GR64:$dst),
+                              (ins GR32:$cond, GR64:$T, GR64:$F),
+                              [(set GR64:$dst,
+                                 (select GR32:$cond, GR64:$T, GR64:$F))]>, Requires<[IsRV64]>;
+}
+
+//Load/Store Instructions
+let mayLoad = 1 in {
+  def LW64_32 : InstLoad <"lw" , 0b0000011, 0b010, load, GR32, mem64>, Requires<[IsRV64]>; 
+  def LH64_32 : InstLoad <"lh" , 0b0000011, 0b001, sextloadi16, GR32, mem64>, Requires<[IsRV64]>; 
+  def LHU64_32: InstLoad <"lhu", 0b0000011, 0b101, zextloadi16, GR32, mem64>, Requires<[IsRV64]>; 
+  def LB64_32 : InstLoad <"lb" , 0b0000011, 0b000, sextloadi8, GR32, mem64>, Requires<[IsRV64]>; 
+  def LBU64_32: InstLoad <"lbu", 0b0000011, 0b100, zextloadi8, GR32, mem64>, Requires<[IsRV64]>; 
+  def LW64 : InstLoad <"lw" , 0b0000011, 0b010, sextloadi32, GR64, mem64>, Requires<[IsRV64]>; 
+  def LH64 : InstLoad <"lh" , 0b0000011, 0b001, sextloadi16, GR64, mem64>, Requires<[IsRV64]>; 
+  def LHU64: InstLoad <"lhu", 0b0000011, 0b101, zextloadi16, GR64, mem64>, Requires<[IsRV64]>; 
+  def LB64 : InstLoad <"lb" , 0b0000011, 0b000, sextloadi8, GR64, mem64>, Requires<[IsRV64]>; 
+  def LBU64: InstLoad <"lbu", 0b0000011, 0b100, zextloadi8, GR64, mem64>, Requires<[IsRV64]>; 
+}
+//extended loads
+def : Pat<(i64 (extloadi1  addr:$addr)), (LBU64 addr:$addr)>;
+def : Pat<(i32 (extloadi1  addr:$addr)), (LBU64_32 addr:$addr)>;
+def : Pat<(extloadi1  addr:$addr), (LBU64 addr:$addr)>;
+def : Pat<(i64 (extloadi8  addr:$addr)), (LBU64 addr:$addr)>;
+def : Pat<(i32 (extloadi8  addr:$addr)), (LBU64_32 addr:$addr)>;
+def : Pat<(extloadi8  addr:$addr), (LBU64 addr:$addr)>;
+def : Pat<(i64 (extloadi16 addr:$addr)), (LHU64 addr:$addr)>;
+def : Pat<(i32 (extloadi16 addr:$addr)), (LHU64_32 addr:$addr)>;
+def : Pat<(extloadi16 addr:$addr), (LHU64 addr:$addr)>;
+def : Pat<(i64 (extloadi32 addr:$addr)), (LW64   addr:$addr)>;
+def : Pat<(extloadi32 addr:$addr), (LW64   addr:$addr)>;
+//def : Pat<(i32 (extloadi1  addr:$addr)), (LBU64_32 addr:$addr)>, Requires<[IsRV64]>;
+def : Pat<(i32 (extloadi8  addr:$addr)), (LBU64_32 addr:$addr)>, Requires<[IsRV64]>;
+//def : Pat<(i32 (extloadi16 addr:$addr)), (LHU64_32 addr:$addr)>, Requires<[IsRV64]>;
+
+let mayStore = 1 in {
+  def SW64 : InstStore<"sw" , 0b0100011, 0b010, truncstorei32, GR64, mem64>, Requires<[IsRV64]>;
+  def SH64 : InstStore<"sh" , 0b0100011, 0b001, truncstorei16, GR64, mem64>, Requires<[IsRV64]>; 
+  def SB64 : InstStore<"sb" , 0b0100011, 0b000, truncstorei8 , GR64, mem64>, Requires<[IsRV64]>; 
+  def SW64_32 : InstStore<"sw" , 0b0100011, 0b010, store, GR32, mem64>, Requires<[IsRV64]>;
+  def SH64_32 : InstStore<"sh" , 0b0100011, 0b001, truncstorei16, GR32, mem64>, Requires<[IsRV64]>; 
+  def SB64_32 : InstStore<"sb" , 0b0100011, 0b000, truncstorei8 , GR32, mem64>, Requires<[IsRV64]>; 
+}
+
+//Upper Immediate
+def LUI64: InstU<0b0110111, (outs GR64:$dst), (ins imm64sxu20:$imm),
+                 "lui\t$dst, $imm",
+                 [(set GR64:$dst, (shl imm64sx20:$imm, (i64 12)))]>;
+
+def AUIPC64: InstU<0b0110111, (outs GR64:$dst), (ins pcimm64:$target),
+                   "auipc\t$dst, $target",
+                   [(set GR64:$dst, (r_pcrel_wrapper imm64:$target))]>;
+
+
+//psuedo load low imm instruction to print operands better
+def LLI64 : InstI<"addi", 0b0010011, 0b000       , add, GR64, GR64, imm64sx12>;
+
+///64 bit immediate loading
+// Transformation Function - get the lower 32 bits.
+def LO32 : SDNodeXForm<imm, [{
+    return getImm(N, N->getZExtValue() & 0xFFFFFFFF);
+}]>;
+
+// Transformation Function - get the higher 32 bits for large immediate loading
+def HI32 : SDNodeXForm<imm, [{
+    uint64_t value = N->getZExtValue() & 0x0000080000000 ? 
+                     (N->getZExtValue() >> 32)+1:
+                     (N->getZExtValue() >> 32);
+    return getImm(N, value);
+}]>;
+def LI64 : InstRISCV<4, (outs GR64:$dst), (ins imm64:$imm), "li\t$dst, $imm",
+  []> {
+    let isPseudo = 1;
+}
+def LI64_32 : InstRISCV<4, (outs GR64:$dst), (ins imm32:$imm), "li\t$dst, $imm",
+  []> {
+    let isPseudo = 1;
+}
+
+def LA64 : InstRISCV<4, (outs GR64:$dst), (ins imm64:$label), "la\t$dst, $label",
+  []>, Requires<[IsRV64]>{
+    let isPseudo = 1;
+}
+//simple immediate loading
+//simple zext i32 to i64
+def : Pat<(i64 (zext GR32:$val)), (SUBREG_TO_REG (i64 0), GR32:$val, sub_32)>;
+//TODO: how does this get handled in rocket/gcc
+def : Pat<(i64 (sext GR32:$val)), (SUBREG_TO_REG (i64 0), GR32:$val, sub_32)>;
+def : Pat<(i64 (anyext GR32:$val)), (SUBREG_TO_REG (i64 0), GR32:$val, sub_32)>;
+def :Pat<(i32 (trunc GR64:$src)), (EXTRACT_SUBREG GR64:$src, sub_32)>;
+def : Pat<(i64 imm64:$imm), (LI64 imm64:$imm)>; //cheat and use gas for these
+//def : Pat<(i32 imm32:$imm), (EXTRACT_SUBREG (SRLI64 (SLLI64 (LI64_32 imm32:$imm), 32), 32), sub_32)>; //cheat and use gas for these
+def : Pat<(i64 imm64sx12:$imm), (LI64 imm64sx12:$imm)>; 
+def : Pat<(i64 imm64sxu32:$imm), (LI64 imm64sxu32:$imm)>; //cheat and use gas for these
+//call
+def : Pat<(r_call (i64 texternalsym:$in)), (CALL64 texternalsym:$in)>;
+def : Pat<(r_call (i64 tglobaladdr:$in)), (CALL64 tglobaladdr:$in)>;
+//pcrel addr loading using LA
+def : Pat<(r_pcrel_wrapper tglobaladdr:$in), (LA64 tglobaladdr:$in)>, Requires<[IsRV64]>;
+def : Pat<(r_pcrel_wrapper tblockaddress:$in), (LA64 tblockaddress:$in)>, Requires<[IsRV64]>;
+def : Pat<(r_pcrel_wrapper tjumptable:$in), (LA64 tjumptable:$in)>, Requires<[IsRV64]>;
+def : Pat<(r_pcrel_wrapper tconstpool:$in), (LA64 tconstpool:$in)>, Requires<[IsRV64]>;
+def : Pat<(r_pcrel_wrapper tglobaltlsaddr:$in), (LA64 tglobaltlsaddr:$in)>, Requires<[IsRV64]>;
+def : Pat<(r_pcrel_wrapper texternalsym:$in), (LA64 texternalsym:$in)>, Requires<[IsRV64]>;
+//constpool
+//def : Pat<(r_pcrel_wrapper tconstpool:$in), (ADDI64 (LUI64 (RISCVHi tconstpool:$in)), (RISCVLo tconstpool:$in))>;
+//global addr loading
+def : Pat<(RISCVHi tglobaladdr:$in), (LUI64 tglobaladdr:$in)>;
+def : Pat<(RISCVHi tblockaddress:$in), (LUI64 tblockaddress:$in)>;
+def : Pat<(RISCVHi tjumptable:$in), (LUI64 tjumptable:$in)>;
+def : Pat<(RISCVHi tconstpool:$in), (LUI64 tconstpool:$in)>;
+def : Pat<(RISCVHi tglobaltlsaddr:$in), (LUI64 tglobaltlsaddr:$in)>;
+def : Pat<(RISCVHi texternalsym:$in), (LUI64 texternalsym:$in)>;
+
+def : Pat<(RISCVLo tglobaladdr:$in), (ADDI64 zero_64, tglobaladdr:$in)>;
+def : Pat<(RISCVLo tblockaddress:$in), (ADDI64 zero_64, tblockaddress:$in)>;
+def : Pat<(RISCVLo tjumptable:$in), (ADDI64 zero_64, tjumptable:$in)>;
+def : Pat<(RISCVLo tconstpool:$in), (ADDI64 zero_64, tconstpool:$in)>;
+def : Pat<(RISCVLo tglobaltlsaddr:$in),
+          (ADDI64 zero_64, tglobaltlsaddr:$in)>;
+def : Pat<(RISCVLo texternalsym:$in), (ADDI64 zero_64, texternalsym:$in)>;
+
+def : Pat<(add GR64:$hi, (RISCVLo tglobaladdr:$lo)),
+          (ADDI64 GR64:$hi, tglobaladdr:$lo)>;
+def : Pat<(add GR64:$hi, (RISCVLo tblockaddress:$lo)),
+          (ADDI64 GR64:$hi, tblockaddress:$lo)>;
+def : Pat<(add GR64:$hi, (RISCVLo tjumptable:$lo)),
+          (ADDI64 GR64:$hi, tjumptable:$lo)>;
+def : Pat<(add GR64:$hi, (RISCVLo tconstpool:$lo)),
+          (ADDI64 GR64:$hi, tconstpool:$lo)>;
+def : Pat<(add GR64:$hi, (RISCVLo tglobaltlsaddr:$lo)),
+          (ADDI64 GR64:$hi, tglobaltlsaddr:$lo)>;
+
+//Fence
+def FENCE64: InstRISCV<4, (outs), (ins fenceImm64:$pred, fenceImm64:$succ), "fence", 
+      [(r_fence64 fenceImm64:$pred, fenceImm64:$succ)]>, Requires<[IsRV64]>{
+        field bits<32> Inst;
+
+        bits<4> pred;
+        bits<4> succ;
+
+        let Inst{31-28} = 0b0000;
+        let Inst{27   } = pred{3};//PI;
+        let Inst{26   } = pred{2};//PO;
+        let Inst{25   } = pred{1};//PR;
+        let Inst{24   } = pred{0};//PW;
+        let Inst{23   } = succ{3};//SI;
+        let Inst{22   } = succ{2};//SO;
+        let Inst{21   } = succ{1};//SR;
+        let Inst{20   } = succ{0};//SW;
+        let Inst{19-15} = 0b00000;
+        let Inst{14-12} = 0b000;
+        let Inst{11- 7} = 0b00000;
+        let Inst{6 - 0} = 0b0001111;
+      }
+
+//Fence.I
+def FENCE64_I: InstRISCV<4, (outs), (ins fenceImm64:$pred, fenceImm64:$succ), "fence.i", 
+      [(r_fence64 fenceImm64:$pred, fenceImm64:$succ)]>, Requires<[IsRV64]>{
+        field bits<32> Inst;
+
+        bits<4> pred;
+        bits<4> succ;
+
+        let Inst{31-28} = 0b0000;
+        let Inst{27   } = 0b0;
+        let Inst{26   } = 0b0;
+        let Inst{25   } = 0b0;
+        let Inst{24   } = 0b0;
+        let Inst{23   } = 0b0;
+        let Inst{22   } = 0b0;
+        let Inst{21   } = 0b0;
+        let Inst{20   } = 0b0;
+        let Inst{19-15} = 0b00000;
+        let Inst{14-12} = 0b001;
+        let Inst{11- 7} = 0b00000;
+        let Inst{6 - 0} = 0b0001111;
+      }
+
+let isReturn = 1, isTerminator = 1, isBarrier = 1, hasCtrlDep = 1,
+    isCodeGenOnly = 1, Defs = [a0_64, a1_64] in {
+  def RET64 : InstRISCV<4, (outs), (ins), "ret", 
+          []>, Requires<[IsRV64]>{
+            field bits<32> Inst;
+            
+            let Inst{31-27} = 0;// destination zero
+            let Inst{26-22} = 1;// target ra
+            let Inst{21-17} = 0;// imm 0
+            let Inst{16-10} = 0;// imm 0
+            let Inst{9 - 7} = 0b000;
+            let Inst{6 - 0} = 0b1101011;
+          }
+   }
+  def : Pat<(r_retflag), (RET)>, Requires<[IsRV64]>;
diff --git a/lib/Target/RISCV/RISCVMCInstLower.cpp b/lib/Target/RISCV/RISCVMCInstLower.cpp
new file mode 100644
index 0000000..73c0a5d
--- /dev/null
+++ b/lib/Target/RISCV/RISCVMCInstLower.cpp
@@ -0,0 +1,118 @@
+//===-- RISCVMCInstLower.cpp - Lower MachineInstr to MCInst -----*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVMCInstLower.h"
+#include "RISCVAsmPrinter.h"
+#include "llvm/MC/MCExpr.h"
+#include "llvm/MC/MCInst.h"
+#include "llvm/MC/MCStreamer.h"
+
+using namespace llvm;
+
+// Where relaxable pairs of reloc-generating instructions exist,
+// we tend to use the longest form by default, since that produces
+// correct assembly in cases where no relaxation is performed.
+// If Opcode is one such instruction, return the opcode for the
+// shortest possible form instead, otherwise return Opcode itself.
+static unsigned getShortenedInstr(unsigned Opcode) {
+  switch (Opcode) {
+  }
+  return Opcode;
+}
+
+// Return the VK_* enumeration for MachineOperand target flags Flags.
+static MCSymbolRefExpr::VariantKind getVariantKind(unsigned Flags) {
+  switch (Flags & RISCVII::MO_SYMBOL_MODIFIER) {
+    case 0:
+      return MCSymbolRefExpr::VK_None;
+    case RISCVII::MO_GOT:
+      return MCSymbolRefExpr::VK_GOT;
+  }
+  llvm_unreachable("Unrecognised MO_ACCESS_MODEL");
+}
+
+RISCVMCInstLower::RISCVMCInstLower(MCContext &ctx, RISCVAsmPrinter &asmprinter)
+    : Ctx(ctx), AsmPrinter(asmprinter) {}
+
+MCOperand RISCVMCInstLower::lowerSymbolOperand(const MachineOperand &MO,
+                                                 const MCSymbol *Symbol,
+                                                 int64_t Offset) const {
+  MCSymbolRefExpr::VariantKind Kind = getVariantKind(MO.getTargetFlags());
+  switch(MO.getTargetFlags()) {
+    case RISCVII::MO_ABS_HI:    Kind = MCSymbolRefExpr::VK_Mips_ABS_HI; break;
+    case RISCVII::MO_ABS_LO:    Kind = MCSymbolRefExpr::VK_Mips_ABS_LO; break;
+    case RISCVII::MO_TPREL_HI:    Kind = MCSymbolRefExpr::VK_Mips_TPREL_HI; break;
+    case RISCVII::MO_TPREL_LO:    Kind = MCSymbolRefExpr::VK_Mips_TPREL_LO; break;
+  }
+  const MCExpr *Expr = MCSymbolRefExpr::create(Symbol, Kind, Ctx);
+  if (Offset) {
+    const MCExpr *OffsetExpr = MCConstantExpr::create(Offset, Ctx);
+    Expr = MCBinaryExpr::createAdd(Expr, OffsetExpr, Ctx);
+  }
+  return MCOperand::createExpr(Expr);
+}
+
+MCOperand RISCVMCInstLower::lowerOperand(const MachineOperand &MO) const {
+  switch (MO.getType()) {
+  default:
+    llvm_unreachable("unknown operand type");
+
+  case MachineOperand::MO_Register:
+    // Ignore all implicit register operands.
+    if (MO.isImplicit())
+      return MCOperand();
+    return MCOperand::createReg(MO.getReg());
+
+  case MachineOperand::MO_Immediate:
+    return MCOperand::createImm(MO.getImm());
+
+  case MachineOperand::MO_MachineBasicBlock:
+    return lowerSymbolOperand(MO, MO.getMBB()->getSymbol(),
+                              /* MO has no offset field */0);
+
+  case MachineOperand::MO_GlobalAddress:
+    return lowerSymbolOperand(MO, AsmPrinter.getSymbol(MO.getGlobal()),
+                              MO.getOffset());
+
+  case MachineOperand::MO_ExternalSymbol: {
+    StringRef Name = MO.getSymbolName();
+    return lowerSymbolOperand(MO, AsmPrinter.GetExternalSymbolSymbol(Name),
+                              MO.getOffset());
+  }
+
+  case MachineOperand::MO_JumpTableIndex:
+    return lowerSymbolOperand(MO, AsmPrinter.GetJTISymbol(MO.getIndex()),
+                              /* MO has no offset field */0);
+
+  case MachineOperand::MO_ConstantPoolIndex:
+    return lowerSymbolOperand(MO, AsmPrinter.GetCPISymbol(MO.getIndex()),
+                              MO.getOffset());
+
+  case MachineOperand::MO_BlockAddress: {
+    const BlockAddress *BA = MO.getBlockAddress();
+    return lowerSymbolOperand(MO, AsmPrinter.GetBlockAddressSymbol(BA),
+                              MO.getOffset());
+  }
+  }
+}
+
+void RISCVMCInstLower::lower(const MachineInstr *MI, MCInst &OutMI) const {
+  unsigned Opcode = MI->getOpcode();
+  // When emitting binary code, start with the shortest form of an instruction
+  // and then relax it where necessary.
+  if (!AsmPrinter.OutStreamer->hasRawTextSupport())
+    Opcode = getShortenedInstr(Opcode);
+  OutMI.setOpcode(Opcode);
+  for (unsigned I = 0, E = MI->getNumOperands(); I != E; ++I) {
+    const MachineOperand &MO = MI->getOperand(I);
+    MCOperand MCOp = lowerOperand(MO);
+    if (MCOp.isValid())
+      OutMI.addOperand(MCOp);
+  }
+}
diff --git a/lib/Target/RISCV/RISCVMCInstLower.h b/lib/Target/RISCV/RISCVMCInstLower.h
new file mode 100644
index 0000000..af911d6
--- /dev/null
+++ b/lib/Target/RISCV/RISCVMCInstLower.h
@@ -0,0 +1,44 @@
+//===-- RISCVMCInstLower.h - Lower MachineInstr to MCInst ------*- C++ -*--===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVMCINSTLOWER_H
+#define LLVM_LIB_TARGET_RISCV_RISCVMCINSTLOWER_H
+
+#include "llvm/Support/DataTypes.h"
+#include "llvm/Support/Compiler.h"
+
+namespace llvm {
+class MCContext;
+class MCInst;
+class MCOperand;
+class MCSymbol;
+class MachineInstr;
+class MachineOperand;
+class RISCVAsmPrinter;
+
+class LLVM_LIBRARY_VISIBILITY RISCVMCInstLower {
+  MCContext &Ctx;
+  RISCVAsmPrinter &AsmPrinter;
+
+public:
+  RISCVMCInstLower(MCContext &ctx, RISCVAsmPrinter &asmPrinter);
+
+  // Lower MachineInstr MI to MCInst OutMI.
+  void lower(const MachineInstr *MI, MCInst &OutMI) const;
+
+  // Return an MCOperand for MO.  Return an empty operand if MO is implicit.
+  MCOperand lowerOperand(const MachineOperand& MO) const;
+
+  // Return an MCOperand for MO, given that it equals Symbol + Offset.
+  MCOperand lowerSymbolOperand(const MachineOperand &MO,
+                               const MCSymbol *Symbol, int64_t Offset) const;
+};
+} // end namespace llvm
+
+#endif
diff --git a/lib/Target/RISCV/RISCVMachineFunctionInfo.cpp b/lib/Target/RISCV/RISCVMachineFunctionInfo.cpp
new file mode 100644
index 0000000..5080bec
--- /dev/null
+++ b/lib/Target/RISCV/RISCVMachineFunctionInfo.cpp
@@ -0,0 +1,33 @@
+//===-- RISCVMachineFunctionInfo.cpp - Private data used for RISCV --------===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVMachineFunctionInfo.h"
+//#include "MCTargetDesc/RISCVBaseInfo.h"
+#include "RISCVInstrInfo.h"
+#include "RISCVSubtarget.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/IR/Function.h"
+
+using namespace llvm;
+
+void RISCVFunctionInfo::createEhDataRegsFI() {
+  //TODO: why is this a magic number
+  for (int I = 0; I < 2; ++I) {
+    const RISCVSubtarget *ST = &MF.getSubtarget<RISCVSubtarget>();
+    const TargetRegisterClass *RC = ST->isRV64() ? &RISCV::GR64BitRegClass : &RISCV::GR32BitRegClass;
+
+    EhDataRegFI[I] = MF.getFrameInfo()->CreateStackObject(RC->getSize(),
+        RC->getAlignment(), false);
+  }
+}
+
+bool RISCVFunctionInfo::isEhDataRegFI(int FI) const {
+  return CallsEhReturn && (FI == EhDataRegFI[0] || FI == EhDataRegFI[1]);
+}
diff --git a/lib/Target/RISCV/RISCVMachineFunctionInfo.h b/lib/Target/RISCV/RISCVMachineFunctionInfo.h
new file mode 100644
index 0000000..dba1c4a
--- /dev/null
+++ b/lib/Target/RISCV/RISCVMachineFunctionInfo.h
@@ -0,0 +1,103 @@
+//===- RISCVMachineFuctionInfo.h - RISCV machine function info --*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVMACHINEFUNCTIONINFO_H
+#define LLVM_LIB_TARGET_RISCV_RISCVMACHINEFUNCTIONINFO_H
+
+#include "llvm/CodeGen/MachineFrameInfo.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/Target/TargetMachine.h"
+
+namespace llvm {
+
+class RISCVFunctionInfo : public MachineFunctionInfo {
+  MachineFunction& MF;
+
+  unsigned SavedGPRFrameSize;
+  unsigned LowSavedGPR;
+  unsigned HighSavedGPR;
+  unsigned VarArgsFirstGPR;
+  unsigned VarArgsFirstFPR;
+  unsigned VarArgsFrameIndex;
+  unsigned RegSaveFrameIndex;
+  bool ManipulatesSP;
+
+  bool HasByvalArg;
+
+  unsigned IncomingArgSize;
+  
+  bool CallsEhReturn;
+  // Frame objects for spilling eh data registers.
+  int EhDataRegFI[2];
+
+public:
+  explicit RISCVFunctionInfo(MachineFunction &MF)
+    : MF(MF), SavedGPRFrameSize(0), LowSavedGPR(0), HighSavedGPR(0), VarArgsFirstGPR(0),
+      VarArgsFirstFPR(0), VarArgsFrameIndex(0), RegSaveFrameIndex(0),
+      ManipulatesSP(false), CallsEhReturn(false) {}
+
+  // Get and set the number of bytes allocated by generic code to store
+  // call-saved GPRs.
+  unsigned getSavedGPRFrameSize() const { return SavedGPRFrameSize; }
+  void setSavedGPRFrameSize(unsigned bytes) { SavedGPRFrameSize = bytes; }
+
+  // Get and set the first call-saved GPR that should be saved and restored
+  // by this function.  This is 0 if no GPRs need to be saved or restored.
+  unsigned getLowSavedGPR() const { return LowSavedGPR; }
+  void setLowSavedGPR(unsigned Reg) { LowSavedGPR = Reg; }
+
+  // Get and set the last call-saved GPR that should be saved and restored
+  // by this function.
+  unsigned getHighSavedGPR() const { return HighSavedGPR; }
+  void setHighSavedGPR(unsigned Reg) { HighSavedGPR = Reg; }
+
+  // Get and set the number of fixed (as opposed to variable) arguments
+  // that are passed in GPRs to this function.
+  unsigned getVarArgsFirstGPR() const { return VarArgsFirstGPR; }
+  void setVarArgsFirstGPR(unsigned GPR) { VarArgsFirstGPR = GPR; }
+
+  // Likewise FPRs.
+  unsigned getVarArgsFirstFPR() const { return VarArgsFirstFPR; }
+  void setVarArgsFirstFPR(unsigned FPR) { VarArgsFirstFPR = FPR; }
+
+  // Get and set the frame index of the first stack vararg.
+  unsigned getVarArgsFrameIndex() const { return VarArgsFrameIndex; }
+  void setVarArgsFrameIndex(unsigned FI) { VarArgsFrameIndex = FI; }
+
+  // Get and set the frame index of the register save area
+  // (i.e. the incoming stack pointer).
+  unsigned getRegSaveFrameIndex() const { return RegSaveFrameIndex; }
+  void setRegSaveFrameIndex(unsigned FI) { RegSaveFrameIndex = FI; }
+
+  // Get and set whether the function directly manipulates the stack pointer,
+  // e.g. through STACKSAVE or STACKRESTORE.
+  bool getManipulatesSP() const { return ManipulatesSP; }
+  void setManipulatesSP(bool MSP) { ManipulatesSP = MSP; }
+
+  bool hasByvalArg() const { return HasByvalArg; }
+  void setFormalArgInfo(unsigned Size, bool HasByval) {
+    IncomingArgSize = Size;
+    HasByvalArg = HasByval;
+  }
+
+  unsigned getIncomingArgSize() const { return IncomingArgSize; };
+  void setIncomingArgSize(unsigned Size) { IncomingArgSize = Size; };
+
+  // Get and set whether the function calls EH_RETURN
+  bool getCallsEhReturn() const { return CallsEhReturn; }
+  void setCallsEhReturn(bool ceret) { CallsEhReturn = ceret; }
+
+  void createEhDataRegsFI();
+  int getEhDataRegFI(unsigned Reg) const { return EhDataRegFI[Reg]; };
+  bool isEhDataRegFI(int FI) const;
+};
+
+} // end llvm namespace
+
+#endif
diff --git a/lib/Target/RISCV/RISCVOperands.td b/lib/Target/RISCV/RISCVOperands.td
new file mode 100644
index 0000000..02447fa
--- /dev/null
+++ b/lib/Target/RISCV/RISCVOperands.td
@@ -0,0 +1,271 @@
+//===-- RISCVOperands.td - RISCV instruction operands --------*- tblgen-*--===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// Class definitions
+//===----------------------------------------------------------------------===//
+
+class ImmediateAsmOperand<string name>
+  : AsmOperandClass {
+  let Name = name;
+  let RenderMethod = "addImmOperands";
+}
+
+// Constructs both a DAG pattern and instruction operand for an immediate
+// of type VT.  PRED returns true if a node is acceptable and XFORM returns
+// the operand value associated with the node.  ASMOP is the name of the
+// associated asm operand, and also forms the basis of the asm print method.
+class Immediate<ValueType vt, code pred, SDNodeXForm xform, string asmop>
+  : PatLeaf<(vt imm), pred, xform>, Operand<vt> {
+  let PrintMethod = "print"##asmop##"Operand";
+  let ParserMatchClass = !cast<AsmOperandClass>(asmop);
+}
+
+// Constructs both a DAG pattern and instruction operand for a PC-relative
+// address with address size VT.  SELF is the name of the operand.
+class PCRelAddress<ValueType vt, string self>
+  : ComplexPattern<vt, 1, "selectPCRelAddress", [r_pcrel_wrapper]>,
+    Operand<vt> {
+  let MIOperandInfo = (ops !cast<Operand>(self));
+}
+
+class PCRelAddressNoWrap<ValueType vt, string self>
+  : ComplexPattern<vt, 1, "selectPCRelAddress">,
+    Operand<vt> {
+  let MIOperandInfo = (ops !cast<Operand>(self));
+}
+
+// Constructs an AsmOperandClass for addressing mode FORMAT, treating the
+// registers as having BITSIZE bits and displacements as having DISPSIZE bits.
+class AddressAsmOperand<string format, string bitsize, string dispsize>
+  : AsmOperandClass {
+  let Name = format##bitsize##"Disp"##dispsize;
+  let ParserMethod = "parse"##format##bitsize;
+  let RenderMethod = "add"##format##"Operands";
+}
+
+//===----------------------------------------------------------------------===//
+// Extracting immediate operands from nodes
+// These all create MVT::i64 nodes to ensure the value is not sign-extended
+// when converted from an SDNode to a MachineOperand later on.
+//===----------------------------------------------------------------------===//
+
+// Truncate an immediate to a 32-bit signed quantity.
+def SIMM32 : SDNodeXForm<imm, [{
+  return CurDAG->getTargetConstant(int32_t(N->getZExtValue()), MVT::i32);
+}]>;
+
+// Truncate an immediate to a 32-bit unsigned quantity.
+def UIMM32 : SDNodeXForm<imm, [{
+  return CurDAG->getTargetConstant(uint32_t(N->getZExtValue()), MVT::i32);
+}]>;
+
+// Truncate an immediate to a 64-bit signed quantity.
+def SIMM64 : SDNodeXForm<imm, [{
+  return CurDAG->getTargetConstant(int64_t(N->getZExtValue()), MVT::i64);
+}]>;
+
+// Truncate an immediate to a 64-bit unsigned quantity.
+def UIMM64 : SDNodeXForm<imm, [{
+  return CurDAG->getTargetConstant(uint64_t(N->getZExtValue()), MVT::i64);
+}]>;
+
+//===----------------------------------------------------------------------===//
+// Immediate asm operands.
+//===----------------------------------------------------------------------===//
+
+def U4Imm  : ImmediateAsmOperand<"U4Imm">;
+def S12Imm : ImmediateAsmOperand<"S12Imm">;
+def U12Imm : ImmediateAsmOperand<"U12Imm">;
+def S20Imm : ImmediateAsmOperand<"S20Imm">;
+def U20Imm : ImmediateAsmOperand<"U20Imm">;
+def S32Imm : ImmediateAsmOperand<"S32Imm">;
+def U32Imm : ImmediateAsmOperand<"U32Imm">;
+def S64Imm : ImmediateAsmOperand<"S64Imm">;
+def U64Imm : ImmediateAsmOperand<"U64Imm">;
+
+//===----------------------------------------------------------------------===//
+// i32 immediates
+//===----------------------------------------------------------------------===//
+
+//sign-extended 12 bit immediate
+def imm32sx12 : Immediate<i32, [{
+  return isInt<12>(N->getSExtValue());
+}], NOOP_SDNodeXForm, "S12Imm">;
+def imm32sxu12 : Immediate<i32, [{
+  return isUInt<12>(N->getSExtValue());
+}], NOOP_SDNodeXForm, "U12Imm">;
+//zero-extended 12 bit immediate
+def imm32zx12 : Immediate<i32, [{
+  return isUInt<12>(N->getZExtValue());
+}], NOOP_SDNodeXForm, "U12Imm">;
+//sign-extended 20 bit immediate
+def imm32sx20 : Immediate<i32, [{
+  return isInt<20>(N->getSExtValue());
+}], NOOP_SDNodeXForm, "S20Imm">;
+def imm32sxu20 : Immediate<i32, [{
+  return isUInt<20>(N->getSExtValue());
+}], NOOP_SDNodeXForm, "U20Imm">;
+//zero-extended 20 bit immediate
+def imm32zx20 : Immediate<i32, [{
+  return isUInt<20>(N->getZExtValue());
+}], NOOP_SDNodeXForm, "U20Imm">;
+
+def simm32 : Immediate<i32, [{}], SIMM32, "S32Imm">;
+def uimm32 : Immediate<i32, [{}], UIMM32, "U32Imm">;
+
+def imm32 : ImmLeaf<i32, [{}]>, Operand<i32>;
+
+//===----------------------------------------------------------------------===//
+// 64-bit immediates
+//===----------------------------------------------------------------------===//
+
+//sign-extended 12 bit immediate
+def imm64sx12 : Immediate<i64, [{
+  return isInt<12>(N->getSExtValue());
+}], NOOP_SDNodeXForm, "S12Imm">;
+def imm64sxu12 : Immediate<i64, [{
+  return isUInt<12>(N->getSExtValue());
+}], NOOP_SDNodeXForm, "U12Imm">;
+//zero-extended 12 bit immediate
+def imm64zx12 : Immediate<i64, [{
+  return isUInt<12>(N->getZExtValue());
+}], NOOP_SDNodeXForm, "U12Imm">;
+//sign-extended 20 bit immediate
+def imm64sx20 : Immediate<i64, [{
+  return isInt<20>(N->getSExtValue());
+}], NOOP_SDNodeXForm, "S20Imm">;
+def imm64sxu20 : Immediate<i64, [{
+  return isUInt<20>(N->getSExtValue());
+}], NOOP_SDNodeXForm, "U20Imm">;
+//zero-extended 20 bit immediate
+def imm64zx20 : Immediate<i64, [{
+  return isUInt<20>(N->getZExtValue());
+}], NOOP_SDNodeXForm, "U20Imm">;
+//sign-extended 32bit immediate for LUI/ADDI max load size
+def imm64sxu32 : Immediate<i64, [{
+  return isUInt<32>(N->getSExtValue());
+}], NOOP_SDNodeXForm, "U32Imm">;
+
+def simm64 : Immediate<i64, [{}], SIMM64, "S64Imm">;
+def uimm64 : Immediate<i64, [{}], UIMM64, "U64Imm">;
+
+def imm64 : ImmLeaf<i64, [{}]>, Operand<i64>;
+
+//===----------------------------------------------------------------------===//
+// Fence immediates
+//===----------------------------------------------------------------------===//
+
+def fenceImm : Immediate<i32, [{
+  return isUInt<4>(N->getZExtValue());
+}], NOOP_SDNodeXForm, "U4Imm">;
+def fenceImm64 : Immediate<i64, [{
+  return isUInt<4>(N->getZExtValue());
+}], NOOP_SDNodeXForm, "U4Imm">;
+
+//===----------------------------------------------------------------------===//
+// Floating-point immediates
+//===----------------------------------------------------------------------===//
+
+// Floating-point zero.
+def fpimm0 : PatLeaf<(fpimm), [{ return N->isExactlyValue(+0.0); }]>;
+
+//===----------------------------------------------------------------------===//
+// Memory address operands
+//===----------------------------------------------------------------------===//
+
+def mem : Operand<i32> {
+  let MIOperandInfo = (ops imm32sx12, GR32);
+  //let EncoderMethod = "getMemRegEncoding";
+  let OperandType = "OPERAND_MEMORY";
+  let PrintMethod = "printMemOperand";
+}
+
+def mem64 : Operand<i64> {
+  let MIOperandInfo = (ops imm64sx12, GR64);
+  //let EncoderMethod = "getMemRegEncoding";
+  let OperandType = "OPERAND_MEMORY";
+  let PrintMethod = "printMemOperand";
+}
+
+def jalrmem : Operand<i32> {
+  let MIOperandInfo = (ops imm32sx12, GR32);
+  //let EncoderMethod = "getMemRegEncoding";
+  let OperandType = "OPERAND_MEMORY";
+  let PrintMethod = "printJALRMemOperand";
+}
+
+def jalrmem64 : Operand<i64> {
+  let MIOperandInfo = (ops imm64sx12, GR64);
+  //let EncoderMethod = "getMemRegEncoding";
+  let OperandType = "OPERAND_MEMORY";
+  let PrintMethod = "printJALRMemOperand";
+}
+
+def memreg : Operand<i32> {
+  let MIOperandInfo = (ops GR32);
+  //let EncoderMethod = "getMemRegEncoding";
+  let OperandType = "OPERAND_MEMORY";
+  let PrintMethod = "printMemRegOperand";
+}
+
+def memreg64 : Operand<i64> {
+  let MIOperandInfo = (ops GR64);
+  //let EncoderMethod = "getMemRegEncoding";
+  let OperandType = "OPERAND_MEMORY";
+  let PrintMethod = "printMemRegOperand";
+}
+
+
+def regaddr : ComplexPattern<iPTR, 1, "selectRegAddr">;
+def addr    : ComplexPattern<iPTR, 2, "selectMemRegAddr">;
+def raaddr  : ComplexPattern<i64 , 2, "selectMemRegAddr", [add]>;
+
+//===----------------------------------------------------------------------===//
+// Symbolic address operands
+//===----------------------------------------------------------------------===//
+
+def jumptarget : Operand<OtherVT> {
+  let EncoderMethod = "getJumpTargetEncoding";
+}
+
+def brtarget : Operand<OtherVT> {
+  let PrintMethod = "printBranchTarget";
+  let EncoderMethod = "getBranchTargetEncoding";
+}
+
+def pcimm : PCRelAddress<i32, "pcimm"> {
+  let EncoderMethod = "getPCImmEncoding";
+}
+
+def pcimm64 : PCRelAddress<i64, "pcimm64"> {
+  let EncoderMethod = "getPCImm64Encoding";
+}
+
+def pcrel32call : PCRelAddress<i32, "pcrel32call"> {
+  let PrintMethod = "printCallOperand";
+  let EncoderMethod = "getCallEncoding";
+}
+
+def pcrel64call : PCRelAddressNoWrap<i64, "pcrel64call"> {
+  let PrintMethod = "printCallOperand";
+  let EncoderMethod = "getCallEncoding";
+}
+
+//===----------------------------------------------------------------------===//
+// Addressing modes
+//===----------------------------------------------------------------------===//
+
+// 12-bit displacement operands.
+def disp12imm32 : Operand<i32>;
+def disp12imm64 : Operand<i64>;
+
+// 20-bit displacement operands.
+def disp20imm32 : Operand<i32>;
+def disp20imm64 : Operand<i64>;
diff --git a/lib/Target/RISCV/RISCVOperators.td b/lib/Target/RISCV/RISCVOperators.td
new file mode 100644
index 0000000..2d07dd4
--- /dev/null
+++ b/lib/Target/RISCV/RISCVOperators.td
@@ -0,0 +1,117 @@
+//===-- RISCVOperators.td - RISCV-specific operators ----------*- tblgen-*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// Type profiles
+//===----------------------------------------------------------------------===//
+def SDT_CallSeqStart        : SDCallSeqStart<[SDTCisVT<0, i64>]>;
+def SDT_CallSeqEnd          : SDCallSeqEnd<[SDTCisVT<0, i64>,
+                                            SDTCisVT<1, i64>]>;
+def SDT_RCall               : SDTypeProfile<0, -1, [SDTCisPtrTy<0>]>;
+def SDT_RJAL                : SDTypeProfile<1, -1, [SDTCisSameAs<0,1>,SDTCisPtrTy<1>]>;
+def SDT_RSelectCC           : SDTypeProfile<1, 3,
+                                            [SDTCisSameAs<0, 1>,
+                                             SDTCisSameAs<1, 2>,
+                                             SDTCisVT<3, i32>]>;
+def SDT_RWrapPtr            : SDTypeProfile<1, 1,
+                                            [SDTCisSameAs<0, 1>,
+                                             SDTCisPtrTy<0>]>;
+def SDT_RFence              : SDTypeProfile<0, 2,[SDTCisVT<0, i32>,
+                                                  SDTCisVT<1, i32>]>;
+def SDT_RFence64            : SDTypeProfile<0, 2,[SDTCisVT<0, i64>,
+                                                  SDTCisVT<1, i64>]>;
+
+//===----------------------------------------------------------------------===//
+// Node definitions
+//===----------------------------------------------------------------------===//
+
+// These are target-independent nodes, but have target-specific formats.
+def callseq_start       : SDNode<"ISD::CALLSEQ_START", SDT_CallSeqStart,
+                                 [SDNPHasChain, SDNPSideEffect, SDNPOutGlue]>;
+def callseq_end         : SDNode<"ISD::CALLSEQ_END",   SDT_CallSeqEnd,
+                                 [SDNPHasChain, SDNPSideEffect, SDNPOptInGlue,
+                                  SDNPOutGlue]>;
+
+// Nodes for RISCVISD::*.  See RISCVISelLowering.h for more details.
+def r_retflag           : SDNode<"RISCVISD::RET_FLAG", SDTNone,
+                                 [SDNPHasChain, SDNPOptInGlue, SDNPVariadic]>;
+def r_call              : SDNode<"RISCVISD::CALL", SDT_RCall,
+                                 [SDNPHasChain, SDNPOutGlue, SDNPOptInGlue,
+                                  SDNPVariadic]>;
+def r_jal               : SDNode<"RISCVISD::JAL", SDT_RJAL,
+                                 [SDNPHasChain, SDNPOutGlue, SDNPOptInGlue,
+                                  SDNPVariadic]>;
+def r_pcrel_wrapper     : SDNode<"RISCVISD::PCREL_WRAPPER", SDT_RWrapPtr, []>;
+def r_select_cc         : SDNode<"RISCVISD::SELECT_CC", SDT_RSelectCC,
+    		                 [SDNPInGlue]>;
+
+def r_fence             : SDNode<"RISCVISD::FENCE", SDT_RFence, [SDNPHasChain, SDNPSideEffect]>;
+def r_fence64           : SDNode<"RISCVISD::FENCE", SDT_RFence64, [SDNPHasChain, SDNPSideEffect]>;
+
+//global addr
+def RISCVHi    : SDNode<"RISCVISD::Hi", SDTIntUnaryOp>;
+def RISCVLo    : SDNode<"RISCVISD::Lo", SDTIntUnaryOp>;
+
+// TprelHi and TprelLo nodes are used to handle Local Exec TLS
+def RISCVTprelHi    : SDNode<"RISCVISD::TprelHi", SDTIntUnaryOp>;
+def RISCVTprelLo    : SDNode<"RISCVISD::TprelLo", SDTIntUnaryOp>;
+
+//===----------------------------------------------------------------------===//
+// Pattern fragments
+//===----------------------------------------------------------------------===//
+
+// Register sign-extend operations.  Sub-32-bit values are represented as i32s.
+def sext8  : PatFrag<(ops node:$src), (sext_inreg node:$src, i8)>;
+def sext16 : PatFrag<(ops node:$src), (sext_inreg node:$src, i16)>;
+//def sext32 : PatFrag<(ops node:$src), (sext (i32 node:$src))>;
+
+// Register zero-extend operations.  Sub-32-bit values are represented as i32s.
+def zext8  : PatFrag<(ops node:$src), (and node:$src, 0xff)>;
+def zext16 : PatFrag<(ops node:$src), (and node:$src, 0xffff)>;
+//def zext32 : PatFrag<(ops node:$src), (zext (i32 node:$src))>;
+
+// Typed floating-point loads.
+def loadf32 : PatFrag<(ops node:$src), (f32 (load node:$src))>;
+def loadf64 : PatFrag<(ops node:$src), (f64 (load node:$src))>;
+
+// Aligned loads.
+class AlignedLoad<SDPatternOperator load>
+  : PatFrag<(ops node:$addr), (load node:$addr), [{
+  LoadSDNode *Load = cast<LoadSDNode>(N);
+  return Load->getAlignment() >= Load->getMemoryVT().getStoreSize();
+}]>;
+def aligned_load        : AlignedLoad<load>;
+def aligned_sextloadi16 : AlignedLoad<sextloadi16>;
+def aligned_sextloadi32 : AlignedLoad<sextloadi32>;
+def aligned_zextloadi16 : AlignedLoad<zextloadi16>;
+def aligned_zextloadi32 : AlignedLoad<zextloadi32>;
+
+// Aligned stores.
+class AlignedStore<SDPatternOperator store>
+  : PatFrag<(ops node:$src, node:$addr), (store node:$src, node:$addr), [{
+  StoreSDNode *Store = cast<StoreSDNode>(N);
+  return Store->getAlignment() >= Store->getMemoryVT().getStoreSize();
+}]>;
+def aligned_store         : AlignedStore<store>;
+def aligned_truncstorei16 : AlignedStore<truncstorei16>;
+def aligned_truncstorei32 : AlignedStore<truncstorei32>;
+
+// Floating-point negative absolute.
+def fnabs : PatFrag<(ops node:$ptr), (fneg (fabs node:$ptr))>;
+
+// Create a unary operator that loads from memory and then performs
+// the given operation on it.
+class loadu<SDPatternOperator operator>
+  : PatFrag<(ops node:$addr), (operator (load node:$addr))>;
+
+// Create a store operator that performs the given unary operation
+// on the value before storing it.
+class storeu<SDPatternOperator operator>
+  : PatFrag<(ops node:$value, node:$addr),
+            (store (operator node:$value), node:$addr)>;
diff --git a/lib/Target/RISCV/RISCVRegisterInfo.cpp b/lib/Target/RISCV/RISCVRegisterInfo.cpp
new file mode 100644
index 0000000..60d151f
--- /dev/null
+++ b/lib/Target/RISCV/RISCVRegisterInfo.cpp
@@ -0,0 +1,205 @@
+//===-- RISCVRegisterInfo.cpp - RISCV register information ------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVRegisterInfo.h"
+#include "RISCVSubtarget.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/ErrorHandling.h"
+#include "llvm/Support/raw_ostream.h"
+
+#define DEBUG_TYPE "riscv-reg-info"
+
+#define GET_REGINFO_TARGET_DESC
+#include "RISCVGenRegisterInfo.inc"
+
+using namespace llvm;
+
+RISCVRegisterInfo::RISCVRegisterInfo(const RISCVSubtarget &STI)
+    : RISCVGenRegisterInfo(RISCV::ra), Subtarget(STI) {}
+
+const uint16_t*
+RISCVRegisterInfo::getCalleeSavedRegs(const MachineFunction *MF) const {
+  if(Subtarget.isRV64())
+    if(Subtarget.hasD())
+      return CSR_RV64D_SaveList;
+    else if(Subtarget.hasF())
+      return CSR_RV64F_SaveList;
+    else
+      return CSR_RV64_SaveList;
+  else
+    if(Subtarget.hasD())
+      return CSR_RV32D_SaveList;
+    else if(Subtarget.hasF())
+      return CSR_RV32F_SaveList;
+    else
+      return CSR_RV32_SaveList;
+}
+
+const uint32_t*
+RISCVRegisterInfo::getCallPreservedMask(const MachineFunction &MF,
+    CallingConv::ID) const {
+  if(Subtarget.isRV64())
+    if(Subtarget.hasD())
+      return CSR_RV64D_RegMask;
+    else if(Subtarget.hasF())
+      return CSR_RV64F_RegMask;
+    else
+      return CSR_RV64_RegMask;
+  else
+    if(Subtarget.hasD())
+      return CSR_RV32D_RegMask;
+    else if(Subtarget.hasF())
+      return CSR_RV32F_RegMask;
+    else
+      return CSR_RV32_RegMask;
+
+}
+
+BitVector
+RISCVRegisterInfo::getReservedRegs(const MachineFunction &MF) const {
+  BitVector Reserved(getNumRegs());
+  const TargetFrameLowering *TFI = MF.getSubtarget().getFrameLowering();
+
+  // zero is reserved so llvm doesn't store things there
+  Reserved.set(RISCV::zero);
+  Reserved.set(RISCV::zero_64);
+
+  if (TFI->hasFP(MF)) {
+    // fp is the frame pointer.  Reserve all aliases.
+    Reserved.set(RISCV::fp);
+    Reserved.set(RISCV::s0);
+    Reserved.set(RISCV::fp_64);
+    Reserved.set(RISCV::s0_64);
+  }
+
+  // sp is the stack pointer.  Reserve all aliases.
+  Reserved.set(RISCV::sp);
+  Reserved.set(RISCV::sp_64);
+  // tp is the thread pointer.  Reserve all aliases.
+  Reserved.set(RISCV::tp);
+  Reserved.set(RISCV::tp_64);
+  // gp shouldn't be used eitehr
+  Reserved.set(RISCV::gp);
+  Reserved.set(RISCV::gp_64);
+  return Reserved;
+}
+
+void RISCVRegisterInfo::eliminateFI(MachineBasicBlock::iterator II,
+                                     unsigned OpNo, int FrameIndex,
+                                     uint64_t StackSize,
+                                     int64_t SPOffset) const {
+  MachineInstr &MI = *II;
+  MachineFunction &MF = *MI.getParent()->getParent();
+  MachineFrameInfo *MFI = MF.getFrameInfo();
+  RISCVFunctionInfo *RISCVFI = MF.getInfo<RISCVFunctionInfo>();
+
+  const std::vector<CalleeSavedInfo> &CSI = MFI->getCalleeSavedInfo();
+  int MinCSFI = 0;
+  int MaxCSFI = -1;
+
+  if (CSI.size()) {
+    MinCSFI = CSI[0].getFrameIdx();
+    MaxCSFI = CSI[CSI.size() - 1].getFrameIdx();
+  }
+
+  bool EhDataRegFI = RISCVFI->isEhDataRegFI(FrameIndex);
+
+  // The following stack frame objects are always referenced relative to $sp:
+  //  1. Outgoing arguments.
+  //  2. Pointer to dynamically allocated stack space.
+  //  3. Locations for callee-saved registers.
+  //  4. Locations for eh data registers.
+  // Everything else is referenced relative to whatever register
+  // getFrameRegister() returns.
+  unsigned FrameReg;
+
+  if ((FrameIndex >= MinCSFI && FrameIndex <= MaxCSFI) || EhDataRegFI)
+    FrameReg = Subtarget.isRV64() ? RISCV::sp_64 : RISCV::sp;
+  else
+    FrameReg = getFrameRegister(MF);
+
+  // Calculate final offset.
+  // - There is no need to change the offset if the frame object is one of the
+  //   following: an outgoing argument, pointer to a dynamically allocated
+  //   stack space or a $gp restore location,
+  // - If the frame object is any of the following, its offset must be adjusted
+  //   by adding the size of the stack:
+  //   incoming argument, callee-saved register location or local variable.
+  bool IsKill = false;
+  int64_t Offset;
+
+  Offset = SPOffset + (int64_t)StackSize;
+  // loads and stores have the immediate before the FI
+  // FIXME: this is a bit hacky
+  if(MI.mayLoadOrStore())
+    Offset += MI.getOperand(OpNo - 1).getImm();
+  else
+    Offset += MI.getOperand(OpNo + 1).getImm();
+
+  DEBUG(errs() << "Offset     : " << Offset << "\n" << "<--------->\n");
+
+  // If MI is not a debug value, make sure Offset fits in the 16-bit immediate
+  // field.
+  if (!MI.isDebugValue() && !isInt<12>(Offset)) {
+    MachineBasicBlock &MBB = *MI.getParent();
+    DebugLoc DL = II->getDebugLoc();
+    unsigned ADD = Subtarget.isRV64() ? RISCV::ADD64 : RISCV::ADD;
+    unsigned Reg;
+    const RISCVInstrInfo &TII =
+        *static_cast<const RISCVInstrInfo *>(
+            MBB.getParent()->getSubtarget().getInstrInfo());
+
+    TII.loadImmediate(MBB, II, &Reg, Offset);
+    BuildMI(MBB, II, DL, TII.get(ADD), Reg).addReg(FrameReg)
+      .addReg(Reg, RegState::Kill);
+
+    FrameReg = Reg;
+    Offset = SignExtend64<12>(0);
+    IsKill = true;
+  }
+
+  MI.getOperand(OpNo).ChangeToRegister(FrameReg, false, false, IsKill);
+  // loads and stores have the immediate before the FI
+  // FIXME: this is a bit hacky
+  if(MI.mayLoadOrStore())
+    MI.getOperand(OpNo - 1).ChangeToImmediate(Offset);
+  else
+    MI.getOperand(OpNo + 1).ChangeToImmediate(Offset);
+}
+
+void
+RISCVRegisterInfo::eliminateFrameIndex(MachineBasicBlock::iterator II,
+                                         int SPAdj, unsigned FIOperandNum,
+                                         RegScavenger *RS) const {
+  MachineInstr &MI = *II;
+  MachineFunction &MF = *MI.getParent()->getParent();
+
+  DEBUG(errs() << "\nFunction : " << MF.getName() << "\n";
+        errs() << "<--------->\n" << MI);
+
+  int FrameIndex = MI.getOperand(FIOperandNum).getIndex();
+  uint64_t stackSize = MF.getFrameInfo()->getStackSize();
+  int64_t spOffset = MF.getFrameInfo()->getObjectOffset(FrameIndex);
+
+  DEBUG(errs() << "FrameIndex : " << FrameIndex << "\n"
+               << "spOffset   : " << spOffset << "\n"
+               << "stackSize  : " << stackSize << "\n");
+
+  eliminateFI(MI, FIOperandNum, FrameIndex, stackSize, spOffset);
+}
+
+unsigned
+RISCVRegisterInfo::getFrameRegister(const MachineFunction &MF) const {
+  const TargetFrameLowering *TFI = MF.getSubtarget().getFrameLowering();
+  return TFI->hasFP(MF) ? 
+      (Subtarget.isRV64() ? RISCV::fp_64 : RISCV::fp) : 
+      (Subtarget.isRV64() ? RISCV::sp_64 : RISCV::sp);
+}
diff --git a/lib/Target/RISCV/RISCVRegisterInfo.h b/lib/Target/RISCV/RISCVRegisterInfo.h
new file mode 100644
index 0000000..f7cc456
--- /dev/null
+++ b/lib/Target/RISCV/RISCVRegisterInfo.h
@@ -0,0 +1,56 @@
+//===-- RISCVRegisterInfo.h - RISCV register information --------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVREGISTERINFO_H
+#define LLVM_LIB_TARGET_RISCV_RISCVREGISTERINFO_H
+
+#include "RISCV.h"
+#include "llvm/Target/TargetRegisterInfo.h"
+#include "RISCVMachineFunctionInfo.h"
+
+#define GET_REGINFO_HEADER
+#include "RISCVGenRegisterInfo.inc"
+
+namespace llvm {
+
+class RISCVInstrInfo;
+class RISCVSubtarget;
+
+struct RISCVRegisterInfo : public RISCVGenRegisterInfo {
+public:
+  const RISCVSubtarget &Subtarget;
+  
+  RISCVRegisterInfo(const RISCVSubtarget &STI);
+
+  // Override TargetRegisterInfo.h.
+  bool requiresRegisterScavenging(const MachineFunction &MF) const override {
+    return true;
+  }
+  bool requiresFrameIndexScavenging(const MachineFunction &MF) const override {
+    return true;
+  }
+  const uint16_t *
+  getCalleeSavedRegs(const MachineFunction *MF = 0) const override;
+  const uint32_t *
+  getCallPreservedMask(const MachineFunction &MF, CallingConv::ID) const override;
+  BitVector getReservedRegs(const MachineFunction &MF) const override;
+  void eliminateFrameIndex(MachineBasicBlock::iterator MI, int SPAdj,
+                           unsigned FIOperandNum,
+                           RegScavenger *RS) const override;
+  unsigned getFrameRegister(const MachineFunction &MF) const override;
+
+private:
+  virtual void eliminateFI(MachineBasicBlock::iterator II, unsigned OpNo,
+                           int FrameIndex, uint64_t StackSize,
+                           int64_t SPOffset) const;
+};
+
+} // end namespace llvm
+
+#endif
diff --git a/lib/Target/RISCV/RISCVRegisterInfo.td b/lib/Target/RISCV/RISCVRegisterInfo.td
new file mode 100644
index 0000000..72160e2
--- /dev/null
+++ b/lib/Target/RISCV/RISCVRegisterInfo.td
@@ -0,0 +1,492 @@
+//===- RISCVRegisterInfo.td - RISCV register definitions ---*- tablegen -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+//===----------------------------------------------------------------------===//
+// Class definitions.
+//===----------------------------------------------------------------------===//
+
+class RISCVReg<string n> : Register<n> {
+  let Namespace = "RISCV";
+}
+
+class RISCVRegWithSubRegs<string n, list<Register> subregs>
+  : RegisterWithSubRegs<n, subregs> {
+  let Namespace = "RISCV";
+}
+
+//Subreg defs
+let Namespace = "RISCV" in {
+  def sub_32       : SubRegIndex<32>;
+  def sub_32even   : SubRegIndex<32>;
+  def sub_32odd    : SubRegIndex<32>;
+  def sub_64even   : SubRegIndex<64>;
+  def sub_64odd    : SubRegIndex<64>;
+  def sub_fp32even : SubRegIndex<32>;
+  def sub_fp32odd  : SubRegIndex<32>;
+  def sub_fp64even : SubRegIndex<64>;
+  def sub_fp64odd  : SubRegIndex<64>;
+}
+
+
+// Define a register class that contains values of type TYPE and an
+// associated operand called NAME.  SIZE is the size and alignment
+// of the registers and REGLIST is the list of individual registers.
+multiclass RISCVRegClass<string name, ValueType type, int size, dag regList, bit allocatable> {
+  def AsmOperand : AsmOperandClass {
+    let Name = name;
+    let ParserMethod = "parse"##name;
+    let RenderMethod = "addRegOperands";
+  }
+  def Bit : RegisterClass<"RISCV", [type], size, regList> {
+    let Size = size;
+    let isAllocatable = allocatable;
+  }
+  def "" : RegisterOperand<!cast<RegisterClass>(name##"Bit")> {
+    let ParserMatchClass = !cast<AsmOperandClass>(name##"AsmOperand");
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// General-purpose registers
+//===----------------------------------------------------------------------===//
+
+//RV32 general purpose regs
+class GPR32<bits<16> num, string n> : RISCVReg<n> {
+  let HWEncoding = num;
+}
+
+// General-purpose registers
+//ABI Names
+def zero : GPR32<0, "x0">, DwarfRegNum<[0]>;
+def ra   : GPR32<1, "x1">, DwarfRegNum<[1]>;
+def sp   : GPR32<2, "x2">, DwarfRegNum<[2]>;
+def gp   : GPR32<3, "x3">, DwarfRegNum<[3]>;
+def tp   : GPR32<4, "x4">, DwarfRegNum<[4]>;
+def t0   : GPR32<5, "x5">, DwarfRegNum<[5]>;
+def t1   : GPR32<6, "x6">, DwarfRegNum<[6]>;
+def t2   : GPR32<7, "x7">, DwarfRegNum<[7]>;
+def fp   : GPR32<8, "x8">, DwarfRegNum<[8]>;
+def s0   : GPR32<8, "x8">, DwarfRegNum<[8]>{
+  let Aliases = [fp];
+}
+def s1   : GPR32<9, "x9">, DwarfRegNum<[9]>;
+//return values
+def a0   : GPR32<10,"x10">, DwarfRegNum<[10]>;
+def a1   : GPR32<11,"x11">, DwarfRegNum<[11]>;
+//function arguments
+def a2   : GPR32<12,"x12">, DwarfRegNum<[12]>;
+def a3   : GPR32<13,"x13">, DwarfRegNum<[13]>;
+def a4   : GPR32<14,"x14">, DwarfRegNum<[14]>;
+def a5   : GPR32<15,"x15">, DwarfRegNum<[15]>;
+def a6   : GPR32<16,"x16">, DwarfRegNum<[16]>;
+def a7   : GPR32<17,"x17">, DwarfRegNum<[17]>;
+//saved registers
+def s2   : GPR32<18,"x18">, DwarfRegNum<[18]>;
+def s3   : GPR32<19,"x19">, DwarfRegNum<[19]>;
+def s4   : GPR32<20,"x20">, DwarfRegNum<[20]>;
+def s5   : GPR32<21,"x21">, DwarfRegNum<[21]>;
+def s6   : GPR32<22,"x22">, DwarfRegNum<[22]>;
+def s7   : GPR32<23,"x23">, DwarfRegNum<[23]>;
+def s8   : GPR32<24,"x24">, DwarfRegNum<[24]>;
+def s9   : GPR32<25,"x25">, DwarfRegNum<[25]>;
+def s10  : GPR32<26,"x26">, DwarfRegNum<[26]>;
+def s11  : GPR32<27,"x27">, DwarfRegNum<[27]>;
+//temp registers
+def t3   : GPR32<28,"x28">, DwarfRegNum<[28]>;
+def t4   : GPR32<29,"x29">, DwarfRegNum<[29]>;
+def t5   : GPR32<30,"x30">, DwarfRegNum<[30]>;
+def t6   : GPR32<31,"x31">, DwarfRegNum<[31]>;
+
+//All regs are 32bit
+defm GR32 : RISCVRegClass<"GR32", i32, 32, (add
+  zero, ra, sp, gp, tp,
+  t0, t1, t2,
+  fp, s0, s1,
+  a0, a1, a2, a3, a4, a5, a6, a7, 
+  s2, s3, s4, s5, s6, s7, s8, s9, s10, s11,
+  t3, t4, t5, t6), 1>;
+
+//Pairs of int arg regs can be used to store double-pointer word args
+class PairGPR64<bits<16> num, string n, list<Register> subregs>
+  : RISCVRegWithSubRegs<n, subregs> {
+  let HWEncoding = num;
+  let SubRegIndices = [sub_32even, sub_32odd];
+  let CoveredBySubRegs = 1;
+}
+
+def a0_p64 : PairGPR64<0, "x10", [a0, a1]>;
+def a1_p64 : PairGPR64<1, "x12", [a2, a3]>;
+def a2_p64 : PairGPR64<2, "x14", [a4, a5]>;
+def a3_p64 : PairGPR64<3, "x16", [a6, a7]>;
+
+defm PairGR64 : RISCVRegClass<"PairGR64", i64, 64, (add
+  a0_p64, a1_p64, a2_p64, a3_p64), 1>;
+
+//RV64 general purpose regs
+class GPR64<bits<16> num, string n, list<Register> subregs> : RISCVRegWithSubRegs<n, subregs> {
+  let HWEncoding = num;
+  let SubRegIndices = [sub_32];
+}
+
+// General-purpose registers
+//ABI Names
+def zero_64 : GPR64<0, "x0",[zero]>, DwarfRegNum<[0]>;
+def ra_64   : GPR64<1, "x1",[ra]  >, DwarfRegNum<[1]>;
+def sp_64   : GPR64<2, "x2",[sp]  >, DwarfRegNum<[2]>;
+def gp_64   : GPR64<3, "x3",[gp]  >, DwarfRegNum<[3]>;
+def tp_64   : GPR64<4, "x4",[tp]  >, DwarfRegNum<[4]>;
+def t0_64   : GPR64<5, "x5",[t0]  >, DwarfRegNum<[5]>;
+def t1_64   : GPR64<6, "x6",[t1]  >, DwarfRegNum<[6]>;
+def t2_64   : GPR64<7, "x7",[t2]  >, DwarfRegNum<[7]>;
+def fp_64   : GPR64<8, "x8",[fp]  >, DwarfRegNum<[8]>;
+def s0_64   : GPR64<8, "x8",[s0]  >, DwarfRegNum<[8]>{
+  let Aliases = [fp_64];
+}
+def s1_64   : GPR64<9, "x9", [s1]  >, DwarfRegNum<[9]>;
+//return values
+def a0_64   : GPR64<10,"x10", [a0]  >, DwarfRegNum<[10]>;
+def a1_64   : GPR64<11,"x11", [a1]  >, DwarfRegNum<[11]>;
+//function arguments
+def a2_64   : GPR64<12,"x12", [a2]  >, DwarfRegNum<[12]>;
+def a3_64   : GPR64<13,"x13", [a3]  >, DwarfRegNum<[13]>;
+def a4_64   : GPR64<14,"x14", [a4]  >, DwarfRegNum<[14]>;
+def a5_64   : GPR64<15,"x15", [a5]  >, DwarfRegNum<[15]>;
+def a6_64   : GPR64<16,"x16", [a6]  >, DwarfRegNum<[16]>;
+def a7_64   : GPR64<17,"x17", [a7]  >, DwarfRegNum<[17]>;
+//saved registers
+def s2_64   : GPR64<18,"x18", [s2]  >, DwarfRegNum<[18]>;
+def s3_64   : GPR64<19,"x19", [s3]  >, DwarfRegNum<[19]>;
+def s4_64   : GPR64<20,"x20", [s4]  >, DwarfRegNum<[20]>;
+def s5_64   : GPR64<21,"x21", [s5]  >, DwarfRegNum<[21]>;
+def s6_64   : GPR64<22,"x22", [s6]  >, DwarfRegNum<[22]>;
+def s7_64   : GPR64<23,"x23", [s7]  >, DwarfRegNum<[23]>;
+def s8_64   : GPR64<24,"x24", [s8]  >, DwarfRegNum<[24]>;
+def s9_64   : GPR64<25,"x25", [s9]  >, DwarfRegNum<[25]>;
+def s10_64  : GPR64<26,"x26", [s10] >, DwarfRegNum<[26]>;
+def s11_64  : GPR64<27,"x27", [s11] >, DwarfRegNum<[27]>;
+//temp registers
+def t3_64   : GPR64<28,"x28", [t3]  >, DwarfRegNum<[28]>;
+def t4_64   : GPR64<29,"x29", [t4]  >, DwarfRegNum<[29]>;
+def t5_64   : GPR64<30,"x30", [t5]  >, DwarfRegNum<[30]>;
+def t6_64   : GPR64<31,"x31", [t6]  >, DwarfRegNum<[31]>;
+
+//All regs are 32bit
+defm GR64 : RISCVRegClass<"GR64", i64, 64, (add
+  zero_64, ra_64, sp_64, gp_64, tp_64,
+  t0_64, t1_64, t2_64,
+  fp_64, s0_64, s1_64,
+  a0_64, a1_64, a2_64, a3_64, a4_64, a5_64, a6_64, a7_64, 
+  s2_64, s3_64, s4_64, s5_64, s6_64, s7_64, s8_64, s9_64, s10_64, s11_64,
+  t3_64, t4_64, t5_64, t6_64), 1>;
+
+//Pairs of int arg regs can be used to store double-pointer word args
+class PairGPR128<bits<16> num, string n, list<Register> subregs>
+  : RISCVRegWithSubRegs<n, subregs> {
+  let HWEncoding = num;
+  let SubRegIndices = [sub_64even, sub_64odd];
+  let CoveredBySubRegs = 1;
+}
+
+def a0_p128 : PairGPR64<0, "x10", [a0_64, a1_64]>;
+def a1_p128 : PairGPR64<1, "x12", [a2_64, a3_64]>;
+def a2_p128 : PairGPR64<2, "x14", [a4_64, a5_64]>;
+def a3_p128 : PairGPR64<3, "x16", [a6_64, a7_64]>;
+
+defm PairGR128 : RISCVRegClass<"PairGR128", i128, 128, (add
+  a0_p128, a1_p128, a2_p128, a3_p128), 1>;
+
+//===----------------------------------------------------------------------===//
+// Floating-point registers
+//===----------------------------------------------------------------------===//
+
+// Lower 32 bits of one of the 16 64-bit floating-point registers
+class FPR32<bits<16> num, string n> : RISCVReg<n> {
+  let HWEncoding = num;
+}
+
+
+// Floating-point registers
+//ABI Names
+//FP temporary registers
+def ft0 : FPR32<0, "f0">, DwarfRegNum<[0]>;
+def ft1 : FPR32<1, "f1">, DwarfRegNum<[1]>;
+def ft2 : FPR32<2, "f2">, DwarfRegNum<[2]>;
+def ft3 : FPR32<3, "f3">, DwarfRegNum<[3]>;
+def ft4 : FPR32<4, "f4">, DwarfRegNum<[4]>;
+def ft5 : FPR32<5, "f5">, DwarfRegNum<[5]>;
+def ft6 : FPR32<6, "f6">, DwarfRegNum<[6]>;
+def ft7 : FPR32<7, "f7">, DwarfRegNum<[7]>;
+
+def fs0 : FPR32<8, "f8">, DwarfRegNum<[8]>;
+def fs1 : FPR32<9, "f9">, DwarfRegNum<[9]>;
+//FP arguments
+def fa0 : FPR32<10,"f10">, DwarfRegNum<[10]>;
+def fa1 : FPR32<11,"f11">, DwarfRegNum<[11]>;
+def fa2 : FPR32<12,"f12">, DwarfRegNum<[12]>;
+def fa3 : FPR32<13,"f13">, DwarfRegNum<[13]>;
+def fa4 : FPR32<14,"f14">, DwarfRegNum<[14]>;
+def fa5 : FPR32<15,"f15">, DwarfRegNum<[15]>;
+def fa6 : FPR32<16,"f16">, DwarfRegNum<[16]>;
+def fa7 : FPR32<17,"f17">, DwarfRegNum<[17]>;
+//FP saved registers
+def fs2  : FPR32<18,"f18">, DwarfRegNum<[18]>;
+def fs3  : FPR32<19,"f19">, DwarfRegNum<[19]>;
+def fs4  : FPR32<20,"f20">, DwarfRegNum<[20]>;
+def fs5  : FPR32<21,"f21">, DwarfRegNum<[21]>;
+def fs6  : FPR32<22,"f22">, DwarfRegNum<[22]>;
+def fs7  : FPR32<23,"f23">, DwarfRegNum<[23]>;
+def fs8  : FPR32<24,"f24">, DwarfRegNum<[24]>;
+def fs9  : FPR32<25,"f25">, DwarfRegNum<[25]>;
+def fs10 : FPR32<26,"f26">, DwarfRegNum<[26]>;
+def fs11 : FPR32<27,"f27">, DwarfRegNum<[27]>;
+//FP temp registers
+def ft8 : FPR32<28,"f28">, DwarfRegNum<[28]>;
+def ft9 : FPR32<29,"f29">, DwarfRegNum<[29]>;
+def ft10: FPR32<30,"f30">, DwarfRegNum<[30]>;
+def ft11: FPR32<31,"f31">, DwarfRegNum<[31]>;
+
+//all fp regs are 32bit
+defm FP32  : RISCVRegClass<"FP32", f32, 32, (add
+  ft0, ft1, ft2, ft3, ft4, ft5, ft6, ft7,
+  fs0, fs1, 
+  fa0, fa1, fa2, fa3, fa4, fa5, fa6, fa7,
+  fs2, fs3, fs4, fs5, fs6, fs7, fs8, fs9, fs10, fs11,
+  ft8, ft9, ft10, ft11
+  ), 1>;
+
+//Pairs of int arg regs can be used to store double-pointer word args
+class PairFPR64<bits<16> num, string n, list<Register> subregs>
+  : RISCVRegWithSubRegs<n, subregs> {
+  let HWEncoding = num;
+  let SubRegIndices = [sub_fp32even, sub_fp32odd];
+  let CoveredBySubRegs = 1;
+}
+
+def fa0_p64 : PairFPR64<0, "f10", [fa0, fa1]>;
+def fa1_p64 : PairFPR64<1, "f12", [fa2, fa3]>;
+def fa2_p64 : PairFPR64<2, "f14", [fa4, fa5]>;
+def fa3_p64 : PairFPR64<3, "f16", [fa6, fa7]>;
+
+defm PairFP64 : RISCVRegClass<"PairFP64", f64, 64, (add
+  fa0_p64, fa1_p64, fa2_p64, fa3_p64), 1>;
+
+// Lower 32 bits of one of the 16 64-bit floating-point registers
+class FPR64<bits<16> num, string n, list<Register> subregs> : RISCVRegWithSubRegs<n, subregs> {
+  let HWEncoding = num;
+  let SubRegIndices = [sub_32];
+}
+
+
+// Floating-point registers
+//ABI Names
+//FP temporary registers
+def ft0_64 : FPR64<0, "f0" , [ft0] >, DwarfRegNum<[0]>;
+def ft1_64 : FPR64<1, "f1" , [ft1] >, DwarfRegNum<[1]>;
+def ft2_64 : FPR64<2, "f2" , [ft2] >, DwarfRegNum<[2]>;
+def ft3_64 : FPR64<3, "f3" , [ft3] >, DwarfRegNum<[3]>;
+def ft4_64 : FPR64<4, "f4" , [ft4] >, DwarfRegNum<[4]>;
+def ft5_64 : FPR64<5, "f5" , [ft5] >, DwarfRegNum<[5]>;
+def ft6_64 : FPR64<6, "f6" , [ft6] >, DwarfRegNum<[6]>;
+def ft7_64 : FPR64<7, "f7" , [ft7] >, DwarfRegNum<[7]>;
+
+def fs0_64 : FPR64<8, "f8" , [fs0] >, DwarfRegNum<[8]>;
+def fs1_64 : FPR64<9, "f9" , [fs1] >, DwarfRegNum<[9]>;
+//FP arguments
+def fa0_64 : FPR64<10,"f10", [fa0] >, DwarfRegNum<[10]>;
+def fa1_64 : FPR64<11,"f11", [fa1] >, DwarfRegNum<[11]>;
+def fa2_64 : FPR64<12,"f12", [fa2] >, DwarfRegNum<[12]>;
+def fa3_64 : FPR64<13,"f13", [fa3] >, DwarfRegNum<[13]>;
+def fa4_64 : FPR64<14,"f14", [fa4] >, DwarfRegNum<[14]>;
+def fa5_64 : FPR64<15,"f15", [fa5] >, DwarfRegNum<[15]>;
+def fa6_64 : FPR64<16,"f16", [fa6] >, DwarfRegNum<[16]>;
+def fa7_64 : FPR64<17,"f17", [fa7] >, DwarfRegNum<[17]>;
+//FP saved registers
+def fs2_64 : FPR64<18,"f18", [fs2] >, DwarfRegNum<[18]>;
+def fs3_64 : FPR64<19,"f19", [fs3] >, DwarfRegNum<[19]>;
+def fs4_64 : FPR64<20,"f20", [fs4] >, DwarfRegNum<[20]>;
+def fs5_64 : FPR64<21,"f21", [fs5] >, DwarfRegNum<[21]>;
+def fs6_64 : FPR64<22,"f22", [fs6] >, DwarfRegNum<[22]>;
+def fs7_64 : FPR64<23,"f23", [fs7] >, DwarfRegNum<[23]>;
+def fs8_64 : FPR64<24,"f24", [fs8] >, DwarfRegNum<[24]>;
+def fs9_64 : FPR64<25,"f25", [fs9] >, DwarfRegNum<[25]>;
+def fs10_64: FPR64<26,"f26", [fs10] >, DwarfRegNum<[26]>;
+def fs11_64: FPR64<27,"f27", [fs11] >, DwarfRegNum<[27]>;
+//FP temp registers
+def ft8_64 : FPR64<28,"f28", [ft8] >, DwarfRegNum<[28]>;
+def ft9_64 : FPR64<29,"f29", [ft9] >, DwarfRegNum<[29]>;
+def ft10_64: FPR64<30,"f30", [ft10] >, DwarfRegNum<[30]>;
+def ft11_64: FPR64<31,"f31", [ft11] >, DwarfRegNum<[31]>;
+
+//all fp regs are 32bit
+defm FP64  : RISCVRegClass<"FP64", f64, 64, (add
+  ft0_64, ft1_64, ft2_64, ft3_64, ft4_64, ft5_64, ft6_64, ft7_64,
+  fs0_64, fs1_64, 
+  fa0_64, fa1_64, fa2_64, fa3_64, fa4_64, fa5_64, fa6_64, fa7_64,
+  fs2_64, fs3_64, fs4_64, fs5_64, fs6_64, fs7_64, fs8_64, fs9_64, fs10_64, fs11_64,
+  ft8_64, ft9_64, ft10_64, ft11_64
+  ), 1>;
+
+//Pairs of int arg regs can be used to store double-pointer word args
+class PairFPR128<bits<16> num, string n, list<Register> subregs>
+  : RISCVRegWithSubRegs<n, subregs> {
+  let HWEncoding = num;
+  let SubRegIndices = [sub_fp64even, sub_fp64odd];
+  let CoveredBySubRegs = 1;
+}
+
+def fa0_p128 : PairFPR128<0, "f10", [fa0_64, fa1_64]>;
+def fa1_p128 : PairFPR128<1, "f12", [fa2_64, fa3_64]>;
+def fa2_p128 : PairFPR128<2, "f14", [fa4_64, fa5_64]>;
+def fa3_p128 : PairFPR128<3, "f16", [fa6_64, fa7_64]>;
+
+defm PairFP128 : RISCVRegClass<"PairFP128", f128, 128, (add
+  fa0_p128, fa1_p128, fa2_p128, fa3_p128), 1>;
+
+//===----------------------------------------------------------------------===//
+// PCR registers (supervisor)
+//===----------------------------------------------------------------------===//
+class PCR<bits<16> num, string n> : RISCVReg<n> {
+  let HWEncoding = num;
+}
+//Scratch register for exception handlers
+def sup0     : PCR<0,"cr0">;
+//Scratch register for exception handlers
+def sup1     : PCR<1,"cr1">;
+//exception program counter
+def epc      : PCR<2, "cr2">;
+//Bad virtual address
+def badvaddr : PCR<3, "cr3">;
+//Page table base register
+def ptbr     : PCR<4, "cr4">;
+//Address space ID
+def asid     : PCR<5, "cr5">;
+//Cycle counter for timer
+def count    : PCR<6, "cr6">;
+//Timer compare value
+def compare  : PCR<7, "cr7">;
+//Exception handler address
+def evec     : PCR<8, "cr8">;
+//Cause of exception
+def cause    : PCR<9, "cr9">;
+//status reg
+def status   : PCR<10, "cr10">;
+//Hart ID
+def hartid   : PCR<11,"cr11">;
+//Implementation ID
+def impl     : PCR<12,"cr12">;
+//Flush address translation cache
+def fatc     : PCR<13, "cr13">;
+//Send inter-processor interrupt
+def send_ipi : PCR<14,"cr14">;
+//Clear inter-processor interrupt
+def clear_ipi: PCR<15,"cr15">;
+//Reserved 
+def pr0      : PCR<16,"cr16">;
+def pr1      : PCR<17,"cr17">;
+def pr2      : PCR<18,"cr18">;
+def pr3      : PCR<19,"cr19">;
+def pr4      : PCR<20,"cr20">;
+def pr5      : PCR<21,"cr21">;
+def pr6      : PCR<22,"cr22">;
+def pr7      : PCR<23,"cr23">;
+def pr8      : PCR<24,"cr24">;
+def pr9      : PCR<25,"cr25">;
+def pr10     : PCR<26,"cr26">;
+def pr11     : PCR<27,"cr27">;
+def pr12     : PCR<28,"cr28">;
+def pr13     : PCR<29,"cr29">;
+//Test output register
+def tohost   : PCR<30,"cr30">;
+//Test input register
+def fromhost : PCR<31,"cr31">;
+
+//PCRs 
+defm PCRReg : RISCVRegClass<"PCRReg", i32, 32, (add
+  //read/write
+  status, epc, evec, ptbr, asid, count, compare, sup0, sup1, tohost, fromhost,
+  //read only
+  badvaddr, cause, hartid, impl, 
+  //write only
+  fatc,  send_ipi, clear_ipi), 0>;
+
+class PCR64<bits<16> num, string n> : RISCVReg<n> {
+  let HWEncoding = num;
+}
+//Scratch register for exception handlers
+def sup0_64     : PCR64<0,"cr0">;
+//Scratch register for exception handlers
+def sup1_64     : PCR64<1,"cr1">;
+//exception program counter
+def epc_64      : PCR64<2, "cr2">;
+//Bad virtual address
+def badvaddr_64 : PCR64<3, "cr3">;
+//Page table base register
+def ptbr_64     : PCR64<4, "cr4">;
+//Address space ID
+def asid_64     : PCR64<5, "cr5">;
+//Cycle counter for timer
+def count_64    : PCR64<6, "cr6">;
+//Timer compare value
+def compare_64  : PCR64<7, "cr7">;
+//Exception handler address
+def evec_64     : PCR64<8, "cr8">;
+//Cause of exception
+def cause_64    : PCR64<9, "cr9">;
+//status reg
+def status_64   : PCR64<10, "cr10">;
+//Hart ID
+def hartid_64   : PCR64<11,"cr11">;
+//Implementation ID
+def impl_64     : PCR64<12,"cr12">;
+//Flush address translation cache
+def fatc_64     : PCR64<13, "cr13">;
+//Send inter-processor interrupt
+def send_ipi_64 : PCR64<14,"cr14">;
+//Clear inter-processor interrupt
+def clear_ipi_64: PCR64<15,"cr15">;
+//Reserved 
+def pr0_64      : PCR64<16,"cr16">;
+def pr1_64      : PCR64<17,"cr17">;
+def pr2_64      : PCR64<18,"cr18">;
+def pr3_64      : PCR64<19,"cr19">;
+def pr4_64      : PCR64<20,"cr20">;
+def pr5_64      : PCR64<21,"cr21">;
+def pr6_64      : PCR64<22,"cr22">;
+def pr7_64      : PCR64<23,"cr23">;
+def pr8_64      : PCR64<24,"cr24">;
+def pr9_64      : PCR64<25,"cr25">;
+def pr10_64     : PCR64<26,"cr26">;
+def pr11_64     : PCR64<27,"cr27">;
+def pr12_64     : PCR64<28,"cr28">;
+def pr13_64     : PCR64<29,"cr29">;
+//Test output register
+def tohost_64   : PCR64<30,"cr30">;
+//Test input register
+def fromhost_64 : PCR64<31,"cr31">;
+
+//PCRs (64-bit)
+defm PCR64Reg : RISCVRegClass<"PCR64Reg", i64, 64, (add
+  //read/write
+  status_64, epc_64, evec_64, ptbr_64, asid_64, count_64, compare_64, 
+  sup0_64, sup1_64, tohost_64, fromhost_64,
+  //read only
+  badvaddr_64, cause_64, hartid_64, impl_64, 
+  //write only
+  fatc_64,  send_ipi_64, clear_ipi_64), 0>;
+
+
+//===----------------------------------------------------------------------===//
+// Other registers
+//===----------------------------------------------------------------------===//
+
+// PC register
+def PC : RISCVReg<"pc">;
+defm PCReg : RISCVRegClass<"PCReg", i32, 32,(add PC), 0>;
+//FP status register
+def FCSR : RISCVReg<"fcsr">;
diff --git a/lib/Target/RISCV/RISCVSubtarget.cpp b/lib/Target/RISCV/RISCVSubtarget.cpp
new file mode 100644
index 0000000..a905ea2
--- /dev/null
+++ b/lib/Target/RISCV/RISCVSubtarget.cpp
@@ -0,0 +1,67 @@
+//===-- RISCVSubtarget.cpp - RISCV subtarget information --------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVSubtarget.h"
+#include "RISCV.h"
+#include "llvm/IR/GlobalValue.h"
+#include "llvm/Support/Debug.h"
+
+#define DEBUG_TYPE "riscv-subtarget"
+
+#define GET_SUBTARGETINFO_TARGET_DESC
+#define GET_SUBTARGETINFO_CTOR
+#include "RISCVGenSubtargetInfo.inc"
+
+using namespace llvm;
+
+RISCVSubtarget &RISCVSubtarget::initializeSubtargetDependencies(StringRef CPU,
+                                                                StringRef FS) {
+  std::string CPUName = CPU;
+  if (CPUName.empty()){
+    //TODO:generate cpu name?
+    CPUName = "";
+  }
+
+  // Parse features string.
+  ParseSubtargetFeatures(CPUName, FS);
+  return *this;
+}
+
+RISCVSubtarget::RISCVSubtarget(const Triple &TT, const std::string &CPU,
+                               const std::string &FS, const TargetMachine &TM)
+    : RISCVGenSubtargetInfo(TT, CPU, FS), RISCVArchVersion(RV32), HasM(false),
+      HasA(false), HasF(false), HasD(false), TargetTriple(TT),
+      InstrInfo(initializeSubtargetDependencies(CPU,FS)), TLInfo(TM, *this), TSInfo(), FrameLowering() {}
+
+// Return true if GV binds locally under reloc model RM.
+static bool bindsLocally(const GlobalValue *GV, Reloc::Model RM) {
+  // For non-PIC, all symbols bind locally.
+  if (RM == Reloc::Static)
+    return true;
+
+  return GV->hasLocalLinkage() || !GV->hasDefaultVisibility();
+}
+
+bool RISCVSubtarget::isPC32DBLSymbol(const GlobalValue *GV,
+                                       Reloc::Model RM,
+                                       CodeModel::Model CM) const {
+  // PC32DBL accesses require the low bit to be clear.  Note that a zero
+  // value selects the default alignment and is therefore OK.
+  if (GV->getAlignment() == 1)
+    return false;
+
+  // For the small model, all locally-binding symbols are in range.
+  if (CM == CodeModel::Small)
+    return bindsLocally(GV, RM);
+
+  // For Medium and above, assume that the symbol is not within the 4GB range.
+  // Taking the address of locally-defined text would be OK, but that
+  // case isn't easy to detect.
+  return false;
+}
diff --git a/lib/Target/RISCV/RISCVSubtarget.h b/lib/Target/RISCV/RISCVSubtarget.h
new file mode 100644
index 0000000..31d7865
--- /dev/null
+++ b/lib/Target/RISCV/RISCVSubtarget.h
@@ -0,0 +1,94 @@
+//===-- RISCVSubtarget.h - RISCV subtarget information ----------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file declares the RISCV specific subclass of TargetSubtargetInfo.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVSUBTARGET_H
+#define LLVM_LIB_TARGET_RISCV_RISCVSUBTARGET_H
+
+#include "RISCVFrameLowering.h"
+#include "RISCVISelLowering.h"
+#include "RISCVInstrInfo.h"
+#include "RISCVRegisterInfo.h"
+#include "llvm/IR/DataLayout.h"
+#include "llvm/ADT/Triple.h"
+#include "llvm/Target/TargetFrameLowering.h"
+#include "llvm/Target/TargetSelectionDAGInfo.h"
+#include "llvm/Target/TargetSubtargetInfo.h"
+#include <string>
+
+#define GET_SUBTARGETINFO_HEADER
+#include "RISCVGenSubtargetInfo.inc"
+
+namespace llvm {
+class GlobalValue;
+class StringRef;
+
+class RISCVSubtarget : public RISCVGenSubtargetInfo {
+protected:
+  enum RISCVArchEnum {
+    RV32,
+    RV64
+  };
+
+  RISCVArchEnum RISCVArchVersion;
+
+  bool HasM;
+  bool HasA;
+  bool HasF;
+  bool HasD;
+
+  bool UseSoftFloat;
+
+private:
+  Triple TargetTriple;
+  RISCVInstrInfo InstrInfo;
+  RISCVTargetLowering TLInfo;
+  TargetSelectionDAGInfo TSInfo;
+  RISCVFrameLowering FrameLowering;
+
+  RISCVSubtarget &initializeSubtargetDependencies(StringRef CPU, StringRef FS);
+
+public:
+  RISCVSubtarget(const Triple &TT, const std::string &CPU,
+                 const std::string &FS, const TargetMachine &TM);
+
+  const TargetFrameLowering *getFrameLowering() const { return &FrameLowering; }
+  const RISCVInstrInfo *getInstrInfo() const { return &InstrInfo; }
+  const RISCVRegisterInfo *getRegisterInfo() const {
+    return &InstrInfo.getRegisterInfo();
+  }
+  const RISCVTargetLowering *getTargetLowering() const { return &TLInfo; }
+  const TargetSelectionDAGInfo *getSelectionDAGInfo() const { return &TSInfo; }
+
+  bool isRV32() const { return RISCVArchVersion == RV32; };
+  bool isRV64() const { return RISCVArchVersion == RV64; };
+
+  bool hasM() const { return HasM; };
+  bool hasA() const { return HasA; };
+  bool hasF() const { return HasF; };
+  bool hasD() const { return HasD; };
+
+  bool useSoftFloat() const { return UseSoftFloat; }
+
+  // Automatically generated by tblgen.
+  void ParseSubtargetFeatures(StringRef CPU, StringRef FS);
+
+  // Return true if GV can be accessed using LARL for reloc model RM
+  // and code model CM.
+  bool isPC32DBLSymbol(const GlobalValue *GV, Reloc::Model RM,
+                       CodeModel::Model CM) const;
+
+  bool isTargetELF() const { return TargetTriple.isOSBinFormatELF(); }
+};
+} // end namespace llvm
+
+#endif
diff --git a/lib/Target/RISCV/RISCVTargetMachine.cpp b/lib/Target/RISCV/RISCVTargetMachine.cpp
new file mode 100644
index 0000000..2cb1fb5
--- /dev/null
+++ b/lib/Target/RISCV/RISCVTargetMachine.cpp
@@ -0,0 +1,101 @@
+//===-- RISCVTargetMachine.cpp - Define TargetMachine for RISCV -*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCVTargetMachine.h"
+#include "llvm/CodeGen/Passes.h"
+#include "llvm/CodeGen/TargetLoweringObjectFileImpl.h"
+#include "llvm/Support/TargetRegistry.h"
+
+using namespace llvm;
+
+extern "C" void LLVMInitializeRISCVTarget() {
+  // Register the target.
+  RegisterTargetMachine<RISCVTargetMachine> A(TheRISCVTarget);
+  RegisterTargetMachine<RISCV64TargetMachine> B(TheRISCV64Target);
+}
+
+static std::string computeDataLayout(const Triple &TT) {
+  
+  std::string Ret =
+       TT.isArch64Bit() ? "e-m:e-i1:8:16-i8:8:16-i64:64-f80:128-n32:64" :
+       "e-m:e-p:32:32:32-i1:8:16-i8:8:16-i16:16-i32:32-"
+       "f32:32-f64:64-f80:128-f128:128-n32";
+  return Ret;
+}
+
+RISCVTargetMachine::RISCVTargetMachine(const Target &T, const Triple &TT,
+                                       StringRef CPU, StringRef FS,
+                                       const TargetOptions &Options,
+                                       Reloc::Model RM, CodeModel::Model CM,
+                                       CodeGenOpt::Level OL)
+    : LLVMTargetMachine(T, computeDataLayout(TT), TT, CPU, FS, Options, RM, CM, OL),
+      TLOF(make_unique<RISCVTargetObjectFile>()),
+      Subtarget(TT, CPU, FS, *this) {
+  initAsmInfo();
+}
+
+RISCV64TargetMachine::RISCV64TargetMachine(const Target &T, const Triple &TT,
+                                       StringRef CPU, StringRef FS,
+                                       const TargetOptions &Options,
+                                       Reloc::Model RM, CodeModel::Model CM,
+                                       CodeGenOpt::Level OL)
+  :RISCVTargetMachine(T, TT, CPU, FS, Options, RM, CM, OL) {}
+
+
+const RISCVSubtarget *
+RISCVTargetMachine::getSubtargetImpl(const Function &F) const {
+  Attribute CPUAttr = F.getFnAttribute("target-cpu");
+  Attribute FSAttr = F.getFnAttribute("target-features");
+
+  std::string CPU = !CPUAttr.hasAttribute(Attribute::None)
+                        ? CPUAttr.getValueAsString().str()
+                        : TargetCPU;
+  std::string FS = !FSAttr.hasAttribute(Attribute::None)
+                       ? FSAttr.getValueAsString().str()
+                       : TargetFS;
+
+  auto &I = SubtargetMap[CPU + FS];
+  if (!I) {
+    // This needs to be done before we create a new subtarget since any
+    // creation will depend on the TM and the code generation flags on the
+    // function that reside in TargetOptions.
+    resetTargetOptions(F);
+    I = llvm::make_unique<RISCVSubtarget>(TargetTriple, CPU, FS, *this);
+  }
+  return I.get();
+}
+
+namespace {
+/// RISCV Code Generator Pass Configuration Options.
+class RISCVPassConfig : public TargetPassConfig {
+public:
+  RISCVPassConfig(RISCVTargetMachine *TM, PassManagerBase &PM)
+    : TargetPassConfig(TM, PM) {}
+
+  RISCVTargetMachine &getRISCVTargetMachine() const {
+    return getTM<RISCVTargetMachine>();
+  }
+
+  bool addInstSelector() override;
+  void addPreEmitPass() override;
+};
+} // end anonymous namespace
+
+bool RISCVPassConfig::addInstSelector() {
+  addPass(createRISCVISelDag(getRISCVTargetMachine(), getOptLevel()));
+  return false;
+}
+
+void RISCVPassConfig::addPreEmitPass(){
+  addPass(createRISCVBranchSelectionPass());
+}
+
+TargetPassConfig *RISCVTargetMachine::createPassConfig(PassManagerBase &PM) {
+  return new RISCVPassConfig(this, PM);
+}
diff --git a/lib/Target/RISCV/RISCVTargetMachine.h b/lib/Target/RISCV/RISCVTargetMachine.h
new file mode 100644
index 0000000..47a85f0
--- /dev/null
+++ b/lib/Target/RISCV/RISCVTargetMachine.h
@@ -0,0 +1,61 @@
+//===- RISCVTargetMachine.h - Define TargetMachine for RISCV ----*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+//
+// This file declares the RISCV specific subclass of TargetMachine.
+//
+//===----------------------------------------------------------------------===//
+
+
+#ifndef LLVM_LIB_TARGET_RISCV_RISCVTARGETMACHINE_H
+#define LLVM_LIB_TARGET_RISCV_RISCVTARGETMACHINE_H
+
+#include "RISCVSubtarget.h"
+#include "llvm/Target/TargetMachine.h"
+
+namespace llvm {
+
+class TargetFrameLowering;
+
+class RISCVTargetMachine : public LLVMTargetMachine {
+
+public:
+  RISCVTargetMachine(const Target &T, const Triple &TT, StringRef CPU,
+                       StringRef FS, const TargetOptions &Options,
+                       Reloc::Model RM, CodeModel::Model CM,
+                       CodeGenOpt::Level OL);
+
+  // Override TargetMachine.
+  const RISCVSubtarget *getSubtargetImpl() const { return &Subtarget; }
+  const RISCVSubtarget *getSubtargetImpl(const Function &F) const override;
+  // Override LLVMTargetMachine
+  TargetPassConfig *createPassConfig(PassManagerBase &PM) override;
+  TargetLoweringObjectFile *getObjFileLowering() const override {
+    return TLOF.get();
+  }
+
+protected:
+  std::unique_ptr<TargetLoweringObjectFile> TLOF;
+  RISCVSubtarget Subtarget;
+
+private:
+  mutable StringMap<std::unique_ptr<RISCVSubtarget>> SubtargetMap;
+};
+
+class RISCV64TargetMachine : public RISCVTargetMachine {
+
+public:
+  RISCV64TargetMachine(const Target &T, const Triple &TT, StringRef CPU,
+                       StringRef FS, const TargetOptions &Options,
+                       Reloc::Model RM, CodeModel::Model CM,
+                       CodeGenOpt::Level OL);
+};
+
+} // end namespace llvm
+
+#endif
diff --git a/lib/Target/RISCV/TargetInfo/CMakeLists.txt b/lib/Target/RISCV/TargetInfo/CMakeLists.txt
new file mode 100644
index 0000000..8ab1ec5
--- /dev/null
+++ b/lib/Target/RISCV/TargetInfo/CMakeLists.txt
@@ -0,0 +1,7 @@
+include_directories( ${CMAKE_CURRENT_BINARY_DIR}/.. ${CMAKE_CURRENT_SOURCE_DIR}/.. )
+
+add_llvm_library(LLVMRISCVInfo
+  RISCVTargetInfo.cpp
+  )
+
+add_dependencies(LLVMRISCVInfo RISCVCommonTableGen)
diff --git a/lib/Target/RISCV/TargetInfo/LLVMBuild.txt b/lib/Target/RISCV/TargetInfo/LLVMBuild.txt
new file mode 100644
index 0000000..caff8a2
--- /dev/null
+++ b/lib/Target/RISCV/TargetInfo/LLVMBuild.txt
@@ -0,0 +1,23 @@
+;===- ./lib/Target/RISCV/TargetInfo/LLVMBuild.txt --------------*- Conf -*--===;
+;
+;                     The LLVM Compiler Infrastructure
+;
+; This file is distributed under the University of Illinois Open Source
+; License. See LICENSE.TXT for details.
+;
+;===------------------------------------------------------------------------===;
+;
+; This is an LLVMBuild description file for the components in this subdirectory.
+;
+; For more information on the LLVMBuild system, please see:
+;
+;   http://llvm.org/docs/LLVMBuild.html
+;
+;===------------------------------------------------------------------------===;
+
+[component_0]
+type = Library
+name = RISCVInfo
+parent = RISCV
+required_libraries = MC Support Target
+add_to_library_groups = RISCV
diff --git a/lib/Target/RISCV/TargetInfo/Makefile b/lib/Target/RISCV/TargetInfo/Makefile
new file mode 100644
index 0000000..491ddd1
--- /dev/null
+++ b/lib/Target/RISCV/TargetInfo/Makefile
@@ -0,0 +1,15 @@
+##===- lib/Target/RISCV/TargetInfo/Makefile ----------------*- Makefile -*-===##
+#
+#                     The LLVM Compiler Infrastructure
+#
+# This file is distributed under the University of Illinois Open Source
+# License. See LICENSE.TXT for details.
+#
+##===----------------------------------------------------------------------===##
+LEVEL = ../../../..
+LIBRARYNAME = LLVMRISCVInfo
+
+# Hack: we need to include 'main' target directory to grab private headers
+CPPFLAGS = -I$(PROJ_OBJ_DIR)/.. -I$(PROJ_SRC_DIR)/..
+
+include $(LEVEL)/Makefile.common
diff --git a/lib/Target/RISCV/TargetInfo/RISCVTargetInfo.cpp b/lib/Target/RISCV/TargetInfo/RISCVTargetInfo.cpp
new file mode 100644
index 0000000..0deb836
--- /dev/null
+++ b/lib/Target/RISCV/TargetInfo/RISCVTargetInfo.cpp
@@ -0,0 +1,23 @@
+//===-- RISCVTargetInfo.cpp - RISCV target implementation -------*- C++ -*-===//
+//
+//                     The LLVM Compiler Infrastructure
+//
+// This file is distributed under the University of Illinois Open Source
+// License. See LICENSE.TXT for details.
+//
+//===----------------------------------------------------------------------===//
+
+#include "RISCV.h"
+#include "llvm/Support/TargetRegistry.h"
+
+using namespace llvm;
+
+Target llvm::TheRISCVTarget, llvm::TheRISCV64Target;
+
+extern "C" void LLVMInitializeRISCVTargetInfo() {
+  RegisterTarget<Triple::riscv, /*HasJIT=*/false>
+    A(TheRISCVTarget, "riscv", "RISCV");
+
+  RegisterTarget<Triple::riscv64, /*HasJIT=*/false>
+    B(TheRISCV64Target, "riscv64", "RISCV 64");
+}
diff --git a/lib/Target/Sparc/AsmParser/SparcAsmParser.cpp b/lib/Target/Sparc/AsmParser/SparcAsmParser.cpp
index 1c4e486..4a33f7f 100644
--- a/lib/Target/Sparc/AsmParser/SparcAsmParser.cpp
+++ b/lib/Target/Sparc/AsmParser/SparcAsmParser.cpp
@@ -77,7 +77,7 @@ class SparcAsmParser : public MCTargetAsmParser {
   bool parseDirectiveWord(unsigned Size, SMLoc L);
 
   bool is64Bit() const {
-    return STI.getTargetTriple().getArch() == Triple::sparcv9;
+    return STI.getTargetTriple().getArchName().startswith("sparcv9");
   }
 
   void expandSET(MCInst &Inst, SMLoc IDLoc,
diff --git a/lib/Target/X86/X86ISelLowering.cpp b/lib/Target/X86/X86ISelLowering.cpp
index 0f29b51..71ccb1a 100644
--- a/lib/Target/X86/X86ISelLowering.cpp
+++ b/lib/Target/X86/X86ISelLowering.cpp
@@ -13573,35 +13573,6 @@ static SDValue LowerVSETCC(SDValue Op, const X86Subtarget *Subtarget,
                        DAG.getConstant(SSECC, dl, MVT::i8));
   }
 
-  MVT VTOp0 = Op0.getSimpleValueType();
-  assert(VTOp0 == Op1.getSimpleValueType() &&
-         "Expected operands with same type!");
-  assert(VT.getVectorNumElements() == VTOp0.getVectorNumElements() &&
-         "Invalid number of packed elements for source and destination!");
-
-  if (VT.is128BitVector() && VTOp0.is256BitVector()) {
-    // On non-AVX512 targets, a vector of MVT::i1 is promoted by the type
-    // legalizer to a wider vector type.  In the case of 'vsetcc' nodes, the
-    // legalizer firstly checks if the first operand in input to the setcc has
-    // a legal type. If so, then it promotes the return type to that same type.
-    // Otherwise, the return type is promoted to the 'next legal type' which,
-    // for a vector of MVT::i1 is always a 128-bit integer vector type.
-    //
-    // We reach this code only if the following two conditions are met:
-    // 1. Both return type and operand type have been promoted to wider types
-    //    by the type legalizer.
-    // 2. The original operand type has been promoted to a 256-bit vector.
-    //
-    // Note that condition 2. only applies for AVX targets.
-    SDValue NewOp = DAG.getSetCC(dl, VTOp0, Op0, Op1, SetCCOpcode);
-    return DAG.getZExtOrTrunc(NewOp, dl, VT);
-  }
-
-  // The non-AVX512 code below works under the assumption that source and
-  // destination types are the same.
-  assert((Subtarget->hasAVX512() || (VT == VTOp0)) &&
-         "Value types for source and destination must be the same!");
-
   // Break 256-bit integer vector compare into smaller ones.
   if (VT.is256BitVector() && !Subtarget->hasInt256())
     return Lower256IntVSETCC(Op, DAG);
diff --git a/lib/Transforms/IPO/PassManagerBuilder.cpp b/lib/Transforms/IPO/PassManagerBuilder.cpp
index 909baae..88e5e47 100644
--- a/lib/Transforms/IPO/PassManagerBuilder.cpp
+++ b/lib/Transforms/IPO/PassManagerBuilder.cpp
@@ -228,7 +228,7 @@ void PassManagerBuilder::populateModulePassManager(
   // Start of function pass.
   // Break up aggregate allocas, using SSAUpdater.
   if (UseNewSROA)
-    MPM.add(createSROAPass());
+    MPM.add(createSROAPass(/*RequiresDomTree*/ false));
   else
     MPM.add(createScalarReplAggregatesPass(-1, false));
   MPM.add(createEarlyCSEPass());              // Catch trivial redundancies
diff --git a/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp b/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp
index 15e0889..ee21c81 100644
--- a/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp
+++ b/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp
@@ -93,8 +93,7 @@ static Value *getFCmpValue(bool isordered, unsigned code,
   case 5: Pred = isordered ? FCmpInst::FCMP_ONE : FCmpInst::FCMP_UNE; break;
   case 6: Pred = isordered ? FCmpInst::FCMP_OLE : FCmpInst::FCMP_ULE; break;
   case 7:
-    if (!isordered)
-      return ConstantInt::get(CmpInst::makeCmpResultType(LHS->getType()), 1);
+    if (!isordered) return ConstantInt::getTrue(LHS->getContext());
     Pred = FCmpInst::FCMP_ORD; break;
   }
   return Builder->CreateFCmp(Pred, LHS, RHS);
diff --git a/lib/Transforms/Scalar/GVN.cpp b/lib/Transforms/Scalar/GVN.cpp
index 89a0d0a..d1eba6e 100644
--- a/lib/Transforms/Scalar/GVN.cpp
+++ b/lib/Transforms/Scalar/GVN.cpp
@@ -1761,8 +1761,7 @@ bool GVN::processNonLocalLoad(LoadInst *LI) {
     if (isa<PHINode>(V))
       V->takeName(LI);
     if (Instruction *I = dyn_cast<Instruction>(V))
-      if (LI->getDebugLoc())
-        I->setDebugLoc(LI->getDebugLoc());
+      I->setDebugLoc(LI->getDebugLoc());
     if (V->getType()->getScalarType()->isPointerTy())
       MD->invalidateCachedPointerInfo(V);
     markInstructionForDeletion(LI);
diff --git a/lib/Transforms/Scalar/Scalarizer.cpp b/lib/Transforms/Scalar/Scalarizer.cpp
index 0493003..d55dc6a 100644
--- a/lib/Transforms/Scalar/Scalarizer.cpp
+++ b/lib/Transforms/Scalar/Scalarizer.cpp
@@ -227,16 +227,10 @@ Value *Scatterer::operator[](unsigned I) {
       if (!Idx)
         break;
       unsigned J = Idx->getZExtValue();
+      CV[J] = Insert->getOperand(1);
       V = Insert->getOperand(0);
-      if (I == J) {
-        CV[J] = Insert->getOperand(1);
+      if (I == J)
         return CV[J];
-      } else if (!CV[J]) {
-        // Only cache the first entry we find for each index we're not actively
-        // searching for. This prevents us from going too far up the chain and
-        // caching incorrect entries.
-        CV[J] = Insert->getOperand(1);
-      }
     }
     CV[I] = Builder.CreateExtractElement(V, Builder.getInt32(I),
                                          V->getName() + ".i" + Twine(I));
diff --git a/lib/Transforms/Utils/Local.cpp b/lib/Transforms/Utils/Local.cpp
index ba8af47..50ca623 100644
--- a/lib/Transforms/Utils/Local.cpp
+++ b/lib/Transforms/Utils/Local.cpp
@@ -869,11 +869,6 @@ bool llvm::EliminateDuplicatePHINodes(BasicBlock *BB) {
       PN->replaceAllUsesWith(*Inserted.first);
       PN->eraseFromParent();
       Changed = true;
-
-      // The RAUW can change PHIs that we already visited. Start over from the
-      // beginning.
-      PHISet.clear();
-      I = BB->begin();
     }
   }
 
diff --git a/test/Analysis/BasicAA/gep-alias.ll b/test/Analysis/BasicAA/gep-alias.ll
index 1e435af..f686010 100644
--- a/test/Analysis/BasicAA/gep-alias.ll
+++ b/test/Analysis/BasicAA/gep-alias.ll
@@ -228,51 +228,3 @@ define i32 @test12(i32 %x, i32 %y, i8* %p) nounwind {
 ; CHECK-LABEL: @test12(
 ; CHECK: ret i32 %r
 }
-
-@P = internal global i32 715827882, align 4
-@Q = internal global i32 715827883, align 4
-@.str = private unnamed_addr constant [7 x i8] c"%u %u\0A\00", align 1
-
-; Make sure we recognize that u[0] and u[Global + Cst] may alias
-; when the addition has wrapping semantic.
-; PR24468.
-; CHECK-LABEL: @test13(
-; Make sure the stores appear before the related loads.
-; CHECK: store i8 42,
-; CHECK: store i8 99,
-; Find the loads and make sure they are used in the arguments to the printf.
-; CHECK: [[T0ADDR:%[a-zA-Z0-9_]+]] = getelementptr inbounds [3 x i8], [3 x i8]* %t, i32 0, i32 0
-; CHECK: [[T0:%[a-zA-Z0-9_]+]] = load i8, i8* [[T0ADDR]], align 1
-; CHECK: [[T0ARG:%[a-zA-Z0-9_]+]] = zext i8 [[T0]] to i32
-; CHECK: [[U0ADDR:%[a-zA-Z0-9_]+]] = getelementptr inbounds [3 x i8], [3 x i8]* %u, i32 0, i32 0
-; CHECK: [[U0:%[a-zA-Z0-9_]+]] = load i8, i8* [[U0ADDR]], align 1
-; CHECK: [[U0ARG:%[a-zA-Z0-9_]+]] = zext i8 [[U0]] to i32
-; CHECK: call i32 (i8*, ...) @printf(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str, i32 0, i32 0), i32 [[T0ARG]], i32 [[U0ARG]])
-; CHECK: ret
-define void @test13() {
-entry:
-  %t = alloca [3 x i8], align 1
-  %u = alloca [3 x i8], align 1
-  %tmp = load i32, i32* @P, align 4
-  %tmp1 = mul i32 %tmp, 3
-  %mul = add i32 %tmp1, -2147483646
-  %idxprom = zext i32 %mul to i64
-  %arrayidx = getelementptr inbounds [3 x i8], [3 x i8]* %t, i64 0, i64 %idxprom
-  store i8 42, i8* %arrayidx, align 1
-  %tmp2 = load i32, i32* @Q, align 4
-  %tmp3 = mul i32 %tmp2, 3
-  %mul2 = add i32 %tmp3, 2147483647
-  %idxprom3 = zext i32 %mul2 to i64
-  %arrayidx4 = getelementptr inbounds [3 x i8], [3 x i8]* %u, i64 0, i64 %idxprom3
-  store i8 99, i8* %arrayidx4, align 1
-  %arrayidx5 = getelementptr inbounds [3 x i8], [3 x i8]* %t, i64 0, i64 0
-  %tmp4 = load i8, i8* %arrayidx5, align 1
-  %conv = zext i8 %tmp4 to i32
-  %arrayidx6 = getelementptr inbounds [3 x i8], [3 x i8]* %u, i64 0, i64 0
-  %tmp5 = load i8, i8* %arrayidx6, align 1
-  %conv7 = zext i8 %tmp5 to i32
-  %call = call i32 (i8*, ...) @printf(i8* getelementptr inbounds ([7 x i8], [7 x i8]* @.str, i64 0, i64 0), i32 %conv, i32 %conv7)
-  ret void
-}
-
-declare i32 @printf(i8*, ...)
diff --git a/test/Analysis/BasicAA/phi-aa.ll b/test/Analysis/BasicAA/phi-aa.ll
index a727782..3944e9e 100644
--- a/test/Analysis/BasicAA/phi-aa.ll
+++ b/test/Analysis/BasicAA/phi-aa.ll
@@ -39,6 +39,7 @@ return:
 
 ; CHECK-LABEL: pr18068
 ; CHECK: MayAlias: i32* %0, i32* %arrayidx5
+; CHECK: NoAlias: i32* %arrayidx13, i32* %arrayidx5
 
 define i32 @pr18068(i32* %jj7, i32* %j) {
 entry:
diff --git a/test/Analysis/BasicAA/zext.ll b/test/Analysis/BasicAA/zext.ll
new file mode 100644
index 0000000..ed35656
--- /dev/null
+++ b/test/Analysis/BasicAA/zext.ll
@@ -0,0 +1,209 @@
+; RUN: opt < %s -basicaa -aa-eval -print-all-alias-modref-info -disable-output 2>&1 | FileCheck %s
+target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
+target triple = "x86_64-unknown-linux-gnu"
+
+; CHECK-LABEL: test_with_zext
+; CHECK:  NoAlias: i8* %a, i8* %b
+
+define void @test_with_zext() {
+  %1 = tail call i8* @malloc(i64 120)
+  %a = getelementptr inbounds i8, i8* %1, i64 8
+  %2 = getelementptr inbounds i8, i8* %1, i64 16
+  %3 = zext i32 3 to i64
+  %b = getelementptr inbounds i8, i8* %2, i64 %3
+  ret void
+}
+
+; CHECK-LABEL: test_with_lshr
+; CHECK:  NoAlias: i8* %a, i8* %b
+
+define void @test_with_lshr(i64 %i) {
+  %1 = tail call i8* @malloc(i64 120)
+  %a = getelementptr inbounds i8, i8* %1, i64 8
+  %2 = getelementptr inbounds i8, i8* %1, i64 16
+  %3 = lshr i64 %i, 2
+  %b = getelementptr inbounds i8, i8* %2, i64 %3
+  ret void
+}
+
+; CHECK-LABEL: test_with_a_loop
+; CHECK:  NoAlias: i8* %a, i8* %b
+
+define void @test_with_a_loop(i8* %mem) {
+  br label %for.loop
+
+for.loop:
+  %i = phi i32 [ 0, %0 ], [ %i.plus1, %for.loop ]
+  %a = getelementptr inbounds i8, i8* %mem, i64 8
+  %a.plus1 = getelementptr inbounds i8, i8* %mem, i64 16
+  %i.64 = zext i32 %i to i64
+  %b = getelementptr inbounds i8, i8* %a.plus1, i64 %i.64
+  %i.plus1 = add nuw nsw i32 %i, 1
+  %cmp = icmp eq i32 %i.plus1, 10
+  br i1 %cmp, label %for.loop.exit, label %for.loop
+
+for.loop.exit:
+  ret void
+}
+
+; CHECK-LABEL: test_with_varying_base_pointer_in_loop
+; CHECK:  NoAlias: i8* %a, i8* %b
+
+define void @test_with_varying_base_pointer_in_loop(i8* %mem.orig) {
+  br label %for.loop
+
+for.loop:
+  %mem = phi i8* [ %mem.orig, %0 ], [ %mem.plus1, %for.loop ]
+  %i = phi i32 [ 0, %0 ], [ %i.plus1, %for.loop ]
+  %a = getelementptr inbounds i8, i8* %mem, i64 8
+  %a.plus1 = getelementptr inbounds i8, i8* %mem, i64 16
+  %i.64 = zext i32 %i to i64
+  %b = getelementptr inbounds i8, i8* %a.plus1, i64 %i.64
+  %i.plus1 = add nuw nsw i32 %i, 1
+  %mem.plus1 = getelementptr inbounds i8, i8* %mem, i64 8
+  %cmp = icmp eq i32 %i.plus1, 10
+  br i1 %cmp, label %for.loop.exit, label %for.loop
+
+for.loop.exit:
+  ret void
+}
+
+; CHECK-LABEL: test_sign_extension
+; CHECK:  PartialAlias: i64* %b.i64, i8* %a
+
+define void @test_sign_extension(i32 %p) {
+  %1 = tail call i8* @malloc(i64 120)
+  %p.64 = zext i32 %p to i64
+  %a = getelementptr inbounds i8, i8* %1, i64 %p.64
+  %p.minus1 = add i32 %p, -1
+  %p.minus1.64 = zext i32 %p.minus1 to i64
+  %b.i8 = getelementptr inbounds i8, i8* %1, i64 %p.minus1.64
+  %b.i64 = bitcast i8* %b.i8 to i64*
+  ret void
+}
+
+; CHECK-LABEL: test_fe_tools
+; CHECK:  PartialAlias: i32* %a, i32* %b
+
+define void @test_fe_tools([8 x i32]* %values) {
+  br label %reorder
+
+for.loop:
+  %i = phi i32 [ 0, %reorder ], [ %i.next, %for.loop ]
+  %idxprom = zext i32 %i to i64
+  %b = getelementptr inbounds [8 x i32], [8 x i32]* %values, i64 0, i64 %idxprom
+  %i.next = add nuw nsw i32 %i, 1
+  %1 = icmp eq i32 %i.next, 10
+  br i1 %1, label %for.loop.exit, label %for.loop
+
+reorder:
+  %a = getelementptr inbounds [8 x i32], [8 x i32]* %values, i64 0, i64 1
+  br label %for.loop
+
+for.loop.exit:
+  ret void
+}
+
+@b = global i32 0, align 4
+@d = global i32 0, align 4
+
+; CHECK-LABEL: test_spec2006
+; CHECK:  PartialAlias: i32** %x, i32** %y
+
+define void @test_spec2006() {
+  %h = alloca [1 x [2 x i32*]], align 16
+  %d.val = load i32, i32* @d, align 4
+  %d.promoted = sext i32 %d.val to i64
+  %1 = icmp slt i32 %d.val, 2
+  br i1 %1, label %.lr.ph, label %3
+
+.lr.ph:                                           ; preds = %0
+  br label %2
+
+; <label>:2                                       ; preds = %.lr.ph, %2
+  %i = phi i32 [ %d.val, %.lr.ph ], [ %i.plus1, %2 ]
+  %i.promoted = sext i32 %i to i64
+  %x = getelementptr inbounds [1 x [2 x i32*]], [1 x [2 x i32*]]* %h, i64 0, i64 %d.promoted, i64 %i.promoted
+  %i.plus1 = add nsw i32 %i, 1
+  %cmp = icmp slt i32 %i.plus1, 2
+  br i1 %cmp, label %2, label %3
+
+; <label>:3                                      ; preds = %._crit_edge, %0
+  %y = getelementptr inbounds [1 x [2 x i32*]], [1 x [2 x i32*]]* %h, i64 0, i64 0, i64 1
+  ret void
+}
+
+; CHECK-LABEL: test_modulo_analysis_easy_case
+; CHECK:  NoAlias: i32** %x, i32** %y
+
+define void @test_modulo_analysis_easy_case(i64 %i) {
+  %h = alloca [1 x [2 x i32*]], align 16
+  %x = getelementptr inbounds [1 x [2 x i32*]], [1 x [2 x i32*]]* %h, i64 0, i64 %i, i64 0
+  %y = getelementptr inbounds [1 x [2 x i32*]], [1 x [2 x i32*]]* %h, i64 0, i64 0, i64 1
+  ret void
+}
+
+; CHECK-LABEL: test_modulo_analysis_in_loop
+; CHECK:  NoAlias: i32** %x, i32** %y
+
+define void @test_modulo_analysis_in_loop() {
+  %h = alloca [1 x [2 x i32*]], align 16
+  br label %for.loop
+
+for.loop:
+  %i = phi i32 [ 0, %0 ], [ %i.plus1, %for.loop ]
+  %i.promoted = sext i32 %i to i64
+  %x = getelementptr inbounds [1 x [2 x i32*]], [1 x [2 x i32*]]* %h, i64 0, i64 %i.promoted, i64 0
+  %y = getelementptr inbounds [1 x [2 x i32*]], [1 x [2 x i32*]]* %h, i64 0, i64 0, i64 1
+  %i.plus1 = add nsw i32 %i, 1
+  %cmp = icmp slt i32 %i.plus1, 2
+  br i1 %cmp, label %for.loop, label %for.loop.exit
+
+for.loop.exit:
+  ret void
+}
+
+; CHECK-LABEL: test_modulo_analysis_with_global
+; CHECK:  PartialAlias: i32** %x, i32** %y
+
+define void @test_modulo_analysis_with_global() {
+  %h = alloca [1 x [2 x i32*]], align 16
+  %b = load i32, i32* @b, align 4
+  %b.promoted = sext i32 %b to i64
+  br label %for.loop
+
+for.loop:
+  %i = phi i32 [ 0, %0 ], [ %i.plus1, %for.loop ]
+  %i.promoted = sext i32 %i to i64
+  %x = getelementptr inbounds [1 x [2 x i32*]], [1 x [2 x i32*]]* %h, i64 0, i64 %i.promoted, i64 %b.promoted
+  %y = getelementptr inbounds [1 x [2 x i32*]], [1 x [2 x i32*]]* %h, i64 0, i64 0, i64 1
+  %i.plus1 = add nsw i32 %i, 1
+  %cmp = icmp slt i32 %i.plus1, 2
+  br i1 %cmp, label %for.loop, label %for.loop.exit
+
+for.loop.exit:
+  ret void
+}
+
+; CHECK-LABEL: test_const_eval
+; CHECK: NoAlias: i8* %a, i8* %b
+define void @test_const_eval(i8* %ptr, i64 %offset) {
+  %a = getelementptr inbounds i8, i8* %ptr, i64 %offset
+  %a.dup = getelementptr inbounds i8, i8* %ptr, i64 %offset
+  %three = zext i32 3 to i64
+  %b = getelementptr inbounds i8, i8* %a.dup, i64 %three
+  ret void
+}
+
+; CHECK-LABEL: test_const_eval_scaled
+; CHECK: MustAlias: i8* %a, i8* %b
+define void @test_const_eval_scaled(i8* %ptr) {
+  %three = zext i32 3 to i64
+  %six = mul i64 %three, 2
+  %a = getelementptr inbounds i8, i8* %ptr, i64 %six
+  %b = getelementptr inbounds i8, i8* %ptr, i64 6
+  ret void
+}
+
+; Function Attrs: nounwind
+declare noalias i8* @malloc(i64)
diff --git a/test/CodeGen/AMDGPU/llvm.dbg.value.ll b/test/CodeGen/AMDGPU/llvm.dbg.value.ll
deleted file mode 100644
index d001bcb..0000000
--- a/test/CodeGen/AMDGPU/llvm.dbg.value.ll
+++ /dev/null
@@ -1,37 +0,0 @@
-; RUN: llc -O0 -march=amdgcn -mtriple=amdgcn-unknown-amdhsa -verify-machineinstrs < %s | FileCheck %s
-
-; CHECK-LABEL: {{^}}test_debug_value:
-; CHECK: s_load_dwordx2
-; CHECK: DEBUG_VALUE: test_debug_value:globalptr_arg <- SGPR0_SGPR1
-; CHECK: buffer_store_dword
-; CHECK: s_endpgm
-define void @test_debug_value(i32 addrspace(1)* nocapture %globalptr_arg) #0 {
-entry:
-  tail call void @llvm.dbg.value(metadata i32 addrspace(1)* %globalptr_arg, i64 0, metadata !10, metadata !13), !dbg !14
-  store i32 123, i32 addrspace(1)* %globalptr_arg, align 4
-  ret void
-}
-
-declare void @llvm.dbg.value(metadata, i64, metadata, metadata) #1
-
-attributes #0 = { nounwind "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
-attributes #1 = { nounwind readnone }
-
-!llvm.dbg.cu = !{!0}
-!llvm.module.flags = !{!11, !12}
-
-!0 = distinct !DICompileUnit(language: DW_LANG_C99, file: !1, producer: "clang version 3.8.0 (trunk 244715) (llvm/trunk 244718)", isOptimized: true, runtimeVersion: 0, emissionKind: 1, enums: !2, subprograms: !3)
-!1 = !DIFile(filename: "/tmp/test_debug_value.cl", directory: "/Users/matt/src/llvm/build_debug")
-!2 = !{}
-!3 = !{!4}
-!4 = !DISubprogram(name: "test_debug_value", scope: !1, file: !1, line: 1, type: !5, isLocal: false, isDefinition: true, scopeLine: 2, flags: DIFlagPrototyped, isOptimized: true, function: void (i32 addrspace(1)*)* @test_debug_value, variables: !9)
-!5 = !DISubroutineType(types: !6)
-!6 = !{null, !7}
-!7 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !8, size: 64, align: 32)
-!8 = !DIBasicType(name: "int", size: 32, align: 32, encoding: DW_ATE_signed)
-!9 = !{!10}
-!10 = !DILocalVariable(tag: DW_TAG_arg_variable, name: "globalptr_arg", arg: 1, scope: !4, file: !1, line: 1, type: !7)
-!11 = !{i32 2, !"Dwarf Version", i32 4}
-!12 = !{i32 2, !"Debug Info Version", i32 3}
-!13 = !DIExpression()
-!14 = !DILocation(line: 1, column: 42, scope: !4)
diff --git a/test/CodeGen/AMDGPU/promote-alloca-bitcast-function.ll b/test/CodeGen/AMDGPU/promote-alloca-bitcast-function.ll
deleted file mode 100644
index 10739df..0000000
--- a/test/CodeGen/AMDGPU/promote-alloca-bitcast-function.ll
+++ /dev/null
@@ -1,22 +0,0 @@
-; RUN: not llc -march=amdgcn < %s 2>&1 | FileCheck %s
-
-; Make sure that AMDGPUPromoteAlloca doesn't crash if the called
-; function is a constantexpr cast of a function.
-
-declare void @foo(float*) #0
-declare void @foo.varargs(...) #0
-
-; CHECK: error: unsupported call to function foo in crash_call_constexpr_cast
-define void @crash_call_constexpr_cast() #0 {
-  %alloca = alloca i32
-  call void bitcast (void (float*)* @foo to void (i32*)*)(i32* %alloca) #0
-  ret void
-}
-
-define void @crash_call_constexpr_cast_varargs() #0 {
-  %alloca = alloca i32
-  call void bitcast (void (...)* @foo.varargs to void (i32*)*)(i32* %alloca) #0
-  ret void
-}
-
-attributes #0 = { nounwind }
diff --git a/test/CodeGen/AMDGPU/promote-alloca-stored-pointer-value.ll b/test/CodeGen/AMDGPU/promote-alloca-stored-pointer-value.ll
deleted file mode 100644
index 2ee98cc..0000000
--- a/test/CodeGen/AMDGPU/promote-alloca-stored-pointer-value.ll
+++ /dev/null
@@ -1,52 +0,0 @@
-; RUN: llc -march=amdgcn < %s | FileCheck -check-prefix=GCN %s
-
-; Pointer value is stored in a candidate for LDS usage.
-
-; GCN-LABEL: {{^}}stored_lds_pointer_value:
-; GCN: buffer_store_dword v
-define void @stored_lds_pointer_value(float* addrspace(1)* %ptr) #0 {
-  %tmp = alloca float
-  store float 0.0, float *%tmp
-  store float* %tmp, float* addrspace(1)* %ptr
-  ret void
-}
-
-; GCN-LABEL: {{^}}stored_lds_pointer_value_gep:
-; GCN-DAG: s_mov_b32 s{{[0-9]+}}, SCRATCH_RSRC_DWORD0
-; GCN-DAG: s_mov_b32 s{{[0-9]+}}, SCRATCH_RSRC_DWORD1
-; GCN: buffer_store_dword v
-; GCN: buffer_store_dword v
-define void @stored_lds_pointer_value_gep(float* addrspace(1)* %ptr, i32 %idx) #0 {
-bb:
-  %tmp = alloca float, i32 16
-  store float 0.0, float* %tmp
-  %tmp2 = getelementptr inbounds float, float* %tmp, i32 %idx
-  store float* %tmp2, float* addrspace(1)* %ptr
-  ret void
-}
-
-; Pointer value is stored in a candidate for vector usage
-; GCN-LABEL: {{^}}stored_vector_pointer_value:
-; GCN-DAG: s_mov_b32 s{{[0-9]+}}, SCRATCH_RSRC_DWORD0
-; GCN-DAG: s_mov_b32 s{{[0-9]+}}, SCRATCH_RSRC_DWORD1
-; GCN: buffer_store_dword
-; GCN: buffer_store_dword
-; GCN: buffer_store_dword
-; GCN: buffer_store_dword
-define void @stored_vector_pointer_value(i32* addrspace(1)* %out, i32 %index) {
-entry:
-  %tmp0 = alloca [4 x i32]
-  %x = getelementptr [4 x i32], [4 x i32]* %tmp0, i32 0, i32 0
-  %y = getelementptr [4 x i32], [4 x i32]* %tmp0, i32 0, i32 1
-  %z = getelementptr [4 x i32], [4 x i32]* %tmp0, i32 0, i32 2
-  %w = getelementptr [4 x i32], [4 x i32]* %tmp0, i32 0, i32 3
-  store i32 0, i32* %x
-  store i32 1, i32* %y
-  store i32 2, i32* %z
-  store i32 3, i32* %w
-  %tmp1 = getelementptr [4 x i32], [4 x i32]* %tmp0, i32 0, i32 %index
-  store i32* %tmp1, i32* addrspace(1)* %out
-  ret void
-}
-
-attributes #0 = { nounwind }
diff --git a/test/CodeGen/AMDGPU/trunc-store.ll b/test/CodeGen/AMDGPU/trunc-store.ll
deleted file mode 100644
index 4ba815f..0000000
--- a/test/CodeGen/AMDGPU/trunc-store.ll
+++ /dev/null
@@ -1,48 +0,0 @@
-; RUN: llc -march=amdgcn -mcpu=verde -verify-machineinstrs < %s | FileCheck -check-prefix=SI -check-prefix=FUNC %s
-; RUN: llc -march=amdgcn -mcpu=tonga -verify-machineinstrs < %s | FileCheck -check-prefix=SI -check-prefix=FUNC %s
-
-; FUNC-LABEL: {{^}}truncstore_arg_v16i32_to_v16i8:
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-define void @truncstore_arg_v16i32_to_v16i8(<16 x i8> addrspace(1)* %out, <16 x i32> %in) {
-  %trunc = trunc <16 x i32> %in to <16 x i8>
-  store <16 x i8> %trunc, <16 x i8> addrspace(1)* %out
-  ret void
-}
-
-; FUNC-LABEL: {{^}}truncstore_arg_v16i64_to_v16i8:
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-; SI: buffer_store_byte
-define void @truncstore_arg_v16i64_to_v16i8(<16 x i8> addrspace(1)* %out, <16 x i64> %in) {
-  %trunc = trunc <16 x i64> %in to <16 x i8>
-  store <16 x i8> %trunc, <16 x i8> addrspace(1)* %out
-  ret void
-}
diff --git a/test/CodeGen/BPF/fi_ri.ll b/test/CodeGen/BPF/fi_ri.ll
deleted file mode 100644
index 64773b4..0000000
--- a/test/CodeGen/BPF/fi_ri.ll
+++ /dev/null
@@ -1,25 +0,0 @@
-; RUN: llc < %s -march=bpf | FileCheck %s
-
-%struct.key_t = type { i32, [16 x i8] }
-
-; Function Attrs: nounwind uwtable
-define i32 @test() #0 {
-  %key = alloca %struct.key_t, align 4
-  %1 = bitcast %struct.key_t* %key to i8*
-; CHECK: mov	r1, 0
-; CHECK: stw	-8(r10), r1
-; CHECK: std	-16(r10), r1
-; CHECK: std	-24(r10), r1
-  call void @llvm.memset.p0i8.i64(i8* %1, i8 0, i64 20, i32 4, i1 false)
-; CHECK: mov	r1, r10
-; CHECK: addi	r1, -20
-  %2 = getelementptr inbounds %struct.key_t, %struct.key_t* %key, i64 0, i32 1, i64 0
-; CHECK: call	test1
-  call void @test1(i8* %2) #3
-  ret i32 0
-}
-
-; Function Attrs: nounwind argmemonly
-declare void @llvm.memset.p0i8.i64(i8* nocapture, i8, i64, i32, i1) #1
-
-declare void @test1(i8*) #2
diff --git a/test/CodeGen/BPF/sockex2.ll b/test/CodeGen/BPF/sockex2.ll
index 5de2787..d372a59 100644
--- a/test/CodeGen/BPF/sockex2.ll
+++ b/test/CodeGen/BPF/sockex2.ll
@@ -311,7 +311,7 @@ flow_dissector.exit.thread:                       ; preds = %86, %12, %196, %199
 ; CHECK-LABEL: bpf_prog2:
 ; CHECK: ldabs_h r0, r6.data + 12 # encoding: [0x28,0x00,0x00,0x00,0x0c,0x00,0x00,0x00]
 ; CHECK: ldabs_h r0, r6.data + 16 # encoding: [0x28,0x00,0x00,0x00,0x10,0x00,0x00,0x00]
-; CHECK: implicit-def: R
+; CHECK-NOT: implicit
 ; CHECK: ld_64   r1
 ; CHECK-NOT: ori
 ; CHECK: call 1 # encoding: [0x85,0x00,0x00,0x00,0x01,0x00,0x00,0x00]
diff --git a/test/CodeGen/BPF/undef.ll b/test/CodeGen/BPF/undef.ll
deleted file mode 100644
index ef712c4..0000000
--- a/test/CodeGen/BPF/undef.ll
+++ /dev/null
@@ -1,68 +0,0 @@
-; RUN: llc < %s -march=bpf | FileCheck %s
-
-%struct.bpf_map_def = type { i32, i32, i32, i32 }
-%struct.__sk_buff = type opaque
-%struct.routing_key_2 = type { [6 x i8] }
-
-@routing = global %struct.bpf_map_def { i32 1, i32 6, i32 12, i32 1024 }, section "maps", align 4
-@routing_miss_0 = global %struct.bpf_map_def { i32 1, i32 1, i32 12, i32 1 }, section "maps", align 4
-@test1 = global %struct.bpf_map_def { i32 2, i32 4, i32 8, i32 1024 }, section "maps", align 4
-@test1_miss_4 = global %struct.bpf_map_def { i32 2, i32 1, i32 8, i32 1 }, section "maps", align 4
-@_license = global [4 x i8] c"GPL\00", section "license", align 1
-@llvm.used = appending global [6 x i8*] [i8* getelementptr inbounds ([4 x i8], [4 x i8]* @_license, i32 0, i32 0), i8* bitcast (i32 (%struct.__sk_buff*)* @ebpf_filter to i8*), i8* bitcast (%struct.bpf_map_def* @routing to i8*), i8* bitcast (%struct.bpf_map_def* @routing_miss_0 to i8*), i8* bitcast (%struct.bpf_map_def* @test1 to i8*), i8* bitcast (%struct.bpf_map_def* @test1_miss_4 to i8*)], section "llvm.metadata"
-
-; Function Attrs: nounwind uwtable
-define i32 @ebpf_filter(%struct.__sk_buff* nocapture readnone %ebpf_packet) #0 section "socket1" {
-  %key = alloca %struct.routing_key_2, align 1
-  %1 = getelementptr inbounds %struct.routing_key_2, %struct.routing_key_2* %key, i64 0, i32 0, i64 0
-; CHECK: mov	r1, 5
-; CHECK: stb	-8(r10), r1
-  store i8 5, i8* %1, align 1
-  %2 = getelementptr inbounds %struct.routing_key_2, %struct.routing_key_2* %key, i64 0, i32 0, i64 1
-; CHECK: mov	r1, 6
-; CHECK: stb	-7(r10), r1
-  store i8 6, i8* %2, align 1
-  %3 = getelementptr inbounds %struct.routing_key_2, %struct.routing_key_2* %key, i64 0, i32 0, i64 2
-; CHECK: mov	r1, 7
-; CHECK: stb	-6(r10), r1
-  store i8 7, i8* %3, align 1
-  %4 = getelementptr inbounds %struct.routing_key_2, %struct.routing_key_2* %key, i64 0, i32 0, i64 3
-; CHECK: mov	r1, 8
-; CHECK: stb	-5(r10), r1
-  store i8 8, i8* %4, align 1
-  %5 = getelementptr inbounds %struct.routing_key_2, %struct.routing_key_2* %key, i64 0, i32 0, i64 4
-; CHECK: mov	r1, 9
-; CHECK: stb	-4(r10), r1
-  store i8 9, i8* %5, align 1
-  %6 = getelementptr inbounds %struct.routing_key_2, %struct.routing_key_2* %key, i64 0, i32 0, i64 5
-; CHECK: mov	r1, 10
-; CHECK: stb	-3(r10), r1
-  store i8 10, i8* %6, align 1
-  %7 = getelementptr inbounds %struct.routing_key_2, %struct.routing_key_2* %key, i64 1, i32 0, i64 0
-; CHECK: mov	r1, r10
-; CHECK: addi	r1, -2
-; CHECK: mov	r2, 0
-; CHECK: sth	6(r1), r2
-; CHECK: sth	4(r1), r2
-; CHECK: sth	2(r1), r2
-; CHECK: sth	24(r10), r2
-; CHECK: sth	22(r10), r2
-; CHECK: sth	20(r10), r2
-; CHECK: sth	18(r10), r2
-; CHECK: sth	16(r10), r2
-; CHECK: sth	14(r10), r2
-; CHECK: sth	12(r10), r2
-; CHECK: sth	10(r10), r2
-; CHECK: sth	8(r10), r2
-; CHECK: sth	6(r10), r2
-; CHECK: sth	-2(r10), r2
-; CHECK: sth	26(r10), r2
-  call void @llvm.memset.p0i8.i64(i8* %7, i8 0, i64 30, i32 1, i1 false)
-  %8 = call i32 (%struct.bpf_map_def*, %struct.routing_key_2*, ...) bitcast (i32 (...)* @bpf_map_lookup_elem to i32 (%struct.bpf_map_def*, %struct.routing_key_2*, ...)*)(%struct.bpf_map_def* nonnull @routing, %struct.routing_key_2* nonnull %key) #3
-  ret i32 undef
-}
-
-; Function Attrs: nounwind argmemonly
-declare void @llvm.memset.p0i8.i64(i8* nocapture, i8, i64, i32, i1) #1
-
-declare i32 @bpf_map_lookup_elem(...) #2
diff --git a/test/CodeGen/Mips/llvm-ir/addrspacecast.ll b/test/CodeGen/Mips/llvm-ir/addrspacecast.ll
deleted file mode 100644
index 060fa4c..0000000
--- a/test/CodeGen/Mips/llvm-ir/addrspacecast.ll
+++ /dev/null
@@ -1,12 +0,0 @@
-; RUN: llc < %s -march=mips -mcpu=mips2 | FileCheck %s -check-prefix=ALL
-
-; Address spaces 1-255 are software defined.
-define i32* @cast(i32 *%arg) {
-  %1 = addrspacecast i32* %arg to i32 addrspace(1)*
-  %2 = addrspacecast i32 addrspace(1)* %1 to i32 addrspace(2)*
-  %3 = addrspacecast i32 addrspace(2)* %2 to i32 addrspace(0)*
-  ret i32* %3
-}
-
-; ALL-LABEL: cast:
-; ALL:           move   $2, $4
diff --git a/test/CodeGen/Mips/llvm-ir/extractelement.ll b/test/CodeGen/Mips/llvm-ir/extractelement.ll
deleted file mode 100644
index 1e1b02d..0000000
--- a/test/CodeGen/Mips/llvm-ir/extractelement.ll
+++ /dev/null
@@ -1,19 +0,0 @@
-; RUN: llc < %s -march=mips -mcpu=mips2 | FileCheck %s -check-prefix=ALL
-
-; This test triggered a bug in the vector splitting where the type legalizer
-; attempted to extract the element with by storing the vector, then reading
-; an element back. However, the address calculation was:
-;   Base + Index * (EltSizeInBits / 8)
-; and EltSizeInBits was 1. This caused the index to be forgotten.
-define i1 @via_stack_bug(i8 signext %idx) {
-  %1 = extractelement <2 x i1> <i1 false, i1 true>, i8 %idx
-  ret i1 %1
-}
-
-; ALL-LABEL: via_stack_bug:
-; ALL-DAG:       addiu  [[ONE:\$[0-9]+]], $zero, 1
-; ALL-DAG:       sb     [[ONE]], 7($sp)
-; ALL-DAG:       sb     $zero, 6($sp)
-; ALL-DAG:       addiu  [[VPTR:\$[0-9]+]], $sp, 6
-; ALL-DAG:       addu   [[EPTR:\$[0-9]+]], $4, [[VPTR]]
-; ALL:           lbu    $2, 0([[EPTR]])
diff --git a/test/CodeGen/Mips/micromips-zero-mat-uses.ll b/test/CodeGen/Mips/micromips-zero-mat-uses.ll
deleted file mode 100644
index b38747a..0000000
--- a/test/CodeGen/Mips/micromips-zero-mat-uses.ll
+++ /dev/null
@@ -1,8 +0,0 @@
-; RUN: llc -march=mips -mcpu=mips32r2 -mattr=+micromips,+nooddspreg -O0 < %s | FileCheck %s
-
-; CHECK: addiu    $[[R0:[0-9]+]], $zero, 0
-; CHECK: subu16   $2, $[[R0]], ${{[0-9]+}}
-define i32 @foo() {
-  %1 = sub i32 0, undef
-  ret i32 %1
-}
diff --git a/test/CodeGen/PowerPC/ctr-loop-tls-const.ll b/test/CodeGen/PowerPC/ctr-loop-tls-const.ll
deleted file mode 100644
index 01f837c..0000000
--- a/test/CodeGen/PowerPC/ctr-loop-tls-const.ll
+++ /dev/null
@@ -1,40 +0,0 @@
-; RUN: llc -mcpu=pwr7 -relocation-model=pic < %s | FileCheck %s
-target datalayout = "E-m:e-i64:64-n32:64"
-target triple = "powerpc64-unknown-linux-gnu"
-
-@x = thread_local global [1600 x i32] zeroinitializer, align 4
-
-; Function Attrs: nounwind
-define void @foo(i32 signext %v) #0 {
-entry:
-  br label %vector.body
-
-vector.body:                                      ; preds = %vector.body, %entry
-  %index = phi i64 [ 0, %entry ], [ %index.next, %vector.body ]
-  %induction5 = or i64 %index, 1
-  %0 = getelementptr inbounds [1600 x i32], [1600 x i32]* @x, i64 0, i64 %index
-  %1 = getelementptr inbounds [1600 x i32], [1600 x i32]* @x, i64 0, i64 %induction5
-  %2 = load i32, i32* %0, align 4
-  %3 = load i32, i32* %1, align 4
-  %4 = add nsw i32 %2, %v
-  %5 = add nsw i32 %3, %v
-  store i32 %4, i32* %0, align 4
-  store i32 %5, i32* %1, align 4
-  %index.next = add i64 %index, 2
-  %6 = icmp eq i64 %index.next, 1600
-  br i1 %6, label %for.cond.cleanup, label %vector.body
-
-for.cond.cleanup:                                 ; preds = %vector.body
-  ret void
-}
-
-; CHECK-LABEL: @foo
-; CHECK-NOT: mtctr
-; CHECK: __tls_get_addr
-
-attributes #0 = { nounwind }
-
-!llvm.module.flags = !{!0}
-
-!0 = !{i32 1, !"PIC Level", i32 2}
-
diff --git a/test/CodeGen/PowerPC/ctrloop-intrin.ll b/test/CodeGen/PowerPC/ctrloop-intrin.ll
deleted file mode 100644
index 7c781cd..0000000
--- a/test/CodeGen/PowerPC/ctrloop-intrin.ll
+++ /dev/null
@@ -1,349 +0,0 @@
-; RUN: llc < %s
-; ModuleID = 'new.bc'
-target datalayout = "e-m:e-i64:64-n32:64"
-target triple = "powerpc64le--linux-gnu"
-
-@.str.87 = external hidden unnamed_addr constant [5 x i8], align 1
-@.str.1.88 = external hidden unnamed_addr constant [4 x i8], align 1
-@.str.2.89 = external hidden unnamed_addr constant [5 x i8], align 1
-@.str.3.90 = external hidden unnamed_addr constant [4 x i8], align 1
-@.str.4.91 = external hidden unnamed_addr constant [14 x i8], align 1
-@.str.5.92 = external hidden unnamed_addr constant [13 x i8], align 1
-@.str.6.93 = external hidden unnamed_addr constant [10 x i8], align 1
-@.str.7.94 = external hidden unnamed_addr constant [9 x i8], align 1
-@.str.8.95 = external hidden unnamed_addr constant [2 x i8], align 1
-@.str.9.96 = external hidden unnamed_addr constant [2 x i8], align 1
-@.str.10.97 = external hidden unnamed_addr constant [3 x i8], align 1
-@.str.11.98 = external hidden unnamed_addr constant [3 x i8], align 1
-
-; Function Attrs: nounwind
-declare void @llvm.lifetime.start(i64, i8* nocapture) #0
-
-; Function Attrs: nounwind
-declare void @llvm.lifetime.end(i64, i8* nocapture) #0
-
-; Function Attrs: nounwind
-declare i8* @halide_string_to_string(i8*, i8*, i8*) #1
-
-; Function Attrs: nounwind
-declare i8* @halide_int64_to_string(i8*, i8*, i64, i32) #1
-
-; Function Attrs: nounwind
-define weak i8* @halide_double_to_string(i8* %dst, i8* %end, double %arg, i32 %scientific) #1 {
-entry:
-  %arg.addr = alloca double, align 8
-  %bits = alloca i64, align 8
-  %buf = alloca [512 x i8], align 1
-  store double %arg, double* %arg.addr, align 8, !tbaa !4
-  %0 = bitcast i64* %bits to i8*
-  call void @llvm.lifetime.start(i64 8, i8* %0) #0
-  store i64 0, i64* %bits, align 8, !tbaa !8
-  %1 = bitcast double* %arg.addr to i8*
-  %call = call i8* @memcpy(i8* %0, i8* %1, i64 8) #2
-  %2 = load i64, i64* %bits, align 8, !tbaa !8
-  %and = and i64 %2, 4503599627370495
-  %shr = lshr i64 %2, 52
-  %shr.tr = trunc i64 %shr to i32
-  %conv = and i32 %shr.tr, 2047
-  %shr2 = lshr i64 %2, 63
-  %conv3 = trunc i64 %shr2 to i32
-  %cmp = icmp eq i32 %conv, 2047
-  br i1 %cmp, label %if.then, label %if.else.15
-
-if.then:                                          ; preds = %entry
-  %tobool = icmp eq i64 %and, 0
-  %tobool5 = icmp ne i32 %conv3, 0
-  br i1 %tobool, label %if.else.9, label %if.then.4
-
-if.then.4:                                        ; preds = %if.then
-  br i1 %tobool5, label %if.then.6, label %if.else
-
-if.then.6:                                        ; preds = %if.then.4
-  %call7 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.87, i64 0, i64 0)) #3
-  br label %cleanup.148
-
-if.else:                                          ; preds = %if.then.4
-  %call8 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.1.88, i64 0, i64 0)) #3
-  br label %cleanup.148
-
-if.else.9:                                        ; preds = %if.then
-  br i1 %tobool5, label %if.then.11, label %if.else.13
-
-if.then.11:                                       ; preds = %if.else.9
-  %call12 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([5 x i8], [5 x i8]* @.str.2.89, i64 0, i64 0)) #3
-  br label %cleanup.148
-
-if.else.13:                                       ; preds = %if.else.9
-  %call14 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([4 x i8], [4 x i8]* @.str.3.90, i64 0, i64 0)) #3
-  br label %cleanup.148
-
-if.else.15:                                       ; preds = %entry
-  %cmp16 = icmp eq i32 %conv, 0
-  %cmp17 = icmp eq i64 %and, 0
-  %or.cond = and i1 %cmp17, %cmp16
-  br i1 %or.cond, label %if.then.18, label %if.end.32
-
-if.then.18:                                       ; preds = %if.else.15
-  %tobool19 = icmp eq i32 %scientific, 0
-  %tobool21 = icmp ne i32 %conv3, 0
-  br i1 %tobool19, label %if.else.26, label %if.then.20
-
-if.then.20:                                       ; preds = %if.then.18
-  br i1 %tobool21, label %if.then.22, label %if.else.24
-
-if.then.22:                                       ; preds = %if.then.20
-  %call23 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([14 x i8], [14 x i8]* @.str.4.91, i64 0, i64 0)) #3
-  br label %cleanup.148
-
-if.else.24:                                       ; preds = %if.then.20
-  %call25 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([13 x i8], [13 x i8]* @.str.5.92, i64 0, i64 0)) #3
-  br label %cleanup.148
-
-if.else.26:                                       ; preds = %if.then.18
-  br i1 %tobool21, label %if.then.28, label %if.else.30
-
-if.then.28:                                       ; preds = %if.else.26
-  %call29 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([10 x i8], [10 x i8]* @.str.6.93, i64 0, i64 0)) #3
-  br label %cleanup.148
-
-if.else.30:                                       ; preds = %if.else.26
-  %call31 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([9 x i8], [9 x i8]* @.str.7.94, i64 0, i64 0)) #3
-  br label %cleanup.148
-
-if.end.32:                                        ; preds = %if.else.15
-  %tobool33 = icmp eq i32 %conv3, 0
-  br i1 %tobool33, label %if.end.37, label %if.then.34
-
-if.then.34:                                       ; preds = %if.end.32
-  %call35 = call i8* @halide_string_to_string(i8* %dst, i8* %end, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.8.95, i64 0, i64 0)) #3
-  %sub36 = fsub double -0.000000e+00, %arg
-  store double %sub36, double* %arg.addr, align 8, !tbaa !4
-  br label %if.end.37
-
-if.end.37:                                        ; preds = %if.then.34, %if.end.32
-  %.pr = phi double [ %sub36, %if.then.34 ], [ %arg, %if.end.32 ]
-  %dst.addr.0 = phi i8* [ %call35, %if.then.34 ], [ %dst, %if.end.32 ]
-  %tobool38 = icmp eq i32 %scientific, 0
-  br i1 %tobool38, label %if.else.62, label %while.condthread-pre-split
-
-while.condthread-pre-split:                       ; preds = %if.end.37
-  %cmp40.261 = fcmp olt double %.pr, 1.000000e+00
-  br i1 %cmp40.261, label %while.body, label %while.cond.41thread-pre-split
-
-while.body:                                       ; preds = %while.body, %while.condthread-pre-split
-  %exponent_base_10.0262 = phi i32 [ %dec, %while.body ], [ 0, %while.condthread-pre-split ]
-  %3 = phi double [ %mul, %while.body ], [ %.pr, %while.condthread-pre-split ]
-  %mul = fmul double %3, 1.000000e+01
-  %dec = add nsw i32 %exponent_base_10.0262, -1
-  %cmp40 = fcmp olt double %mul, 1.000000e+00
-  br i1 %cmp40, label %while.body, label %while.cond.while.cond.41thread-pre-split_crit_edge
-
-while.cond.while.cond.41thread-pre-split_crit_edge: ; preds = %while.body
-  store double %mul, double* %arg.addr, align 8, !tbaa !4
-  br label %while.cond.41thread-pre-split
-
-while.cond.41thread-pre-split:                    ; preds = %while.cond.while.cond.41thread-pre-split_crit_edge, %while.condthread-pre-split
-  %.pr246 = phi double [ %mul, %while.cond.while.cond.41thread-pre-split_crit_edge ], [ %.pr, %while.condthread-pre-split ]
-  %exponent_base_10.0.lcssa = phi i32 [ %dec, %while.cond.while.cond.41thread-pre-split_crit_edge ], [ 0, %while.condthread-pre-split ]
-  %cmp42.257 = fcmp ult double %.pr246, 1.000000e+01
-  br i1 %cmp42.257, label %while.end.44, label %while.body.43
-
-while.body.43:                                    ; preds = %while.body.43, %while.cond.41thread-pre-split
-  %exponent_base_10.1258 = phi i32 [ %inc, %while.body.43 ], [ %exponent_base_10.0.lcssa, %while.cond.41thread-pre-split ]
-  %4 = phi double [ %div, %while.body.43 ], [ %.pr246, %while.cond.41thread-pre-split ]
-  %div = fdiv double %4, 1.000000e+01
-  %inc = add nsw i32 %exponent_base_10.1258, 1
-  %cmp42 = fcmp ult double %div, 1.000000e+01
-  br i1 %cmp42, label %while.cond.41.while.end.44_crit_edge, label %while.body.43
-
-while.cond.41.while.end.44_crit_edge:             ; preds = %while.body.43
-  store double %div, double* %arg.addr, align 8, !tbaa !4
-  br label %while.end.44
-
-while.end.44:                                     ; preds = %while.cond.41.while.end.44_crit_edge, %while.cond.41thread-pre-split
-  %exponent_base_10.1.lcssa = phi i32 [ %inc, %while.cond.41.while.end.44_crit_edge ], [ %exponent_base_10.0.lcssa, %while.cond.41thread-pre-split ]
-  %.lcssa = phi double [ %div, %while.cond.41.while.end.44_crit_edge ], [ %.pr246, %while.cond.41thread-pre-split ]
-  %mul45 = fmul double %.lcssa, 1.000000e+06
-  %add = fadd double %mul45, 5.000000e-01
-  %conv46 = fptoui double %add to i64
-  %div47 = udiv i64 %conv46, 1000000
-  %5 = mul i64 %div47, -1000000
-  %sub49 = add i64 %conv46, %5
-  %call50 = call i8* @halide_int64_to_string(i8* %dst.addr.0, i8* %end, i64 %div47, i32 1) #3
-  %call51 = call i8* @halide_string_to_string(i8* %call50, i8* %end, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.9.96, i64 0, i64 0)) #3
-  %call52 = call i8* @halide_int64_to_string(i8* %call51, i8* %end, i64 %sub49, i32 6) #3
-  %cmp53 = icmp sgt i32 %exponent_base_10.1.lcssa, -1
-  br i1 %cmp53, label %if.then.54, label %if.else.56
-
-if.then.54:                                       ; preds = %while.end.44
-  %call55 = call i8* @halide_string_to_string(i8* %call52, i8* %end, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.10.97, i64 0, i64 0)) #3
-  br label %if.end.59
-
-if.else.56:                                       ; preds = %while.end.44
-  %call57 = call i8* @halide_string_to_string(i8* %call52, i8* %end, i8* getelementptr inbounds ([3 x i8], [3 x i8]* @.str.11.98, i64 0, i64 0)) #3
-  %sub58 = sub nsw i32 0, %exponent_base_10.1.lcssa
-  br label %if.end.59
-
-if.end.59:                                        ; preds = %if.else.56, %if.then.54
-  %exponent_base_10.2 = phi i32 [ %exponent_base_10.1.lcssa, %if.then.54 ], [ %sub58, %if.else.56 ]
-  %dst.addr.1 = phi i8* [ %call55, %if.then.54 ], [ %call57, %if.else.56 ]
-  %conv60 = sext i32 %exponent_base_10.2 to i64
-  %call61 = call i8* @halide_int64_to_string(i8* %dst.addr.1, i8* %end, i64 %conv60, i32 2) #3
-  br label %cleanup.148
-
-if.else.62:                                       ; preds = %if.end.37
-  br i1 %cmp16, label %if.then.64, label %if.end.66
-
-if.then.64:                                       ; preds = %if.else.62
-  %call65 = call i8* @halide_double_to_string(i8* %dst.addr.0, i8* %end, double 0.000000e+00, i32 0) #3
-  br label %cleanup.148
-
-if.end.66:                                        ; preds = %if.else.62
-  %add68 = or i64 %and, 4503599627370496
-  %sub70 = add nsw i32 %conv, -1075
-  %cmp71 = icmp ult i32 %conv, 1075
-  br i1 %cmp71, label %if.then.72, label %if.end.105
-
-if.then.72:                                       ; preds = %if.end.66
-  %cmp73 = icmp slt i32 %sub70, -52
-  br i1 %cmp73, label %if.end.84, label %if.else.76
-
-if.else.76:                                       ; preds = %if.then.72
-  %sub77 = sub nsw i32 1075, %conv
-  %sh_prom = zext i32 %sub77 to i64
-  %shr78 = lshr i64 %add68, %sh_prom
-  %shl81 = shl i64 %shr78, %sh_prom
-  %sub82 = sub i64 %add68, %shl81
-  br label %if.end.84
-
-if.end.84:                                        ; preds = %if.else.76, %if.then.72
-  %integer_part.0 = phi i64 [ %shr78, %if.else.76 ], [ 0, %if.then.72 ]
-  %f.0.in = phi i64 [ %sub82, %if.else.76 ], [ %add68, %if.then.72 ]
-  %f.0 = uitofp i64 %f.0.in to double
-  %conv85.244 = zext i32 %sub70 to i64
-  %shl86 = shl i64 %conv85.244, 52
-  %add88 = add i64 %shl86, 4696837146684686336
-  %6 = bitcast i64 %add88 to double
-  %mul90 = fmul double %6, %f.0
-  %add91 = fadd double %mul90, 5.000000e-01
-  %conv92 = fptoui double %add91 to i64
-  %conv93 = uitofp i64 %conv92 to double
-  %and96 = and i64 %conv92, 1
-  %notlhs = fcmp oeq double %conv93, %add91
-  %notrhs = icmp ne i64 %and96, 0
-  %not.or.cond245 = and i1 %notrhs, %notlhs
-  %dec99 = sext i1 %not.or.cond245 to i64
-  %fractional_part.0 = add i64 %dec99, %conv92
-  %cmp101 = icmp eq i64 %fractional_part.0, 1000000
-  %inc103 = zext i1 %cmp101 to i64
-  %inc103.integer_part.0 = add i64 %inc103, %integer_part.0
-  %.fractional_part.0 = select i1 %cmp101, i64 0, i64 %fractional_part.0
-  br label %if.end.105
-
-if.end.105:                                       ; preds = %if.end.84, %if.end.66
-  %integer_part.2 = phi i64 [ %inc103.integer_part.0, %if.end.84 ], [ %add68, %if.end.66 ]
-  %integer_exponent.0 = phi i32 [ 0, %if.end.84 ], [ %sub70, %if.end.66 ]
-  %fractional_part.2 = phi i64 [ %.fractional_part.0, %if.end.84 ], [ 0, %if.end.66 ]
-  %7 = bitcast [512 x i8]* %buf to i8*
-  call void @llvm.lifetime.start(i64 512, i8* %7) #0
-  %add.ptr = getelementptr inbounds [512 x i8], [512 x i8]* %buf, i64 0, i64 512
-  %add.ptr106 = getelementptr inbounds [512 x i8], [512 x i8]* %buf, i64 0, i64 480
-  %call109 = call i8* @halide_int64_to_string(i8* %add.ptr106, i8* %add.ptr, i64 %integer_part.2, i32 1) #3
-  %cmp110.252 = icmp sgt i32 %integer_exponent.0, 0
-  br i1 %cmp110.252, label %for.cond.112.preheader, label %for.cond.cleanup
-
-for.cond.112.preheader:                           ; preds = %if.end.138, %if.end.105
-  %i.0255 = phi i32 [ %inc140, %if.end.138 ], [ 0, %if.end.105 ]
-  %int_part_ptr.0253 = phi i8* [ %int_part_ptr.1, %if.end.138 ], [ %add.ptr106, %if.end.105 ]
-  %int_part_ptr.02534 = ptrtoint i8* %int_part_ptr.0253 to i64
-  %cmp114.249 = icmp eq i8* %call109, %int_part_ptr.0253
-  br i1 %cmp114.249, label %if.end.138, label %for.body.116.preheader
-
-for.body.116.preheader:                           ; preds = %for.cond.112.preheader
-  %8 = sub i64 0, %int_part_ptr.02534
-  %scevgep5 = getelementptr i8, i8* %call109, i64 %8
-  %scevgep56 = ptrtoint i8* %scevgep5 to i64
-  call void @llvm.ppc.mtctr.i64(i64 %scevgep56)
-  br label %for.body.116
-
-for.cond.cleanup:                                 ; preds = %if.end.138, %if.end.105
-  %int_part_ptr.0.lcssa = phi i8* [ %add.ptr106, %if.end.105 ], [ %int_part_ptr.1, %if.end.138 ]
-  %9 = bitcast [512 x i8]* %buf to i8*
-  %call142 = call i8* @halide_string_to_string(i8* %dst.addr.0, i8* %end, i8* %int_part_ptr.0.lcssa) #3
-  %call143 = call i8* @halide_string_to_string(i8* %call142, i8* %end, i8* getelementptr inbounds ([2 x i8], [2 x i8]* @.str.9.96, i64 0, i64 0)) #3
-  %call144 = call i8* @halide_int64_to_string(i8* %call143, i8* %end, i64 %fractional_part.2, i32 6) #3
-  call void @llvm.lifetime.end(i64 512, i8* %9) #0
-  br label %cleanup.148
-
-for.cond.cleanup.115:                             ; preds = %for.body.116
-  br i1 %cmp125, label %if.then.136, label %if.end.138
-
-for.body.116:                                     ; preds = %for.body.116, %for.body.116.preheader
-  %call109.pn = phi i8* [ %p.0251, %for.body.116 ], [ %call109, %for.body.116.preheader ]
-  %carry.0250 = phi i32 [ %carry.1, %for.body.116 ], [ 0, %for.body.116.preheader ]
-  %call109.pn2 = ptrtoint i8* %call109.pn to i64
-  %p.0251 = getelementptr inbounds i8, i8* %call109.pn, i64 -1
-  %scevgep3 = getelementptr i8, i8* inttoptr (i64 -1 to i8*), i64 %call109.pn2
-  %10 = load i8, i8* %scevgep3, align 1, !tbaa !10
-  %sub118 = add i8 %10, -48
-  %conv120 = sext i8 %sub118 to i32
-  %mul121 = shl nsw i32 %conv120, 1
-  %add122 = or i32 %mul121, %carry.0250
-  %11 = trunc i32 %add122 to i8
-  %cmp125 = icmp sgt i8 %11, 9
-  %sub128 = add nsw i32 %add122, 246
-  %carry.1 = zext i1 %cmp125 to i32
-  %new_digit.0.in = select i1 %cmp125, i32 %sub128, i32 %add122
-  %add133 = add nsw i32 %new_digit.0.in, 48
-  %conv134 = trunc i32 %add133 to i8
-  %scevgep = getelementptr i8, i8* inttoptr (i64 -1 to i8*), i64 %call109.pn2
-  store i8 %conv134, i8* %scevgep, align 1, !tbaa !10
-  %12 = call i1 @llvm.ppc.is.decremented.ctr.nonzero()
-  br i1 %12, label %for.body.116, label %for.cond.cleanup.115
-
-if.then.136:                                      ; preds = %for.cond.cleanup.115
-  %incdec.ptr137 = getelementptr inbounds i8, i8* %int_part_ptr.0253, i64 -1
-  store i8 49, i8* %incdec.ptr137, align 1, !tbaa !10
-  br label %if.end.138
-
-if.end.138:                                       ; preds = %if.then.136, %for.cond.cleanup.115, %for.cond.112.preheader
-  %int_part_ptr.1 = phi i8* [ %incdec.ptr137, %if.then.136 ], [ %call109, %for.cond.112.preheader ], [ %int_part_ptr.0253, %for.cond.cleanup.115 ]
-  %inc140 = add nuw nsw i32 %i.0255, 1
-  %exitcond = icmp eq i32 %inc140, %integer_exponent.0
-  br i1 %exitcond, label %for.cond.cleanup, label %for.cond.112.preheader
-
-cleanup.148:                                      ; preds = %for.cond.cleanup, %if.then.64, %if.end.59, %if.else.30, %if.then.28, %if.else.24, %if.then.22, %if.else.13, %if.then.11, %if.else, %if.then.6
-  %retval.1 = phi i8* [ %call7, %if.then.6 ], [ %call8, %if.else ], [ %call12, %if.then.11 ], [ %call14, %if.else.13 ], [ %call23, %if.then.22 ], [ %call25, %if.else.24 ], [ %call29, %if.then.28 ], [ %call31, %if.else.30 ], [ %call65, %if.then.64 ], [ %call61, %if.end.59 ], [ %call144, %for.cond.cleanup ]
-  %13 = bitcast i64* %bits to i8*
-  call void @llvm.lifetime.end(i64 8, i8* %13) #0
-  ret i8* %retval.1
-}
-
-; Function Attrs: nounwind
-declare i8* @memcpy(i8*, i8* nocapture readonly, i64) #1
-
-; Function Attrs: nounwind
-declare void @llvm.ppc.mtctr.i64(i64) #0
-
-; Function Attrs: nounwind
-declare i1 @llvm.ppc.is.decremented.ctr.nonzero() #0
-
-attributes #0 = { nounwind }
-attributes #1 = { nounwind "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "stack-protector-buffer-size"="8" "unsafe-fp-math"="false" "use-soft-float"="false" }
-attributes #2 = { nounwind }
-attributes #3 = { nounwind }
-
-!llvm.ident = !{!0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0, !0}
-!llvm.module.flags = !{!1, !2, !3}
-
-!0 = !{!"clang version 3.7.0 (branches/release_37 246867) (llvm/branches/release_37 246866)"}
-!1 = !{i32 2, !"halide_use_soft_float_abi", i32 0}
-!2 = !{i32 2, !"halide_mcpu", !"pwr8"}
-!3 = !{i32 2, !"halide_mattrs", !"+altivec,+vsx,+power8-altivec,+direct-move"}
-!4 = !{!5, !5, i64 0}
-!5 = !{!"double", !6, i64 0}
-!6 = !{!"omnipotent char", !7, i64 0}
-!7 = !{!"Simple C/C++ TBAA"}
-!8 = !{!9, !9, i64 0}
-!9 = !{!"long long", !6, i64 0}
-!10 = !{!6, !6, i64 0}
diff --git a/test/CodeGen/PowerPC/fp2int2fp-ppcfp128.ll b/test/CodeGen/PowerPC/fp2int2fp-ppcfp128.ll
deleted file mode 100644
index 7742ffe..0000000
--- a/test/CodeGen/PowerPC/fp2int2fp-ppcfp128.ll
+++ /dev/null
@@ -1,16 +0,0 @@
-; RUN: llc -mcpu=a2 < %s | FileCheck %s
-target datalayout = "E-m:e-i64:64-n32:64"
-target triple = "powerpc64-bgq-linux"
-
-define linkonce_odr double @test1() {
-entry:
-  %conv6.i.i = fptosi ppc_fp128 undef to i64
-  %conv.i = sitofp i64 %conv6.i.i to double
-  ret double %conv.i
-
-; CHECK-LABEL: @test1
-; CHECK: bl __fixtfdi
-; CHECK: fcfid
-; CHECK: blr
-}
-
diff --git a/test/CodeGen/PowerPC/no-rlwimi-trivial-commute.mir b/test/CodeGen/PowerPC/no-rlwimi-trivial-commute.mir
deleted file mode 100644
index 5c998d0..0000000
--- a/test/CodeGen/PowerPC/no-rlwimi-trivial-commute.mir
+++ /dev/null
@@ -1,92 +0,0 @@
-# RUN: llc -start-after=dead-mi-elimination -stop-after=twoaddressinstruction -o /dev/null %s | FileCheck %s
-
---- |
-  target datalayout = "E-m:e-i64:64-n32:64"
-  target triple = "powerpc64-unknown-linux-gnu"
-  
-  @d = global i32 15, align 4
-  @b = global i32* @d, align 8
-  @a = common global i32 0, align 4
-  
-  ; Function Attrs: nounwind
-  define signext i32 @main() #0 {
-  entry:
-    %0 = load i32*, i32** @b, align 8
-    %1 = load i32, i32* @a, align 4
-    %lnot = icmp eq i32 %1, 0
-    %lnot.ext = zext i1 %lnot to i32
-    %shr.i = lshr i32 2072, %lnot.ext
-    %call.lobit = lshr i32 %shr.i, 7
-    %2 = and i32 %call.lobit, 1
-    %3 = load i32, i32* %0, align 4
-    %or = or i32 %2, %3
-    store i32 %or, i32* %0, align 4
-    %4 = load i32, i32* @a, align 4
-    %lnot.1 = icmp eq i32 %4, 0
-    %lnot.ext.1 = zext i1 %lnot.1 to i32
-    %shr.i.1 = lshr i32 2072, %lnot.ext.1
-    %call.lobit.1 = lshr i32 %shr.i.1, 7
-    %5 = and i32 %call.lobit.1, 1
-    %or.1 = or i32 %5, %or
-    store i32 %or.1, i32* %0, align 4
-    ret i32 %or.1
-  }
-  
-  attributes #0 = { nounwind "target-cpu"="ppc64" }
-
-...
----
-name:            main
-alignment:       2
-exposesReturnsTwice: false
-hasInlineAsm:    false
-isSSA:           true
-tracksRegLiveness: true
-tracksSubRegLiveness: false
-registers:       
-  - { id: 0, class: g8rc_and_g8rc_nox0 }
-  - { id: 1, class: g8rc_and_g8rc_nox0 }
-  - { id: 2, class: gprc }
-  - { id: 3, class: gprc }
-  - { id: 4, class: gprc }
-  - { id: 5, class: g8rc_and_g8rc_nox0 }
-  - { id: 6, class: g8rc_and_g8rc_nox0 }
-  - { id: 7, class: gprc }
-  - { id: 8, class: gprc }
-  - { id: 9, class: gprc }
-  - { id: 10, class: g8rc }
-frameInfo:       
-  isFrameAddressTaken: false
-  isReturnAddressTaken: false
-  hasStackMap:     false
-  hasPatchPoint:   false
-  stackSize:       0
-  offsetAdjustment: 0
-  maxAlignment:    0
-  adjustsStack:    false
-  hasCalls:        false
-  maxCallFrameSize: 0
-  hasOpaqueSPAdjustment: false
-  hasVAStart:      false
-  hasMustTailInVarArgFunc: false
-body:             |
-  bb.0.entry:
-    liveins: %x2
-
-    %0 = ADDIStocHA %x2, @b
-    %1 = LD target-flags(ppc-toc-lo) @b, killed %0 :: (load 8 from @b)
-    %2 = LWZ 0, %1 :: (load 4 from %ir.0)
-    %3 = LI 0
-    %4 = RLWIMI %3, killed %2, 0, 0, 31
-    ; CHECK-LABEL: name: main
-    ; CHECK: %[[REG1:[0-9]+]] = LI 0
-    ; CHECK: %[[REG2:[0-9]+]] = COPY %[[REG1]]
-    ; CHECK: %[[REG2]] = RLWIMI %[[REG2]], killed %2, 0, 0, 31
-    %8 = RLWIMI %3, %4, 0, 0, 31
-    STW %4, 0, %1 :: (store 4 into %ir.0)
-    %10 = EXTSW_32_64 %8
-    STW %8, 0, %1 :: (store 4 into %ir.0)
-    %x3 = COPY %10
-    BLR8 implicit %x3, implicit %lr8, implicit %rm
-
-...
diff --git a/test/CodeGen/PowerPC/p8altivec-shuffles-pred.ll b/test/CodeGen/PowerPC/p8altivec-shuffles-pred.ll
deleted file mode 100644
index 052f556..0000000
--- a/test/CodeGen/PowerPC/p8altivec-shuffles-pred.ll
+++ /dev/null
@@ -1,28 +0,0 @@
-; RUN: llc < %s | FileCheck %s
-target datalayout = "E-m:e-i64:64-n32:64"
-target triple = "powerpc64-unknown-linux-gnu"
-
-; Function Attrs: nounwind
-define <2 x i32> @test1(<4 x i32> %wide.vec) #0 {
-entry:
-  %strided.vec = shufflevector <4 x i32> %wide.vec, <4 x i32> undef, <2 x i32> <i32 0, i32 2>
-  ret <2 x i32> %strided.vec
-
-; CHECK-LABEL: @test1
-; CHECK: vsldoi 2, 2, 2, 12
-; CHECK: blr
-}
-
-; Function Attrs: nounwind
-define <16 x i8> @test2(<16 x i8> %wide.vec) #0 {
-entry:
-  %strided.vec = shufflevector <16 x i8> %wide.vec, <16 x i8> undef, <16 x i32> <i32 undef, i32 undef, i32 undef, i32 undef, i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef, i32 8, i32 9, i32 10, i32 11>
-  ret <16 x i8> %strided.vec
-
-; CHECK-LABEL: @test2
-; CHECK: vsldoi 2, 2, 2, 12
-; CHECK: blr
-}
-
-attributes #0 = { nounwind "target-cpu"="pwr7" }
-
diff --git a/test/CodeGen/PowerPC/pr24546.ll b/test/CodeGen/PowerPC/pr24546.ll
deleted file mode 100644
index 3bb638a..0000000
--- a/test/CodeGen/PowerPC/pr24546.ll
+++ /dev/null
@@ -1,116 +0,0 @@
-; RUN: llc -mcpu=pwr8 -mtriple=powerpc64le-unknown-linux-gnu < %s
-
-; Verify that we no longer crash in VSX swap removal when debug values
-; are in the code stream.
-
-@php_intpow10.powers = external unnamed_addr constant [23 x double], align 8
-
-; Function Attrs: nounwind
-define double @_php_math_round(double %value, i32 signext %places, i32 signext %mode) #0 {
-entry:
-  br i1 undef, label %if.then, label %if.else, !dbg !32
-
-if.then:                                          ; preds = %entry
-  %conv = sitofp i32 undef to double, !dbg !34
-  br i1 undef, label %if.then.i, label %if.end.i, !dbg !36
-
-if.then.i:                                        ; preds = %if.then
-  %call.i = tail call double @pow(double 1.000000e+01, double undef) #3, !dbg !39
-  br label %php_intpow10.exit, !dbg !41
-
-if.end.i:                                         ; preds = %if.then
-  %0 = load double, double* undef, align 8, !dbg !42, !tbaa !43
-  br label %php_intpow10.exit, !dbg !47
-
-php_intpow10.exit:                                ; preds = %if.end.i, %if.then.i
-  %retval.0.i = phi double [ %call.i, %if.then.i ], [ %0, %if.end.i ], !dbg !48
-  tail call void @llvm.dbg.value(metadata double %retval.0.i, i64 0, metadata !15, metadata !49), !dbg !50
-  %div = fdiv double %conv, %retval.0.i, !dbg !51
-  br label %if.end.15, !dbg !52
-
-if.else:                                          ; preds = %entry
-  %mul = fmul double %value, undef, !dbg !53
-  br label %if.end.15
-
-if.end.15:                                        ; preds = %if.else, %php_intpow10.exit
-  %tmp_value.1 = phi double [ %div, %php_intpow10.exit ], [ %mul, %if.else ]
-  ret double %tmp_value.1, !dbg !57
-}
-
-declare signext i32 @php_intlog10abs(...) #1
-
-declare signext i32 @php_round_helper(...) #1
-
-; Function Attrs: nounwind
-declare double @pow(double, double) #0
-
-; Function Attrs: nounwind readnone
-declare void @llvm.dbg.value(metadata, i64, metadata, metadata) #2
-
-attributes #0 = { nounwind "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="ppc64le" "target-features"="+altivec,+bpermd,+crypto,+direct-move,+extdiv,+power8-vector,+vsx,-qpx" "unsafe-fp-math"="false" "use-soft-float"="false" }
-attributes #1 = { "disable-tail-calls"="false" "less-precise-fpmad"="false" "no-frame-pointer-elim"="true" "no-frame-pointer-elim-non-leaf" "no-infs-fp-math"="false" "no-nans-fp-math"="false" "stack-protector-buffer-size"="8" "target-cpu"="ppc64le" "target-features"="+altivec,+bpermd,+crypto,+direct-move,+extdiv,+power8-vector,+vsx,-qpx" "unsafe-fp-math"="false" "use-soft-float"="false" }
-attributes #2 = { nounwind readnone }
-attributes #3 = { nounwind }
-
-!llvm.dbg.cu = !{!0}
-!llvm.module.flags = !{!29, !30}
-!llvm.ident = !{!31}
-
-!0 = distinct !DICompileUnit(language: DW_LANG_C99, file: !1, producer: "clang version 3.8.0 (git://github.com/llvm-mirror/clang.git e0848b6353721eb1b278a5bbea257bbf6316251e) (git://github.com/llvm-mirror/llvm.git 8724a428dfd5e78d7865bb01783708e83f9ed128)", isOptimized: true, runtimeVersion: 0, emissionKind: 1, enums: !2, retainedTypes: !3, subprograms: !5, globals: !23)
-!1 = !DIFile(filename: "testcase.i", directory: "/tmp/glibc.build")
-!2 = !{}
-!3 = !{!4}
-!4 = !DIBasicType(name: "double", size: 64, align: 64, encoding: DW_ATE_float)
-!5 = !{!6, !18}
-!6 = !DISubprogram(name: "_php_math_round", scope: !1, file: !1, line: 15, type: !7, isLocal: false, isDefinition: true, scopeLine: 16, flags: DIFlagPrototyped, isOptimized: true, function: double (double, i32, i32)* @_php_math_round, variables: !10)
-!7 = !DISubroutineType(types: !8)
-!8 = !{!4, !4, !9, !9}
-!9 = !DIBasicType(name: "int", size: 32, align: 32, encoding: DW_ATE_signed)
-!10 = !{!11, !12, !13, !14, !15, !16, !17}
-!11 = !DILocalVariable(tag: DW_TAG_arg_variable, name: "value", arg: 1, scope: !6, file: !1, line: 15, type: !4)
-!12 = !DILocalVariable(tag: DW_TAG_arg_variable, name: "places", arg: 2, scope: !6, file: !1, line: 15, type: !9)
-!13 = !DILocalVariable(tag: DW_TAG_arg_variable, name: "mode", arg: 3, scope: !6, file: !1, line: 15, type: !9)
-!14 = !DILocalVariable(tag: DW_TAG_auto_variable, name: "f1", scope: !6, file: !1, line: 17, type: !4)
-!15 = !DILocalVariable(tag: DW_TAG_auto_variable, name: "f2", scope: !6, file: !1, line: 17, type: !4)
-!16 = !DILocalVariable(tag: DW_TAG_auto_variable, name: "tmp_value", scope: !6, file: !1, line: 18, type: !4)
-!17 = !DILocalVariable(tag: DW_TAG_auto_variable, name: "precision_places", scope: !6, file: !1, line: 19, type: !9)
-!18 = !DISubprogram(name: "php_intpow10", scope: !1, file: !1, line: 1, type: !19, isLocal: true, isDefinition: true, scopeLine: 2, flags: DIFlagPrototyped, isOptimized: true, variables: !21)
-!19 = !DISubroutineType(types: !20)
-!20 = !{!4, !9}
-!21 = !{!22}
-!22 = !DILocalVariable(tag: DW_TAG_auto_variable, name: "power", arg: 1, scope: !18, file: !1, line: 1, type: !9)
-!23 = !{!24}
-!24 = !DIGlobalVariable(name: "powers", scope: !18, file: !1, line: 3, type: !25, isLocal: true, isDefinition: true, variable: [23 x double]* @php_intpow10.powers)
-!25 = !DICompositeType(tag: DW_TAG_array_type, baseType: !26, size: 1472, align: 64, elements: !27)
-!26 = !DIDerivedType(tag: DW_TAG_const_type, baseType: !4)
-!27 = !{!28}
-!28 = !DISubrange(count: 23)
-!29 = !{i32 2, !"Dwarf Version", i32 4}
-!30 = !{i32 2, !"Debug Info Version", i32 3}
-!31 = !{!"clang version 3.8.0 (git://github.com/llvm-mirror/clang.git e0848b6353721eb1b278a5bbea257bbf6316251e) (git://github.com/llvm-mirror/llvm.git 8724a428dfd5e78d7865bb01783708e83f9ed128)"}
-!32 = !DILocation(line: 21, column: 32, scope: !33)
-!33 = distinct !DILexicalBlock(scope: !6, file: !1, line: 21, column: 6)
-!34 = !DILocation(line: 22, column: 15, scope: !35)
-!35 = distinct !DILexicalBlock(scope: !33, file: !1, line: 21, column: 67)
-!36 = !DILocation(line: 8, column: 16, scope: !37, inlinedAt: !38)
-!37 = distinct !DILexicalBlock(scope: !18, file: !1, line: 8, column: 6)
-!38 = distinct !DILocation(line: 23, column: 8, scope: !35)
-!39 = !DILocation(line: 9, column: 10, scope: !40, inlinedAt: !38)
-!40 = distinct !DILexicalBlock(scope: !37, file: !1, line: 8, column: 31)
-!41 = !DILocation(line: 9, column: 3, scope: !40, inlinedAt: !38)
-!42 = !DILocation(line: 11, column: 9, scope: !18, inlinedAt: !38)
-!43 = !{!44, !44, i64 0}
-!44 = !{!"double", !45, i64 0}
-!45 = !{!"omnipotent char", !46, i64 0}
-!46 = !{!"Simple C/C++ TBAA"}
-!47 = !DILocation(line: 11, column: 2, scope: !18, inlinedAt: !38)
-!48 = !DILocation(line: 23, column: 8, scope: !35)
-!49 = !DIExpression()
-!50 = !DILocation(line: 17, column: 13, scope: !6)
-!51 = !DILocation(line: 24, column: 25, scope: !35)
-!52 = !DILocation(line: 25, column: 2, scope: !35)
-!53 = !DILocation(line: 27, column: 22, scope: !54)
-!54 = distinct !DILexicalBlock(scope: !55, file: !1, line: 26, column: 20)
-!55 = distinct !DILexicalBlock(scope: !56, file: !1, line: 26, column: 7)
-!56 = distinct !DILexicalBlock(scope: !33, file: !1, line: 25, column: 9)
-!57 = !DILocation(line: 32, column: 2, scope: !6)
diff --git a/test/CodeGen/PowerPC/pr25157.ll b/test/CodeGen/PowerPC/pr25157.ll
deleted file mode 100644
index 7137d67..0000000
--- a/test/CodeGen/PowerPC/pr25157.ll
+++ /dev/null
@@ -1,58 +0,0 @@
-; RUN: llc -mcpu=pwr8 -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s
-
-; Verify correct generation of an lxsspx rather than an invalid optimization
-; to lxvdsx.  Bugpoint-reduced test from Eric Schweitz.
-
-%struct.BSS38.51.4488.9911.14348.16813.20264.24701.28152.31603.35054.39491.44914.45407.46393.46886.47872.49351.49844.50830.51323.52309.53295.53788.54281.55267.55760.59211.61625 = type <{ [28 x i8] }>
-%struct_main1_2_.491.4928.10351.14788.17253.20704.25141.28592.32043.35494.39931.45354.45847.46833.47326.48312.49791.50284.51270.51763.52749.53735.54228.54721.55707.56200.59651.61626 = type <{ [64 x i8] }>
-
-@.BSS38 = external global %struct.BSS38.51.4488.9911.14348.16813.20264.24701.28152.31603.35054.39491.44914.45407.46393.46886.47872.49351.49844.50830.51323.52309.53295.53788.54281.55267.55760.59211.61625, align 32
-@_main1_2_ = external global %struct_main1_2_.491.4928.10351.14788.17253.20704.25141.28592.32043.35494.39931.45354.45847.46833.47326.48312.49791.50284.51270.51763.52749.53735.54228.54721.55707.56200.59651.61626, section ".comm", align 16
-
-define void @aercalc_() {
-L.entry:
-  br i1 undef, label %L.LB38_2426, label %L.LB38_2911
-
-L.LB38_2911:
-  br i1 undef, label %L.LB38_2140, label %L.LB38_2640
-
-L.LB38_2640:
-  unreachable
-
-L.LB38_2426:
-  br i1 undef, label %L.LB38_2438, label %L.LB38_2920
-
-L.LB38_2920:
-  br i1 undef, label %L.LB38_2438, label %L.LB38_2921
-
-L.LB38_2921:
-  br label %L.LB38_2140
-
-L.LB38_2140:
-  ret void
-
-L.LB38_2438:
-  br i1 undef, label %L.LB38_2451, label %L.LB38_2935
-
-L.LB38_2935:
-  br i1 undef, label %L.LB38_2451, label %L.LB38_2936
-
-L.LB38_2936:
-  unreachable
-
-L.LB38_2451:
-  br i1 undef, label %L.LB38_2452, label %L.LB38_2937
-
-L.LB38_2937:
-  unreachable
-
-L.LB38_2452:
-  %0 = load float, float* bitcast (i8* getelementptr inbounds (%struct.BSS38.51.4488.9911.14348.16813.20264.24701.28152.31603.35054.39491.44914.45407.46393.46886.47872.49351.49844.50830.51323.52309.53295.53788.54281.55267.55760.59211.61625, %struct.BSS38.51.4488.9911.14348.16813.20264.24701.28152.31603.35054.39491.44914.45407.46393.46886.47872.49351.49844.50830.51323.52309.53295.53788.54281.55267.55760.59211.61625* @.BSS38, i64 0, i32 0, i64 16) to float*), align 16
-  %1 = fpext float %0 to double
-  %2 = insertelement <2 x double> undef, double %1, i32 1
-  store <2 x double> %2, <2 x double>* bitcast (i8* getelementptr inbounds (%struct_main1_2_.491.4928.10351.14788.17253.20704.25141.28592.32043.35494.39931.45354.45847.46833.47326.48312.49791.50284.51270.51763.52749.53735.54228.54721.55707.56200.59651.61626, %struct_main1_2_.491.4928.10351.14788.17253.20704.25141.28592.32043.35494.39931.45354.45847.46833.47326.48312.49791.50284.51270.51763.52749.53735.54228.54721.55707.56200.59651.61626* @_main1_2_, i64 0, i32 0, i64 32) to <2 x double>*), align 16
-  unreachable
-}
-
-; CHECK-LABEL: @aercalc_
-; CHECK: lxsspx
diff --git a/test/CodeGen/PowerPC/rlwimi-and-or-bits.ll b/test/CodeGen/PowerPC/rlwimi-and-or-bits.ll
deleted file mode 100644
index a74bc72..0000000
--- a/test/CodeGen/PowerPC/rlwimi-and-or-bits.ll
+++ /dev/null
@@ -1,27 +0,0 @@
-; RUN: llc < %s | FileCheck %s
-target datalayout = "E-m:e-i64:64-n32:64"
-target triple = "powerpc64-unknown-linux-gnu"
-
-@m = external global i32, align 4
-
-; Function Attrs: nounwind
-define signext i32 @main() #0 {
-entry:
-
-; CHECK-LABEL: @main
-; CHECK-NOT: rlwimi
-; CHECK: andi
-
-  %0 = load i32, i32* @m, align 4
-  %or = or i32 %0, 250
-  store i32 %or, i32* @m, align 4
-  %and = and i32 %or, 249
-  %sub.i = sub i32 %and, 0
-  %sext = shl i32 %sub.i, 24
-  %conv = ashr exact i32 %sext, 24
-  ret i32 %conv
-}
-
-attributes #0 = { nounwind "target-cpu"="pwr7" }
-attributes #1 = { nounwind }
-
diff --git a/test/CodeGen/PowerPC/select-i1-vs-i1.ll b/test/CodeGen/PowerPC/select-i1-vs-i1.ll
deleted file mode 100644
index 6dabbaa..0000000
--- a/test/CodeGen/PowerPC/select-i1-vs-i1.ll
+++ /dev/null
@@ -1,1685 +0,0 @@
-; RUN: llc < %s | FileCheck %s
-target datalayout = "E-m:e-i64:64-n32:64"
-target triple = "powerpc64-unknown-linux-gnu"
-
-; FIXME: We should check the operands to the cr* logical operation itself, but
-; unfortunately, FileCheck does not yet understand how to do arithmetic, so we
-; can't do so without introducing a register-allocation dependency.
-
-define signext i32 @testi32slt(i32 signext %c1, i32 signext %c2, i32 signext %c3, i32 signext %c4, i32 signext %a1, i32 signext %a2) #0 {
-entry:
-  %cmp1 = icmp eq i32 %c3, %c4
-  %cmp3tmp = icmp eq i32 %c1, %c2
-  %cmp3 = icmp slt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i32 %a1, i32 %a2
-  ret i32 %cond
-
-; CHECK-LABEL: @testi32slt
-; CHECK-DAG: cmpw {{[0-9]+}}, 5, 6
-; CHECK-DAG: cmpw {{[0-9]+}}, 3, 4
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define signext i32 @testi32ult(i32 signext %c1, i32 signext %c2, i32 signext %c3, i32 signext %c4, i32 signext %a1, i32 signext %a2) #0 {
-entry:
-  %cmp1 = icmp eq i32 %c3, %c4
-  %cmp3tmp = icmp eq i32 %c1, %c2
-  %cmp3 = icmp ult i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i32 %a1, i32 %a2
-  ret i32 %cond
-
-; CHECK-LABEL: @testi32ult
-; CHECK-DAG: cmpw {{[0-9]+}}, 5, 6
-; CHECK-DAG: cmpw {{[0-9]+}}, 3, 4
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define signext i32 @testi32sle(i32 signext %c1, i32 signext %c2, i32 signext %c3, i32 signext %c4, i32 signext %a1, i32 signext %a2) #0 {
-entry:
-  %cmp1 = icmp eq i32 %c3, %c4
-  %cmp3tmp = icmp eq i32 %c1, %c2
-  %cmp3 = icmp sle i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i32 %a1, i32 %a2
-  ret i32 %cond
-
-; CHECK-LABEL: @testi32sle
-; CHECK-DAG: cmpw {{[0-9]+}}, 5, 6
-; CHECK-DAG: cmpw {{[0-9]+}}, 3, 4
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define signext i32 @testi32ule(i32 signext %c1, i32 signext %c2, i32 signext %c3, i32 signext %c4, i32 signext %a1, i32 signext %a2) #0 {
-entry:
-  %cmp1 = icmp eq i32 %c3, %c4
-  %cmp3tmp = icmp eq i32 %c1, %c2
-  %cmp3 = icmp ule i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i32 %a1, i32 %a2
-  ret i32 %cond
-
-; CHECK-LABEL: @testi32ule
-; CHECK-DAG: cmpw {{[0-9]+}}, 5, 6
-; CHECK-DAG: cmpw {{[0-9]+}}, 3, 4
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define signext i32 @testi32eq(i32 signext %c1, i32 signext %c2, i32 signext %c3, i32 signext %c4, i32 signext %a1, i32 signext %a2) #0 {
-entry:
-  %cmp1 = icmp eq i32 %c3, %c4
-  %cmp3tmp = icmp eq i32 %c1, %c2
-  %cmp3 = icmp eq i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i32 %a1, i32 %a2
-  ret i32 %cond
-
-; CHECK-LABEL: @testi32eq
-; CHECK-DAG: cmpw {{[0-9]+}}, 5, 6
-; CHECK-DAG: cmpw {{[0-9]+}}, 3, 4
-; CHECK: creqv [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define signext i32 @testi32sge(i32 signext %c1, i32 signext %c2, i32 signext %c3, i32 signext %c4, i32 signext %a1, i32 signext %a2) #0 {
-entry:
-  %cmp1 = icmp eq i32 %c3, %c4
-  %cmp3tmp = icmp eq i32 %c1, %c2
-  %cmp3 = icmp sge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i32 %a1, i32 %a2
-  ret i32 %cond
-
-; CHECK-LABEL: @testi32sge
-; CHECK-DAG: cmpw {{[0-9]+}}, 5, 6
-; CHECK-DAG: cmpw {{[0-9]+}}, 3, 4
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define signext i32 @testi32uge(i32 signext %c1, i32 signext %c2, i32 signext %c3, i32 signext %c4, i32 signext %a1, i32 signext %a2) #0 {
-entry:
-  %cmp1 = icmp eq i32 %c3, %c4
-  %cmp3tmp = icmp eq i32 %c1, %c2
-  %cmp3 = icmp uge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i32 %a1, i32 %a2
-  ret i32 %cond
-
-; CHECK-LABEL: @testi32uge
-; CHECK-DAG: cmpw {{[0-9]+}}, 5, 6
-; CHECK-DAG: cmpw {{[0-9]+}}, 3, 4
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define signext i32 @testi32sgt(i32 signext %c1, i32 signext %c2, i32 signext %c3, i32 signext %c4, i32 signext %a1, i32 signext %a2) #0 {
-entry:
-  %cmp1 = icmp eq i32 %c3, %c4
-  %cmp3tmp = icmp eq i32 %c1, %c2
-  %cmp3 = icmp sgt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i32 %a1, i32 %a2
-  ret i32 %cond
-
-; CHECK-LABEL: @testi32sgt
-; CHECK-DAG: cmpw {{[0-9]+}}, 5, 6
-; CHECK-DAG: cmpw {{[0-9]+}}, 3, 4
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define signext i32 @testi32ugt(i32 signext %c1, i32 signext %c2, i32 signext %c3, i32 signext %c4, i32 signext %a1, i32 signext %a2) #0 {
-entry:
-  %cmp1 = icmp eq i32 %c3, %c4
-  %cmp3tmp = icmp eq i32 %c1, %c2
-  %cmp3 = icmp ugt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i32 %a1, i32 %a2
-  ret i32 %cond
-
-; CHECK-LABEL: @testi32ugt
-; CHECK-DAG: cmpw {{[0-9]+}}, 5, 6
-; CHECK-DAG: cmpw {{[0-9]+}}, 3, 4
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define signext i32 @testi32ne(i32 signext %c1, i32 signext %c2, i32 signext %c3, i32 signext %c4, i32 signext %a1, i32 signext %a2) #0 {
-entry:
-  %cmp1 = icmp eq i32 %c3, %c4
-  %cmp3tmp = icmp eq i32 %c1, %c2
-  %cmp3 = icmp ne i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i32 %a1, i32 %a2
-  ret i32 %cond
-
-; CHECK-LABEL: @testi32ne
-; CHECK-DAG: cmpw {{[0-9]+}}, 5, 6
-; CHECK-DAG: cmpw {{[0-9]+}}, 3, 4
-; CHECK: crxor [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define i64 @testi64slt(i64 %c1, i64 %c2, i64 %c3, i64 %c4, i64 %a1, i64 %a2) #0 {
-entry:
-  %cmp1 = icmp eq i64 %c3, %c4
-  %cmp3tmp = icmp eq i64 %c1, %c2
-  %cmp3 = icmp slt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i64 %a1, i64 %a2
-  ret i64 %cond
-
-; CHECK-LABEL: @testi64slt
-; CHECK-DAG: cmpd {{([0-9]+, )?}}5, 6
-; CHECK-DAG: cmpd {{([0-9]+, )?}}3, 4
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define i64 @testi64ult(i64 %c1, i64 %c2, i64 %c3, i64 %c4, i64 %a1, i64 %a2) #0 {
-entry:
-  %cmp1 = icmp eq i64 %c3, %c4
-  %cmp3tmp = icmp eq i64 %c1, %c2
-  %cmp3 = icmp ult i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i64 %a1, i64 %a2
-  ret i64 %cond
-
-; CHECK-LABEL: @testi64ult
-; CHECK-DAG: cmpd {{([0-9]+, )?}}5, 6
-; CHECK-DAG: cmpd {{([0-9]+, )?}}3, 4
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define i64 @testi64sle(i64 %c1, i64 %c2, i64 %c3, i64 %c4, i64 %a1, i64 %a2) #0 {
-entry:
-  %cmp1 = icmp eq i64 %c3, %c4
-  %cmp3tmp = icmp eq i64 %c1, %c2
-  %cmp3 = icmp sle i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i64 %a1, i64 %a2
-  ret i64 %cond
-
-; CHECK-LABEL: @testi64sle
-; CHECK-DAG: cmpd {{([0-9]+, )?}}5, 6
-; CHECK-DAG: cmpd {{([0-9]+, )?}}3, 4
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define i64 @testi64ule(i64 %c1, i64 %c2, i64 %c3, i64 %c4, i64 %a1, i64 %a2) #0 {
-entry:
-  %cmp1 = icmp eq i64 %c3, %c4
-  %cmp3tmp = icmp eq i64 %c1, %c2
-  %cmp3 = icmp ule i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i64 %a1, i64 %a2
-  ret i64 %cond
-
-; CHECK-LABEL: @testi64ule
-; CHECK-DAG: cmpd {{([0-9]+, )?}}5, 6
-; CHECK-DAG: cmpd {{([0-9]+, )?}}3, 4
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define i64 @testi64eq(i64 %c1, i64 %c2, i64 %c3, i64 %c4, i64 %a1, i64 %a2) #0 {
-entry:
-  %cmp1 = icmp eq i64 %c3, %c4
-  %cmp3tmp = icmp eq i64 %c1, %c2
-  %cmp3 = icmp eq i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i64 %a1, i64 %a2
-  ret i64 %cond
-
-; CHECK-LABEL: @testi64eq
-; CHECK-DAG: cmpd {{([0-9]+, )?}}5, 6
-; CHECK-DAG: cmpd {{([0-9]+, )?}}3, 4
-; CHECK: creqv [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define i64 @testi64sge(i64 %c1, i64 %c2, i64 %c3, i64 %c4, i64 %a1, i64 %a2) #0 {
-entry:
-  %cmp1 = icmp eq i64 %c3, %c4
-  %cmp3tmp = icmp eq i64 %c1, %c2
-  %cmp3 = icmp sge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i64 %a1, i64 %a2
-  ret i64 %cond
-
-; CHECK-LABEL: @testi64sge
-; CHECK-DAG: cmpd {{([0-9]+, )?}}5, 6
-; CHECK-DAG: cmpd {{([0-9]+, )?}}3, 4
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define i64 @testi64uge(i64 %c1, i64 %c2, i64 %c3, i64 %c4, i64 %a1, i64 %a2) #0 {
-entry:
-  %cmp1 = icmp eq i64 %c3, %c4
-  %cmp3tmp = icmp eq i64 %c1, %c2
-  %cmp3 = icmp uge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i64 %a1, i64 %a2
-  ret i64 %cond
-
-; CHECK-LABEL: @testi64uge
-; CHECK-DAG: cmpd {{([0-9]+, )?}}5, 6
-; CHECK-DAG: cmpd {{([0-9]+, )?}}3, 4
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define i64 @testi64sgt(i64 %c1, i64 %c2, i64 %c3, i64 %c4, i64 %a1, i64 %a2) #0 {
-entry:
-  %cmp1 = icmp eq i64 %c3, %c4
-  %cmp3tmp = icmp eq i64 %c1, %c2
-  %cmp3 = icmp sgt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i64 %a1, i64 %a2
-  ret i64 %cond
-
-; CHECK-LABEL: @testi64sgt
-; CHECK-DAG: cmpd {{([0-9]+, )?}}5, 6
-; CHECK-DAG: cmpd {{([0-9]+, )?}}3, 4
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define i64 @testi64ugt(i64 %c1, i64 %c2, i64 %c3, i64 %c4, i64 %a1, i64 %a2) #0 {
-entry:
-  %cmp1 = icmp eq i64 %c3, %c4
-  %cmp3tmp = icmp eq i64 %c1, %c2
-  %cmp3 = icmp ugt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i64 %a1, i64 %a2
-  ret i64 %cond
-
-; CHECK-LABEL: @testi64ugt
-; CHECK-DAG: cmpd {{([0-9]+, )?}}5, 6
-; CHECK-DAG: cmpd {{([0-9]+, )?}}3, 4
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define i64 @testi64ne(i64 %c1, i64 %c2, i64 %c3, i64 %c4, i64 %a1, i64 %a2) #0 {
-entry:
-  %cmp1 = icmp eq i64 %c3, %c4
-  %cmp3tmp = icmp eq i64 %c1, %c2
-  %cmp3 = icmp ne i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, i64 %a1, i64 %a2
-  ret i64 %cond
-
-; CHECK-LABEL: @testi64ne
-; CHECK-DAG: cmpd {{([0-9]+, )?}}5, 6
-; CHECK-DAG: cmpd {{([0-9]+, )?}}3, 4
-; CHECK: crxor [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: isel 3, 7, 8, [[REG1]]
-; CHECK: blr
-}
-
-define float @testfloatslt(float %c1, float %c2, float %c3, float %c4, float %a1, float %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp slt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, float %a1, float %a2
-  ret float %cond
-
-; CHECK-LABEL: @testfloatslt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define float @testfloatult(float %c1, float %c2, float %c3, float %c4, float %a1, float %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ult i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, float %a1, float %a2
-  ret float %cond
-
-; CHECK-LABEL: @testfloatult
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define float @testfloatsle(float %c1, float %c2, float %c3, float %c4, float %a1, float %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sle i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, float %a1, float %a2
-  ret float %cond
-
-; CHECK-LABEL: @testfloatsle
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define float @testfloatule(float %c1, float %c2, float %c3, float %c4, float %a1, float %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ule i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, float %a1, float %a2
-  ret float %cond
-
-; CHECK-LABEL: @testfloatule
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define float @testfloateq(float %c1, float %c2, float %c3, float %c4, float %a1, float %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp eq i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, float %a1, float %a2
-  ret float %cond
-
-; CHECK-LABEL: @testfloateq
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: creqv [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define float @testfloatsge(float %c1, float %c2, float %c3, float %c4, float %a1, float %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, float %a1, float %a2
-  ret float %cond
-
-; CHECK-LABEL: @testfloatsge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define float @testfloatuge(float %c1, float %c2, float %c3, float %c4, float %a1, float %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp uge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, float %a1, float %a2
-  ret float %cond
-
-; CHECK-LABEL: @testfloatuge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define float @testfloatsgt(float %c1, float %c2, float %c3, float %c4, float %a1, float %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sgt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, float %a1, float %a2
-  ret float %cond
-
-; CHECK-LABEL: @testfloatsgt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define float @testfloatugt(float %c1, float %c2, float %c3, float %c4, float %a1, float %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ugt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, float %a1, float %a2
-  ret float %cond
-
-; CHECK-LABEL: @testfloatugt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define float @testfloatne(float %c1, float %c2, float %c3, float %c4, float %a1, float %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ne i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, float %a1, float %a2
-  ret float %cond
-
-; CHECK-LABEL: @testfloatne
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crxor [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define double @testdoubleslt(double %c1, double %c2, double %c3, double %c4, double %a1, double %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq double %c3, %c4
-  %cmp3tmp = fcmp oeq double %c1, %c2
-  %cmp3 = icmp slt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, double %a1, double %a2
-  ret double %cond
-
-; CHECK-LABEL: @testdoubleslt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define double @testdoubleult(double %c1, double %c2, double %c3, double %c4, double %a1, double %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq double %c3, %c4
-  %cmp3tmp = fcmp oeq double %c1, %c2
-  %cmp3 = icmp ult i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, double %a1, double %a2
-  ret double %cond
-
-; CHECK-LABEL: @testdoubleult
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define double @testdoublesle(double %c1, double %c2, double %c3, double %c4, double %a1, double %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq double %c3, %c4
-  %cmp3tmp = fcmp oeq double %c1, %c2
-  %cmp3 = icmp sle i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, double %a1, double %a2
-  ret double %cond
-
-; CHECK-LABEL: @testdoublesle
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define double @testdoubleule(double %c1, double %c2, double %c3, double %c4, double %a1, double %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq double %c3, %c4
-  %cmp3tmp = fcmp oeq double %c1, %c2
-  %cmp3 = icmp ule i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, double %a1, double %a2
-  ret double %cond
-
-; CHECK-LABEL: @testdoubleule
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define double @testdoubleeq(double %c1, double %c2, double %c3, double %c4, double %a1, double %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq double %c3, %c4
-  %cmp3tmp = fcmp oeq double %c1, %c2
-  %cmp3 = icmp eq i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, double %a1, double %a2
-  ret double %cond
-
-; CHECK-LABEL: @testdoubleeq
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: creqv [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define double @testdoublesge(double %c1, double %c2, double %c3, double %c4, double %a1, double %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq double %c3, %c4
-  %cmp3tmp = fcmp oeq double %c1, %c2
-  %cmp3 = icmp sge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, double %a1, double %a2
-  ret double %cond
-
-; CHECK-LABEL: @testdoublesge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define double @testdoubleuge(double %c1, double %c2, double %c3, double %c4, double %a1, double %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq double %c3, %c4
-  %cmp3tmp = fcmp oeq double %c1, %c2
-  %cmp3 = icmp uge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, double %a1, double %a2
-  ret double %cond
-
-; CHECK-LABEL: @testdoubleuge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define double @testdoublesgt(double %c1, double %c2, double %c3, double %c4, double %a1, double %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq double %c3, %c4
-  %cmp3tmp = fcmp oeq double %c1, %c2
-  %cmp3 = icmp sgt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, double %a1, double %a2
-  ret double %cond
-
-; CHECK-LABEL: @testdoublesgt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define double @testdoubleugt(double %c1, double %c2, double %c3, double %c4, double %a1, double %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq double %c3, %c4
-  %cmp3tmp = fcmp oeq double %c1, %c2
-  %cmp3 = icmp ugt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, double %a1, double %a2
-  ret double %cond
-
-; CHECK-LABEL: @testdoubleugt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define double @testdoublene(double %c1, double %c2, double %c3, double %c4, double %a1, double %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq double %c3, %c4
-  %cmp3tmp = fcmp oeq double %c1, %c2
-  %cmp3 = icmp ne i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, double %a1, double %a2
-  ret double %cond
-
-; CHECK-LABEL: @testdoublene
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crxor [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: fmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: fmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testv4floatslt(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp slt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; FIXME: This test (and the other v4f32 tests) should use the same bclr
-; technique as the v2f64 tests below.
-
-; CHECK-LABEL: @testv4floatslt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK-DAG: xxlor [[REG2:[0-9]+]], 34, 34
-; CHECK-DAG: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: xxlor [[REG2]], 35, 35
-; CHECK: .LBB[[BB]]:
-; CHECK: xxlor 34, [[REG2]], [[REG2]]
-; CHECK: blr
-}
-
-define <4 x float> @testv4floatult(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ult i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testv4floatult
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK-DAG: xxlor [[REG2:[0-9]+]], 34, 34
-; CHECK-DAG: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: xxlor [[REG2]], 35, 35
-; CHECK: .LBB[[BB]]:
-; CHECK: xxlor 34, [[REG2]], [[REG2]]
-; CHECK: blr
-}
-
-define <4 x float> @testv4floatsle(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sle i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testv4floatsle
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK-DAG: xxlor [[REG2:[0-9]+]], 34, 34
-; CHECK-DAG: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: xxlor [[REG2]], 35, 35
-; CHECK: .LBB[[BB]]:
-; CHECK: xxlor 34, [[REG2]], [[REG2]]
-; CHECK: blr
-}
-
-define <4 x float> @testv4floatule(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ule i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testv4floatule
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK-DAG: xxlor [[REG2:[0-9]+]], 34, 34
-; CHECK-DAG: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: xxlor [[REG2]], 35, 35
-; CHECK: .LBB[[BB]]:
-; CHECK: xxlor 34, [[REG2]], [[REG2]]
-; CHECK: blr
-}
-
-define <4 x float> @testv4floateq(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp eq i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testv4floateq
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK-DAG: xxlor [[REG2:[0-9]+]], 34, 34
-; CHECK-DAG: creqv [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: xxlor [[REG2]], 35, 35
-; CHECK: .LBB[[BB]]:
-; CHECK: xxlor 34, [[REG2]], [[REG2]]
-; CHECK: blr
-}
-
-define <4 x float> @testv4floatsge(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testv4floatsge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK-DAG: xxlor [[REG2:[0-9]+]], 34, 34
-; CHECK-DAG: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: xxlor [[REG2]], 35, 35
-; CHECK: .LBB[[BB]]:
-; CHECK: xxlor 34, [[REG2]], [[REG2]]
-; CHECK: blr
-}
-
-define <4 x float> @testv4floatuge(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp uge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testv4floatuge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK-DAG: xxlor [[REG2:[0-9]+]], 34, 34
-; CHECK-DAG: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: xxlor [[REG2]], 35, 35
-; CHECK: .LBB[[BB]]:
-; CHECK: xxlor 34, [[REG2]], [[REG2]]
-; CHECK: blr
-}
-
-define <4 x float> @testv4floatsgt(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sgt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testv4floatsgt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK-DAG: xxlor [[REG2:[0-9]+]], 34, 34
-; CHECK-DAG: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: xxlor [[REG2]], 35, 35
-; CHECK: .LBB[[BB]]:
-; CHECK: xxlor 34, [[REG2]], [[REG2]]
-; CHECK: blr
-}
-
-define <4 x float> @testv4floatugt(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ugt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testv4floatugt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK-DAG: xxlor [[REG2:[0-9]+]], 34, 34
-; CHECK-DAG: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: xxlor [[REG2]], 35, 35
-; CHECK: .LBB[[BB]]:
-; CHECK: xxlor 34, [[REG2]], [[REG2]]
-; CHECK: blr
-}
-
-define <4 x float> @testv4floatne(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ne i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testv4floatne
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK-DAG: xxlor [[REG2:[0-9]+]], 34, 34
-; CHECK-DAG: crxor [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: xxlor [[REG2]], 35, 35
-; CHECK: .LBB[[BB]]:
-; CHECK: xxlor 34, [[REG2]], [[REG2]]
-; CHECK: blr
-}
-
-define ppc_fp128 @testppc_fp128eq(ppc_fp128 %c1, ppc_fp128 %c2, ppc_fp128 %c3, ppc_fp128 %c4, ppc_fp128 %a1, ppc_fp128 %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq ppc_fp128 %c3, %c4
-  %cmp3tmp = fcmp oeq ppc_fp128 %c1, %c2
-  %cmp3 = icmp eq i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, ppc_fp128 %a1, ppc_fp128 %a2
-  ret ppc_fp128 %cond
-
-; FIXME: Because of the way that the late SELECT_* pseudo-instruction expansion
-; works, we end up with two blocks with the same predicate. These could be
-; combined.
-
-; CHECK-LABEL: @testppc_fp128eq
-; CHECK-DAG: fcmpu {{[0-9]+}}, 6, 8
-; CHECK-DAG: fcmpu {{[0-9]+}}, 5, 7
-; CHECK-DAG: fcmpu {{[0-9]+}}, 2, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 3
-; CHECK: crand [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: crand [[REG2:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: creqv [[REG3:[0-9]+]], [[REG2]], [[REG1]]
-; CHECK: bc 12, [[REG3]], .LBB[[BB1:[0-9_]+]]
-; CHECK: fmr 9, 11
-; CHECK: .LBB[[BB1]]:
-; CHECK: bc 12, [[REG3]], .LBB[[BB2:[0-9_]+]]
-; CHECK: fmr 10, 12
-; CHECK: .LBB[[BB2]]:
-; CHECK-DAG: fmr 1, 9
-; CHECK-DAG: fmr 2, 10
-; CHECK: blr
-}
-
-define <2 x double> @testv2doubleslt(float %c1, float %c2, float %c3, float %c4, <2 x double> %a1, <2 x double> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp slt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <2 x double> %a1, <2 x double> %a2
-  ret <2 x double> %cond
-
-; CHECK-LABEL: @testv2doubleslt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bclr 12, [[REG1]], 0
-; CHECK: vor 2, 3, 3
-; CHECK: blr
-}
-
-define <2 x double> @testv2doubleult(float %c1, float %c2, float %c3, float %c4, <2 x double> %a1, <2 x double> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ult i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <2 x double> %a1, <2 x double> %a2
-  ret <2 x double> %cond
-
-; CHECK-LABEL: @testv2doubleult
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bclr 12, [[REG1]], 0
-; CHECK: vor 2, 3, 3
-; CHECK: blr
-}
-
-define <2 x double> @testv2doublesle(float %c1, float %c2, float %c3, float %c4, <2 x double> %a1, <2 x double> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sle i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <2 x double> %a1, <2 x double> %a2
-  ret <2 x double> %cond
-
-; CHECK-LABEL: @testv2doublesle
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bclr 12, [[REG1]], 0
-; CHECK: vor 2, 3, 3
-; CHECK: blr
-}
-
-define <2 x double> @testv2doubleule(float %c1, float %c2, float %c3, float %c4, <2 x double> %a1, <2 x double> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ule i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <2 x double> %a1, <2 x double> %a2
-  ret <2 x double> %cond
-
-; CHECK-LABEL: @testv2doubleule
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bclr 12, [[REG1]], 0
-; CHECK: vor 2, 3, 3
-; CHECK: blr
-}
-
-define <2 x double> @testv2doubleeq(float %c1, float %c2, float %c3, float %c4, <2 x double> %a1, <2 x double> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp eq i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <2 x double> %a1, <2 x double> %a2
-  ret <2 x double> %cond
-
-; CHECK-LABEL: @testv2doubleeq
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: creqv [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bclr 12, [[REG1]], 0
-; CHECK: vor 2, 3, 3
-; CHECK: blr
-}
-
-define <2 x double> @testv2doublesge(float %c1, float %c2, float %c3, float %c4, <2 x double> %a1, <2 x double> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <2 x double> %a1, <2 x double> %a2
-  ret <2 x double> %cond
-
-; CHECK-LABEL: @testv2doublesge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bclr 12, [[REG1]], 0
-; CHECK: vor 2, 3, 3
-; CHECK: blr
-}
-
-define <2 x double> @testv2doubleuge(float %c1, float %c2, float %c3, float %c4, <2 x double> %a1, <2 x double> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp uge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <2 x double> %a1, <2 x double> %a2
-  ret <2 x double> %cond
-
-; CHECK-LABEL: @testv2doubleuge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bclr 12, [[REG1]], 0
-; CHECK: vor 2, 3, 3
-; CHECK: blr
-}
-
-define <2 x double> @testv2doublesgt(float %c1, float %c2, float %c3, float %c4, <2 x double> %a1, <2 x double> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sgt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <2 x double> %a1, <2 x double> %a2
-  ret <2 x double> %cond
-
-; CHECK-LABEL: @testv2doublesgt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bclr 12, [[REG1]], 0
-; CHECK: vor 2, 3, 3
-; CHECK: blr
-}
-
-define <2 x double> @testv2doubleugt(float %c1, float %c2, float %c3, float %c4, <2 x double> %a1, <2 x double> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ugt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <2 x double> %a1, <2 x double> %a2
-  ret <2 x double> %cond
-
-; CHECK-LABEL: @testv2doubleugt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bclr 12, [[REG1]], 0
-; CHECK: vor 2, 3, 3
-; CHECK: blr
-}
-
-define <2 x double> @testv2doublene(float %c1, float %c2, float %c3, float %c4, <2 x double> %a1, <2 x double> %a2) #0 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ne i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <2 x double> %a1, <2 x double> %a2
-  ret <2 x double> %cond
-
-; CHECK-LABEL: @testv2doublene
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crxor [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bclr 12, [[REG1]], 0
-; CHECK: vor 2, 3, 3
-; CHECK: blr
-}
-
-define <4 x double> @testqv4doubleslt(float %c1, float %c2, float %c3, float %c4, <4 x double> %a1, <4 x double> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp slt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x double> %a1, <4 x double> %a2
-  ret <4 x double> %cond
-
-; CHECK-LABEL: @testqv4doubleslt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x double> @testqv4doubleult(float %c1, float %c2, float %c3, float %c4, <4 x double> %a1, <4 x double> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ult i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x double> %a1, <4 x double> %a2
-  ret <4 x double> %cond
-
-; CHECK-LABEL: @testqv4doubleult
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x double> @testqv4doublesle(float %c1, float %c2, float %c3, float %c4, <4 x double> %a1, <4 x double> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sle i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x double> %a1, <4 x double> %a2
-  ret <4 x double> %cond
-
-; CHECK-LABEL: @testqv4doublesle
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x double> @testqv4doubleule(float %c1, float %c2, float %c3, float %c4, <4 x double> %a1, <4 x double> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ule i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x double> %a1, <4 x double> %a2
-  ret <4 x double> %cond
-
-; CHECK-LABEL: @testqv4doubleule
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x double> @testqv4doubleeq(float %c1, float %c2, float %c3, float %c4, <4 x double> %a1, <4 x double> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp eq i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x double> %a1, <4 x double> %a2
-  ret <4 x double> %cond
-
-; CHECK-LABEL: @testqv4doubleeq
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: creqv [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x double> @testqv4doublesge(float %c1, float %c2, float %c3, float %c4, <4 x double> %a1, <4 x double> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x double> %a1, <4 x double> %a2
-  ret <4 x double> %cond
-
-; CHECK-LABEL: @testqv4doublesge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x double> @testqv4doubleuge(float %c1, float %c2, float %c3, float %c4, <4 x double> %a1, <4 x double> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp uge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x double> %a1, <4 x double> %a2
-  ret <4 x double> %cond
-
-; CHECK-LABEL: @testqv4doubleuge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x double> @testqv4doublesgt(float %c1, float %c2, float %c3, float %c4, <4 x double> %a1, <4 x double> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sgt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x double> %a1, <4 x double> %a2
-  ret <4 x double> %cond
-
-; CHECK-LABEL: @testqv4doublesgt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x double> @testqv4doubleugt(float %c1, float %c2, float %c3, float %c4, <4 x double> %a1, <4 x double> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ugt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x double> %a1, <4 x double> %a2
-  ret <4 x double> %cond
-
-; CHECK-LABEL: @testqv4doubleugt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x double> @testqv4doublene(float %c1, float %c2, float %c3, float %c4, <4 x double> %a1, <4 x double> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ne i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x double> %a1, <4 x double> %a2
-  ret <4 x double> %cond
-
-; CHECK-LABEL: @testqv4doublene
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crxor [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testqv4floatslt(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp slt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testqv4floatslt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testqv4floatult(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ult i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testqv4floatult
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testqv4floatsle(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sle i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testqv4floatsle
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testqv4floatule(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ule i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testqv4floatule
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testqv4floateq(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp eq i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testqv4floateq
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: creqv [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testqv4floatsge(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testqv4floatsge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testqv4floatuge(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp uge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testqv4floatuge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testqv4floatsgt(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sgt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testqv4floatsgt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testqv4floatugt(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ugt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testqv4floatugt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x float> @testqv4floatne(float %c1, float %c2, float %c3, float %c4, <4 x float> %a1, <4 x float> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ne i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x float> %a1, <4 x float> %a2
-  ret <4 x float> %cond
-
-; CHECK-LABEL: @testqv4floatne
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crxor [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x i1> @testqv4i1slt(float %c1, float %c2, float %c3, float %c4, <4 x i1> %a1, <4 x i1> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp slt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x i1> %a1, <4 x i1> %a2
-  ret <4 x i1> %cond
-
-; CHECK-LABEL: @testqv4i1slt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x i1> @testqv4i1ult(float %c1, float %c2, float %c3, float %c4, <4 x i1> %a1, <4 x i1> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ult i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x i1> %a1, <4 x i1> %a2
-  ret <4 x i1> %cond
-
-; CHECK-LABEL: @testqv4i1ult
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x i1> @testqv4i1sle(float %c1, float %c2, float %c3, float %c4, <4 x i1> %a1, <4 x i1> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sle i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x i1> %a1, <4 x i1> %a2
-  ret <4 x i1> %cond
-
-; CHECK-LABEL: @testqv4i1sle
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x i1> @testqv4i1ule(float %c1, float %c2, float %c3, float %c4, <4 x i1> %a1, <4 x i1> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ule i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x i1> %a1, <4 x i1> %a2
-  ret <4 x i1> %cond
-
-; CHECK-LABEL: @testqv4i1ule
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x i1> @testqv4i1eq(float %c1, float %c2, float %c3, float %c4, <4 x i1> %a1, <4 x i1> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp eq i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x i1> %a1, <4 x i1> %a2
-  ret <4 x i1> %cond
-
-; CHECK-LABEL: @testqv4i1eq
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: creqv [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x i1> @testqv4i1sge(float %c1, float %c2, float %c3, float %c4, <4 x i1> %a1, <4 x i1> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x i1> %a1, <4 x i1> %a2
-  ret <4 x i1> %cond
-
-; CHECK-LABEL: @testqv4i1sge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x i1> @testqv4i1uge(float %c1, float %c2, float %c3, float %c4, <4 x i1> %a1, <4 x i1> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp uge i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x i1> %a1, <4 x i1> %a2
-  ret <4 x i1> %cond
-
-; CHECK-LABEL: @testqv4i1uge
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crorc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x i1> @testqv4i1sgt(float %c1, float %c2, float %c3, float %c4, <4 x i1> %a1, <4 x i1> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp sgt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x i1> %a1, <4 x i1> %a2
-  ret <4 x i1> %cond
-
-; CHECK-LABEL: @testqv4i1sgt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x i1> @testqv4i1ugt(float %c1, float %c2, float %c3, float %c4, <4 x i1> %a1, <4 x i1> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ugt i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x i1> %a1, <4 x i1> %a2
-  ret <4 x i1> %cond
-
-; CHECK-LABEL: @testqv4i1ugt
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crandc [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-define <4 x i1> @testqv4i1ne(float %c1, float %c2, float %c3, float %c4, <4 x i1> %a1, <4 x i1> %a2) #1 {
-entry:
-  %cmp1 = fcmp oeq float %c3, %c4
-  %cmp3tmp = fcmp oeq float %c1, %c2
-  %cmp3 = icmp ne i1 %cmp3tmp, %cmp1
-  %cond = select i1 %cmp3, <4 x i1> %a1, <4 x i1> %a2
-  ret <4 x i1> %cond
-
-; CHECK-LABEL: @testqv4i1ne
-; CHECK-DAG: fcmpu {{[0-9]+}}, 3, 4
-; CHECK-DAG: fcmpu {{[0-9]+}}, 1, 2
-; CHECK: crxor [[REG1:[0-9]+]], {{[0-9]+}}, {{[0-9]+}}
-; CHECK: bc 12, [[REG1]], .LBB[[BB:[0-9_]+]]
-; CHECK: qvfmr 5, 6
-; CHECK: .LBB[[BB]]:
-; CHECK: qvfmr 1, 5
-; CHECK: blr
-}
-
-attributes #0 = { nounwind readnone "target-cpu"="pwr7" }
-attributes #1 = { nounwind readnone "target-cpu"="a2q" }
-
diff --git a/test/CodeGen/PowerPC/vsx-fma-mutate-trivial-copy.ll b/test/CodeGen/PowerPC/vsx-fma-mutate-trivial-copy.ll
deleted file mode 100644
index a5b4474..0000000
--- a/test/CodeGen/PowerPC/vsx-fma-mutate-trivial-copy.ll
+++ /dev/null
@@ -1,38 +0,0 @@
-; RUN: llc < %s | FileCheck %s
-target datalayout = "E-m:e-i64:64-n32:64"
-target triple = "powerpc64-unknown-linux-gnu"
-
-; Function Attrs: nounwind
-define void @LSH_recall_init(float %d_min, float %W) #0 {
-entry:
-  br i1 undef, label %for.body.lr.ph, label %for.end
-
-; CHECK-LABEL: @LSH_recall_init
-; CHECK: xsnmsubadp
-
-for.body.lr.ph:                                   ; preds = %entry
-  %conv3 = fpext float %W to double
-  br label %for.body
-
-for.body:                                         ; preds = %for.body, %for.body.lr.ph
-  %div = fdiv fast float 0.000000e+00, 0.000000e+00
-  %add = fadd fast float %div, %d_min
-  %conv2 = fpext float %add to double
-  %0 = tail call double @llvm.sqrt.f64(double %conv2)
-  %div4 = fdiv fast double %conv3, %0
-  %call = tail call signext i32 bitcast (i32 (...)* @p_col_helper to i32 (double)*)(double %div4) #2
-  br label %for.body
-
-for.end:                                          ; preds = %entry
-  ret void
-}
-
-; Function Attrs: nounwind readnone
-declare double @llvm.sqrt.f64(double) #1
-
-declare signext i32 @p_col_helper(...) #2
-
-attributes #0 = { nounwind "no-infs-fp-math"="true" "no-nans-fp-math"="true" "target-cpu"="pwr7" "unsafe-fp-math"="true" }
-attributes #1 = { nounwind readnone }
-attributes #2 = { nounwind }
-
diff --git a/test/CodeGen/PowerPC/vsx-fma-mutate-undef.ll b/test/CodeGen/PowerPC/vsx-fma-mutate-undef.ll
deleted file mode 100644
index e3f4001..0000000
--- a/test/CodeGen/PowerPC/vsx-fma-mutate-undef.ll
+++ /dev/null
@@ -1,33 +0,0 @@
-; RUN: llc < %s | FileCheck %s
-target datalayout = "e-m:e-i64:64-n32:64"
-target triple = "powerpc64le-unknown-linux-gnu"
-
-; Function Attrs: nounwind
-define void @acosh_float8() #0 {
-entry:
-  br i1 undef, label %if.then, label %if.end
-
-if.then:                                          ; preds = %entry
-  %0 = tail call <4 x float> @llvm.fmuladd.v4f32(<4 x float> undef, <4 x float> <float 0x3FE62E4200000000, float 0x3FE62E4200000000, float 0x3FE62E4200000000, float 0x3FE62E4200000000>, <4 x float> undef) #0
-  %astype.i.i.74.i = bitcast <4 x float> %0 to <4 x i32>
-  %and.i.i.76.i = and <4 x i32> %astype.i.i.74.i, undef
-  %or.i.i.79.i = or <4 x i32> %and.i.i.76.i, undef
-  %astype5.i.i.80.i = bitcast <4 x i32> %or.i.i.79.i to <4 x float>
-  %1 = shufflevector <4 x float> %astype5.i.i.80.i, <4 x float> undef, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 undef, i32 undef, i32 undef, i32 undef>
-  %2 = shufflevector <8 x float> undef, <8 x float> %1, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
-  store <8 x float> %2, <8 x float>* undef, align 32
-  br label %if.end
-
-; CHECK-LABEL: @acosh_float8
-; CHECK: xvmaddasp
-
-if.end:                                           ; preds = %if.then, %entry
-  ret void
-}
-
-; Function Attrs: nounwind readnone
-declare <4 x float> @llvm.fmuladd.v4f32(<4 x float>, <4 x float>, <4 x float>) #1
-
-attributes #0 = { nounwind }
-attributes #1 = { nounwind readnone }
-
diff --git a/test/CodeGen/PowerPC/xvcmpeqdp-v2f64.ll b/test/CodeGen/PowerPC/xvcmpeqdp-v2f64.ll
deleted file mode 100644
index ef63233..0000000
--- a/test/CodeGen/PowerPC/xvcmpeqdp-v2f64.ll
+++ /dev/null
@@ -1,38 +0,0 @@
-; RUN: llc < %s | FileCheck %s
-target datalayout = "e-m:e-i64:64-n32:64"
-target triple = "powerpc64le-unknown-linux-gnu"
-
-; Function Attrs: nounwind
-define void @__fmax_double3_3D_exec() #0 {
-entry:
-  br i1 undef, label %if.then.i, label %fmax_double3.exit
-
-if.then.i:                                        ; preds = %entry
-  %cmp24.i.i = fcmp ord <3 x double> undef, zeroinitializer
-  %sext25.i.i = sext <3 x i1> %cmp24.i.i to <3 x i64>
-  %neg.i.i = xor <3 x i64> %sext25.i.i, <i64 -1, i64 -1, i64 -1>
-  %or.i.i = or <3 x i64> undef, %neg.i.i
-  %neg.i.i.i = select <3 x i1> undef, <3 x i64> zeroinitializer, <3 x i64> %sext25.i.i
-  %and.i.i.i = and <3 x i64> undef, %neg.i.i.i
-  %and26.i.i.i = and <3 x i64> undef, %or.i.i
-  %or.i.i.i = or <3 x i64> %and.i.i.i, %and26.i.i.i
-  %astype32.i.i.i = bitcast <3 x i64> %or.i.i.i to <3 x double>
-  %extractVec33.i.i.i = shufflevector <3 x double> %astype32.i.i.i, <3 x double> undef, <4 x i32> <i32 0, i32 1, i32 2, i32 undef>
-  store <4 x double> %extractVec33.i.i.i, <4 x double>* undef, align 32
-  br label %fmax_double3.exit
-
-; CHECK-LABEL: @__fmax_double3_3D_exec
-; CHECK: xvcmpeqdp
-
-fmax_double3.exit:                                ; preds = %if.then.i, %entry
-  br i1 undef, label %if.then, label %do.end
-
-if.then:                                          ; preds = %fmax_double3.exit
-  unreachable
-
-do.end:                                           ; preds = %fmax_double3.exit
-  ret void
-}
-
-attributes #0 = { nounwind }
-
diff --git a/test/CodeGen/RISCV/add.ll b/test/CodeGen/RISCV/add.ll
new file mode 100644
index 0000000..9f70a23
--- /dev/null
+++ b/test/CodeGen/RISCV/add.ll
@@ -0,0 +1,6 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1(i32 %a, i32 %b) {
+  %add = add i32 %a, %b
+  ret i32 %add
+}
diff --git a/test/CodeGen/RISCV/addi.ll b/test/CodeGen/RISCV/addi.ll
new file mode 100644
index 0000000..87d3c7e
--- /dev/null
+++ b/test/CodeGen/RISCV/addi.ll
@@ -0,0 +1,6 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1(i32 %a) {
+  %addi = add i32 %a, 2
+  ret i32 %addi
+}
diff --git a/test/CodeGen/RISCV/auipc.ll b/test/CodeGen/RISCV/auipc.ll
new file mode 100644
index 0000000..220bdd8
--- /dev/null
+++ b/test/CodeGen/RISCV/auipc.ll
@@ -0,0 +1,8 @@
+; RUN: llc -march=riscv < %s
+
+@G = global i32 1
+
+define i32 @f1() {
+  %tmp = load i32, i32* @G
+  ret i32 %tmp
+}
diff --git a/test/CodeGen/RISCV/beq.ll b/test/CodeGen/RISCV/beq.ll
new file mode 100644
index 0000000..2168916
--- /dev/null
+++ b/test/CodeGen/RISCV/beq.ll
@@ -0,0 +1,28 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1(i32 %a, i32 %b) {
+  br label %loop
+loop:
+  %cond = icmp eq i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+  ret i32 1
+exit:
+  ret i32 2
+}
+
+define void @f2(i32 *%src, i32 *%dst, i32 %target, i32 %a, i32 %b) {
+  %val = load volatile i32, i32 *%src
+  %cond = icmp eq i32 %target, %val
+  br i1 %cond, label %then, label %else
+
+then:
+  store i32 %a, i32* %dst, align 4
+  br label %exit
+
+else:
+  store i32 %b, i32* %dst, align 4
+  br label %exit
+
+exit:
+  ret void
+}
diff --git a/test/CodeGen/RISCV/bigimm.ll b/test/CodeGen/RISCV/bigimm.ll
new file mode 100644
index 0000000..59a908e
--- /dev/null
+++ b/test/CodeGen/RISCV/bigimm.ll
@@ -0,0 +1,5 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1() {
+  ret i32 19088743 ;0x01234567
+}
diff --git a/test/CodeGen/RISCV/branch.ll b/test/CodeGen/RISCV/branch.ll
new file mode 100644
index 0000000..0b7b4b2
--- /dev/null
+++ b/test/CodeGen/RISCV/branch.ll
@@ -0,0 +1,124 @@
+; RUN: llc -march=riscv < %s | FileCheck %s
+
+define i32 @eq(i32 %a, i32 %b) {
+; CHECK-LABEL: eq:
+  br label %loop
+loop:
+  %cond = icmp eq i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+; CHECK: beq
+  ret i32 1
+exit:
+  ret i32 2
+}
+
+define i32 @ne(i32 %a, i32 %b) {
+; CHECK-LABEL: ne:
+  br label %loop
+loop:
+  %cond = icmp ne i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+; CHECK: bne
+  ret i32 1
+exit:
+  ret i32 2
+}
+
+define i32 @lt(i32 %a, i32 %b) {
+; CHECK-LABEL: lt:
+  br label %loop
+loop:
+  %cond = icmp slt i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+; CHECK: blt
+  ret i32 1
+exit:
+  ret i32 2
+}
+
+define i32 @ge(i32 %a, i32 %b) {
+; CHECK-LABEL: ge:
+  br label %loop
+loop:
+  %cond = icmp sge i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+; CHECK: bge
+  ret i32 1
+exit:
+  ret i32 2
+}
+
+define i32 @ltu(i32 %a, i32 %b) {
+; CHECK-LABEL: ltu:
+  br label %loop
+loop:
+  %cond = icmp ult i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+; CHECK: bltu
+  ret i32 1
+exit:
+  ret i32 2
+}
+
+define i32 @geu(i32 %a, i32 %b) {
+; CHECK-LABEL: geu:
+  br label %loop
+loop:
+  %cond = icmp uge i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+; CHECK: bgeu
+  ret i32 1
+exit:
+  ret i32 2
+}
+
+; Synthesizable branches.
+
+define i32 @gt(i32 %a, i32 %b) {
+; CHECK-LABEL: gt:
+  br label %loop
+loop:
+  %cond = icmp sgt i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+; CHECK: blt
+  ret i32 1
+exit:
+  ret i32 2
+}
+
+define i32 @le(i32 %a, i32 %b) {
+; CHECK-LABEL: le:
+  br label %loop
+loop:
+  %cond = icmp sle i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+; CHECK: bge
+  ret i32 1
+exit:
+  ret i32 2
+}
+
+define i32 @gtu(i32 %a, i32 %b) {
+; CHECK-LABEL: gtu:
+  br label %loop
+loop:
+  %cond = icmp ugt i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+; CHECK: bltu
+  ret i32 1
+exit:
+  ret i32 2
+}
+
+define i32 @leu(i32 %a, i32 %b) {
+; CHECK-LABEL: leu:
+  br label %loop
+loop:
+  %cond = icmp ule i32 %a, %b
+  br i1 %cond, label %loop, label %exit
+; CHECK: bgeu
+  ret i32 1
+exit:
+  ret i32 2
+}
+
diff --git a/test/CodeGen/RISCV/call.ll b/test/CodeGen/RISCV/call.ll
new file mode 100644
index 0000000..8305bad
--- /dev/null
+++ b/test/CodeGen/RISCV/call.ll
@@ -0,0 +1,10 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @foo(i32 %x) noinline {
+        ret i32 42
+}
+
+define i32 @main() {
+        %r = call i32 @foo( i32 15 )            ; <i32> [#uses=1]
+        ret i32 %r
+}
diff --git a/test/CodeGen/RISCV/jal.ll b/test/CodeGen/RISCV/jal.ll
new file mode 100644
index 0000000..8f05781
--- /dev/null
+++ b/test/CodeGen/RISCV/jal.ll
@@ -0,0 +1,17 @@
+; RUN: llc -march=riscv < %s
+
+define i8* @f1() nounwind {
+entry:
+  %0 = call i8* @llvm.returnaddress(i32 0)
+  ret i8* %0
+}
+
+define i8* @f2() nounwind {
+entry:
+  call void @g()
+  %0 = call i8* @llvm.returnaddress(i32 0)
+  ret i8* %0
+}
+
+declare i8* @llvm.returnaddress(i32) nounwind readnone
+declare void @g()
diff --git a/test/CodeGen/RISCV/jalr.ll b/test/CodeGen/RISCV/jalr.ll
new file mode 100644
index 0000000..d27d58b
--- /dev/null
+++ b/test/CodeGen/RISCV/jalr.ll
@@ -0,0 +1,10 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @foo(i32 %x) {
+        ret i32 42
+}
+
+define i32 @main() {
+        %r = call i32 @foo( i32 15 )            ; <i32> [#uses=1]
+        ret i32 %r
+}
diff --git a/test/CodeGen/RISCV/jump.ll b/test/CodeGen/RISCV/jump.ll
new file mode 100644
index 0000000..18dd793
--- /dev/null
+++ b/test/CodeGen/RISCV/jump.ll
@@ -0,0 +1,8 @@
+; RUN: llc -march=riscv < %s
+
+define void @f1(i8 *%dest) {
+  br label %loop
+loop:
+  store volatile i8 1, i8 *%dest
+  br label %loop
+}
diff --git a/test/CodeGen/RISCV/lit.local.cfg b/test/CodeGen/RISCV/lit.local.cfg
new file mode 100644
index 0000000..c638201
--- /dev/null
+++ b/test/CodeGen/RISCV/lit.local.cfg
@@ -0,0 +1,2 @@
+if not 'RISCV' in config.root.targets:
+    config.unsupported = True
diff --git a/test/CodeGen/RISCV/load.ll b/test/CodeGen/RISCV/load.ll
new file mode 100644
index 0000000..5fa96d8
--- /dev/null
+++ b/test/CodeGen/RISCV/load.ll
@@ -0,0 +1,41 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1(i32* %p) {
+entry:
+  %b = load i32, i32* %p, align 4
+  ret i32 %b
+}
+
+define i32 @offset(i32* %p) {
+  %addr = getelementptr i32, i32* %p, i32 5
+  %ret = load i32, i32* %addr, align 4
+  ret i32 %ret
+}
+
+define i32 @lh(i16* %p) {
+entry:
+  %b = load i16, i16* %p, align 2
+  %ret = sext i16 %b to i32
+  ret i32 %ret
+}
+
+define i32 @lhoffset(i16* %p) {
+  %addr = getelementptr i16, i16* %p, i16 5
+  %b = load i16, i16* %addr, align 2
+  %ret = sext i16 %b to i32
+  ret i32 %ret
+}
+
+define i32 @lb(i8* %p) {
+entry:
+  %b = load i8, i8* %p, align 1
+  %ret = sext i8 %b to i32
+  ret i32 %ret
+}
+
+define i32 @lboffset(i8* %p) {
+  %addr = getelementptr i8, i8* %p, i8 5
+  %b = load i8, i8* %addr, align 1
+  %ret = sext i8 %b to i32
+  ret i32 %ret
+}
diff --git a/test/CodeGen/RISCV/lui.ll b/test/CodeGen/RISCV/lui.ll
new file mode 100644
index 0000000..c515675
--- /dev/null
+++ b/test/CodeGen/RISCV/lui.ll
@@ -0,0 +1,6 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1() {
+  ;ret i32 454664;
+  ret i32 1164410880;
+}
diff --git a/test/CodeGen/RISCV/math.ll b/test/CodeGen/RISCV/math.ll
new file mode 100644
index 0000000..0319d80
--- /dev/null
+++ b/test/CodeGen/RISCV/math.ll
@@ -0,0 +1,17 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1(i32 %a, i32 %b, i32 %c) {
+  %add = add  i32 %a, %b
+  %sub = sub  i32 %add, %c
+  %sll = shl  i32 %sub, %b
+  %slt = icmp slt i32 %sll, %b
+  %sltext = zext i1 %slt to i32
+  %sltu = icmp ult i32 %sltext, %b
+  %sltuext = zext i1 %sltu to i32
+  %srl = lshr i32 %sltuext, %b
+  %sra = ashr i32 %srl, %b
+  %xor = xor  i32 %sra, %b
+  %or  = or   i32 %xor, %b
+  %and = and  i32 %or, %b
+  ret i32 %and
+}
diff --git a/test/CodeGen/RISCV/mathi.ll b/test/CodeGen/RISCV/mathi.ll
new file mode 100644
index 0000000..ebf7c31
--- /dev/null
+++ b/test/CodeGen/RISCV/mathi.ll
@@ -0,0 +1,44 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @addi(i32 %a) {
+  %result     = add  i32 %a, 100
+  ret i32 %result
+}
+define i32 @subi(i32 %a) {
+  %result     = sub  i32 %a, 100
+  ret i32 %result
+}
+define i32 @slli(i32 %a) {
+  %result     = shl  i32 %a, 5
+  ret i32 %result
+}
+define i32 @slti(i32 %a) {
+  %slti    = icmp slt i32 %a, 4
+  %result  = zext i1 %slti to i32
+  ret i32 %result
+}
+define i32 @sltiu(i32 %a) {
+  %sltiu   = icmp slt i32 %a, 4
+  %result  = zext i1 %sltiu to i32
+  ret i32 %result
+}
+define i32 @srli(i32 %a) {
+  %result     = lshr  i32 %a, 5
+  ret i32 %result
+}
+define i32 @srai(i32 %a) {
+  %result     = ashr  i32 %a, 5
+  ret i32 %result
+}
+define i32 @xori(i32 %a) {
+  %result     = xor  i32 %a, 567
+  ret i32 %result
+}
+define i32 @ori(i32 %a) {
+  %result     = or  i32 %a, 567
+  ret i32 %result
+}
+define i32 @andi(i32 %a) {
+  %result     = and  i32 %a, 567
+  ret i32 %result
+}
diff --git a/test/CodeGen/RISCV/mul.ll b/test/CodeGen/RISCV/mul.ll
new file mode 100644
index 0000000..a3c2df9
--- /dev/null
+++ b/test/CodeGen/RISCV/mul.ll
@@ -0,0 +1,6 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1(i32 %a, i32 %b, i32 %c) {
+  %mul = mul  i32 %a, %b
+  ret i32 %mul
+}
diff --git a/test/CodeGen/RISCV/shift.ll b/test/CodeGen/RISCV/shift.ll
new file mode 100644
index 0000000..00de5fe
--- /dev/null
+++ b/test/CodeGen/RISCV/shift.ll
@@ -0,0 +1,6 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1(i32 %a) {
+  %shl = shl i32 %a, 1
+  ret i32 %shl
+}
diff --git a/test/CodeGen/RISCV/simple_mathi.ll b/test/CodeGen/RISCV/simple_mathi.ll
new file mode 100644
index 0000000..82c95bc
--- /dev/null
+++ b/test/CodeGen/RISCV/simple_mathi.ll
@@ -0,0 +1,18 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1(i32 %a) {
+  %addi = add  i32 %a, 2
+  ret i32 %addi
+}
+define i32 @f2(i32 %a) {
+  %xori = xor  i32 %a, 15
+  ret i32 %xori
+}
+define i32 @f3(i32 %a) {
+  %ori = or  i32 %a, 15
+  ret i32 %ori
+}
+define i32 @f4(i32 %a) {
+  %andi = and  i32 %a, 2
+  ret i32 %andi
+}
diff --git a/test/CodeGen/RISCV/slli.ll b/test/CodeGen/RISCV/slli.ll
new file mode 100644
index 0000000..514d7dc
--- /dev/null
+++ b/test/CodeGen/RISCV/slli.ll
@@ -0,0 +1,6 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1(i32 %a) {
+  %sll = shl  i32 %a, 6
+  ret i32 %sll
+}
diff --git a/test/CodeGen/RISCV/slt.ll b/test/CodeGen/RISCV/slt.ll
new file mode 100644
index 0000000..789db5a
--- /dev/null
+++ b/test/CodeGen/RISCV/slt.ll
@@ -0,0 +1,15 @@
+; RUN: llc -march=riscv < %s
+
+define void @f1(i32 %a, i32 %b, i32 *%dest) {
+  %slt = icmp slt i32 %a, %b
+  %conv = zext i1 %slt to i32
+  store i32 %conv, i32* %dest, align 4
+  ret void
+}
+
+define void @f2(i32 %a, i32 %b, i32 *%dest) {
+  %sltu = icmp ult i32 %a, %b
+  %conv = zext i1 %sltu to i32
+  store i32 %conv, i32* %dest, align 4
+  ret void
+}
diff --git a/test/CodeGen/RISCV/slti.ll b/test/CodeGen/RISCV/slti.ll
new file mode 100644
index 0000000..b0403e8
--- /dev/null
+++ b/test/CodeGen/RISCV/slti.ll
@@ -0,0 +1,15 @@
+; RUN: llc -march=riscv < %s
+
+define void @f1(i32 %a, i32 %b, i32 *%dest) {
+  %slti = icmp slt i32 %a, 50
+  %conv = zext i1 %slti to i32
+  store i32 %conv, i32* %dest, align 4
+  ret void
+}
+
+define void @f2(i32 %a, i32 %b, i32 *%dest) {
+  %sltiu = icmp ult i32 %a, 30
+  %conv = zext i1 %sltiu to i32
+  store i32 %conv, i32* %dest, align 4
+  ret void
+}
diff --git a/test/CodeGen/RISCV/srli.ll b/test/CodeGen/RISCV/srli.ll
new file mode 100644
index 0000000..47abd4b
--- /dev/null
+++ b/test/CodeGen/RISCV/srli.ll
@@ -0,0 +1,10 @@
+; RUN: llc -march=riscv < %s
+
+define i32 @f1(i32 %a) {
+  %srl = lshr  i32 %a, 6
+  ret i32 %srl
+}
+define i32 @f2(i32 %a) {
+  %sra = ashr  i32 %a, 6
+  ret i32 %sra
+}
diff --git a/test/CodeGen/RISCV/store.ll b/test/CodeGen/RISCV/store.ll
new file mode 100644
index 0000000..cd7da0c
--- /dev/null
+++ b/test/CodeGen/RISCV/store.ll
@@ -0,0 +1,35 @@
+; RUN: llc -march=riscv < %s
+
+define void @f1(i32* %p, i32 %a) {
+entry:
+  store i32 %a, i32* %p, align 4
+  ret void
+}
+
+define void @offset(i32* %p, i32 %offset, i32 %a) {
+  %addr = getelementptr i32, i32* %p, i32 5
+  store i32 %a, i32* %addr, align 4
+  ret void
+}
+
+define void @sh(i16* %p, i16 %a) {
+  store i16 %a, i16* %p, align 2
+  ret void
+}
+
+define void @shoffset(i16* %p, i16 %offset, i16 %a) {
+  %addr = getelementptr i16, i16* %p, i16 5
+  store i16 %a, i16* %addr, align 2
+  ret void
+}
+
+define void @sb(i8* %p, i8 %a) {
+  store i8 %a, i8* %p, align 1
+  ret void
+}
+
+define void @sboffset(i8* %p, i8 %offset, i8 %a) {
+  %addr = getelementptr i8, i8* %p, i8 5
+  store i8 %a, i8* %addr, align 1
+  ret void
+}
diff --git a/test/CodeGen/X86/pr24374.ll b/test/CodeGen/X86/pr24374.ll
deleted file mode 100644
index 7f331e1..0000000
--- a/test/CodeGen/X86/pr24374.ll
+++ /dev/null
@@ -1,37 +0,0 @@
-; RUN: llc < %s | FileCheck %s
-target datalayout = "e-m:w-i64:64-f80:128-n8:16:32:64-S128"
-target triple = "x86_64-w64-windows-gnu"
-
-@llvm.global_ctors = appending global [1 x { i32, void ()*, i8* }] [{ i32, void ()*, i8* } { i32 65535, void ()* @g, i8* null }]
-
-declare i32 @__gxx_personality_seh0(...)
-
-; Function Attrs: nounwind
-define void @f() #0 personality i8* bitcast (i32 (...)* @__gxx_personality_seh0 to i8*) {
-entry:
-  invoke void @g()
-          to label %exit unwind label %lpad
-
-lpad:                                             ; preds = %entry
-  landingpad { i8*, i32 }
-          cleanup
-  unreachable
-
-exit:                                             ; preds = %entry
-  unreachable
-}
-; CHECK-LABEL: f:
-; CHECK:       .seh_proc f
-; CHECK:               .seh_handler __gxx_personality_seh0, @unwind, @except
-; CHECK:       callq g
-; CHECK:               .seh_handlerdata
-; CHECK:               .seh_endproc
-
-define void @g() {
-  unreachable
-}
-; CHECK-LABEL: g:
-; CHECK:       .seh_proc g
-; CHECK:       .seh_endproc
-
-attributes #0 = { nounwind }
diff --git a/test/CodeGen/X86/setcc-lowering.ll b/test/CodeGen/X86/setcc-lowering.ll
deleted file mode 100644
index 3149fb5..0000000
--- a/test/CodeGen/X86/setcc-lowering.ll
+++ /dev/null
@@ -1,29 +0,0 @@
-; RUN: llc -mtriple=x86_64-unknown-unknown -mattr=+avx < %s | FileCheck %s
-
-; Verify that we don't crash during codegen due to a wrong lowering
-; of a setcc node with illegal operand types and return type.
-
-define <8 x i16> @pr25080(<8 x i32> %a) {
-; CHECK-LABEL: pr25080:
-; CHECK:       # BB#0: # %entry
-; CHECK-NEXT:    vandps {{.*}}(%rip), %ymm0, %ymm0
-; CHECK-NEXT:    vextractf128 $1, %ymm0, %xmm1
-; CHECK-NEXT:    vpxor %xmm2, %xmm2, %xmm2
-; CHECK-NEXT:    vpcmpeqd %xmm2, %xmm1, %xmm1
-; CHECK-NEXT:    vmovdqa {{.*#+}} xmm3 = [0,1,4,5,8,9,12,13,8,9,12,13,12,13,14,15]
-; CHECK-NEXT:    vpshufb %xmm3, %xmm1, %xmm1
-; CHECK-NEXT:    vpcmpeqd %xmm2, %xmm0, %xmm0
-; CHECK-NEXT:    vpshufb %xmm3, %xmm0, %xmm0
-; CHECK-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm0[0],xmm1[0]
-; CHECK-NEXT:    vpor {{.*}}(%rip), %xmm0, %xmm0
-; CHECK-NEXT:    vpsllw $15, %xmm0, %xmm0
-; CHECK-NEXT:    vpsraw $15, %xmm0, %xmm0
-; CHECK-NEXT:    vzeroupper
-; CHECK-NEXT:    retq
-entry:
-  %0 = trunc <8 x i32> %a to <8 x i23>
-  %1 = icmp eq <8 x i23> %0, zeroinitializer
-  %2 = or <8 x i1> %1, <i1 true, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false>
-  %3 = sext <8 x i1> %2 to <8 x i16>
-  ret <8 x i16> %3
-}
diff --git a/test/DebugInfo/gvn.ll b/test/DebugInfo/gvn.ll
deleted file mode 100644
index 3ca3663..0000000
--- a/test/DebugInfo/gvn.ll
+++ /dev/null
@@ -1,135 +0,0 @@
-; RUN: opt < %s -O2 -gvn -S | FileCheck %s
-;
-; Produced at -O2 from:
-; struct context {
-;   int cur_pid
-; };
-; int a, b, c, f, d;
-; int pid_for_task(int);
-; sample(struct context *p1)
-; {
-;   if (c)
-;     b = a;
-;   if (a && p1->cur_pid)
-;     sample_internal();
-; }
-; callback() {
-;   f = pid_for_task(d);
-;   sample(&f);
-; }
-
-target datalayout = "e-m:o-i64:64-i128:128-n32:64-S128"
-target triple = "arm64-apple-ios"
-
-%struct.context = type { i32 }
-
-@c = common global i32 0, align 4
-@a = common global i32 0, align 4
-@b = common global i32 0, align 4
-@d = common global i32 0, align 4
-@f = common global i32 0, align 4
-
-; Function Attrs: nounwind
-declare i32 @sample_internal(...)
-
-; Function Attrs: nounwind
-define i32 @callback() #0 {
-entry:
-  %0 = load i32, i32* @d, align 4, !dbg !37
-
-  ; Verify that the call still has a debug location after GVN.
-  ; CHECK: %call = tail call i32 @pid_for_task(i32 %0) #{{[0-9]}}, !dbg
-  %call = tail call i32 @pid_for_task(i32 %0) #3, !dbg !37
-
-  store i32 %call, i32* @f, align 4, !dbg !37
-  tail call void @llvm.dbg.value(metadata %struct.context* bitcast (i32* @f to %struct.context*), i64 0, metadata !25, metadata !26) #3, !dbg !38
-  %1 = load i32, i32* @c, align 4, !dbg !40
-  %tobool.i = icmp eq i32 %1, 0, !dbg !40
-  %.pr.i = load i32, i32* @a, align 4, !dbg !41
-  br i1 %tobool.i, label %if.end.i, label %if.then.i, !dbg !42
-
-if.then.i:                                        ; preds = %entry
-  store i32 %.pr.i, i32* @b, align 4, !dbg !43
-  br label %if.end.i, !dbg !43
-
-if.end.i:                                         ; preds = %if.then.i, %entry
-  %tobool1.i = icmp eq i32 %.pr.i, 0, !dbg !41
-
-  ; This instruction has no debug location -- in this
-  ; particular case it was removed by a bug in SimplifyCFG.
-  %2 = load i32, i32* @f, align 4
-
-  ; GVN is supposed to replace the load of @f with a direct reference to %call.
-  ; CHECK: %tobool2.i = icmp eq i32 %call, 0, !dbg
-  %tobool2.i = icmp eq i32 %2, 0, !dbg !41
-
-  %or.cond = or i1 %tobool1.i, %tobool2.i, !dbg !41
-  br i1 %or.cond, label %sample.exit, label %if.then.3.i, !dbg !41
-
-if.then.3.i:                                      ; preds = %if.end.i
-  %call.i = tail call i32 bitcast (i32 (...)* @sample_internal to i32 ()*)() #3, !dbg !44
-  br label %sample.exit, !dbg !44
-
-sample.exit:                                      ; preds = %if.end.i, %if.then.3.i
-  ret i32 undef, !dbg !45
-}
-
-declare i32 @pid_for_task(i32) #1
-
-; Function Attrs: nounwind readnone
-declare void @llvm.dbg.value(metadata, i64, metadata, metadata) #2
-
-attributes #0 = { nounwind }
-attributes #2 = { nounwind readnone }
-attributes #3 = { nounwind }
-
-!llvm.dbg.cu = !{!0}
-!llvm.module.flags = !{!22, !23}
-!llvm.ident = !{!24}
-
-!0 = distinct !DICompileUnit(language: DW_LANG_C99, file: !1, producer: "clang version 3.8.0 (trunk 244473) (llvm/trunk 244644)", isOptimized: false, runtimeVersion: 0, emissionKind: 1, enums: !2, subprograms: !3, globals: !16)
-!1 = !DIFile(filename: "test.c", directory: "/")
-!2 = !{}
-!3 = !{!4, !13}
-!4 = !DISubprogram(name: "sample", scope: !5, file: !5, line: 6, type: !6, isLocal: false, isDefinition: true, scopeLine: 7, flags: DIFlagPrototyped, isOptimized: false, variables: !2)
-!5 = !DIFile(filename: "test.i", directory: "/")
-!6 = !DISubroutineType(types: !7)
-!7 = !{!8, !9}
-!8 = !DIBasicType(name: "int", size: 32, align: 32, encoding: DW_ATE_signed)
-!9 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !10, size: 64, align: 64)
-!10 = !DICompositeType(tag: DW_TAG_structure_type, name: "context", file: !5, line: 1, size: 32, align: 32, elements: !11)
-!11 = !{!12}
-!12 = !DIDerivedType(tag: DW_TAG_member, name: "cur_pid", scope: !10, file: !5, line: 2, baseType: !8, size: 32, align: 32)
-!13 = !DISubprogram(name: "callback", scope: !5, file: !5, line: 13, type: !14, isLocal: false, isDefinition: true, scopeLine: 13, isOptimized: false, function: i32 ()* @callback, variables: !2)
-!14 = !DISubroutineType(types: !15)
-!15 = !{!8}
-!16 = !{!17, !18, !19, !20, !21}
-!17 = !DIGlobalVariable(name: "a", scope: !0, file: !5, line: 4, type: !8, isLocal: false, isDefinition: true, variable: i32* @a)
-!18 = !DIGlobalVariable(name: "b", scope: !0, file: !5, line: 4, type: !8, isLocal: false, isDefinition: true, variable: i32* @b)
-!19 = !DIGlobalVariable(name: "c", scope: !0, file: !5, line: 4, type: !8, isLocal: false, isDefinition: true, variable: i32* @c)
-!20 = !DIGlobalVariable(name: "f", scope: !0, file: !5, line: 4, type: !8, isLocal: false, isDefinition: true, variable: i32* @f)
-!21 = !DIGlobalVariable(name: "d", scope: !0, file: !5, line: 4, type: !8, isLocal: false, isDefinition: true, variable: i32* @d)
-!22 = !{i32 2, !"Dwarf Version", i32 2}
-!23 = !{i32 2, !"Debug Info Version", i32 3}
-!24 = !{!"clang version 3.8.0 (trunk 244473) (llvm/trunk 244644)"}
-!25 = !DILocalVariable(tag: DW_TAG_auto_variable, name: "p1", arg: 1, scope: !4, file: !5, line: 6, type: !9)
-!26 = !DIExpression()
-!27 = !DILocation(line: 6, scope: !4)
-!28 = !DILocation(line: 8, scope: !29)
-!29 = distinct !DILexicalBlock(scope: !4, file: !5, line: 8)
-!30 = !DILocation(line: 10, scope: !31)
-!31 = distinct !DILexicalBlock(scope: !4, file: !5, line: 10)
-!32 = !DILocation(line: 8, scope: !4)
-!33 = !DILocation(line: 9, scope: !29)
-!34 = !DILocation(line: 10, scope: !4)
-!35 = !DILocation(line: 11, scope: !31)
-!36 = !DILocation(line: 12, scope: !4)
-!37 = !DILocation(line: 14, scope: !13)
-!38 = !DILocation(line: 6, scope: !4, inlinedAt: !39)
-!39 = distinct !DILocation(line: 15, scope: !13)
-!40 = !DILocation(line: 8, scope: !29, inlinedAt: !39)
-!41 = !DILocation(line: 10, scope: !31, inlinedAt: !39)
-!42 = !DILocation(line: 8, scope: !4, inlinedAt: !39)
-!43 = !DILocation(line: 9, scope: !29, inlinedAt: !39)
-!44 = !DILocation(line: 11, scope: !31, inlinedAt: !39)
-!45 = !DILocation(line: 16, scope: !13)
diff --git a/test/LTO/X86/diagnostic-handler-noexit.ll b/test/LTO/X86/diagnostic-handler-noexit.ll
deleted file mode 100644
index be768c9..0000000
--- a/test/LTO/X86/diagnostic-handler-noexit.ll
+++ /dev/null
@@ -1,13 +0,0 @@
-; LTO default diagnostic handler should be non-exiting.
-; This test verifies that after addModule() encounters an error, the diagnostic
-; handler does not call exit(1) and instead returns to the caller of addModule.
-
-; RUN: llvm-as <%s >%t1
-; RUN: llvm-as <%s >%t2
-; RUN: not llvm-lto -o /dev/null %t1 %t2 2>&1 | FileCheck %s
-
-target triple = "x86_64-unknown-linux-gnu"
-
-; CHECK: Linking globals named 'goodboy': symbol multiply defined!
-; CHECK: llvm-lto{{.*}}: error adding file
-@goodboy = global i32 3203383023, align 4    ; 0xbeefbeef
diff --git a/test/MC/AMDGPU/vop3.s b/test/MC/AMDGPU/vop3.s
index 6391467..2056233 100644
--- a/test/MC/AMDGPU/vop3.s
+++ b/test/MC/AMDGPU/vop3.s
@@ -1,8 +1,5 @@
-// RUN: llvm-mc -arch=amdgcn -show-encoding %s | FileCheck %s --check-prefix=SICI
-// RUN: llvm-mc -arch=amdgcn -mcpu=SI -show-encoding %s | FileCheck %s --check-prefix=SICI
-// RUN: not llvm-mc -arch=amdgcn -mcpu=tonga -show-encoding %s | FileCheck %s --check-prefix=VI
-// RUN: not llvm-mc -arch=amdgcn -mcpu=tonga -show-encoding %s 2>&1 | FileCheck %s -check-prefix=NOVI
-
+// RUN: llvm-mc -arch=amdgcn -show-encoding %s | FileCheck %s
+// RUN: llvm-mc -arch=amdgcn -mcpu=SI -show-encoding %s | FileCheck %s
 
 //===----------------------------------------------------------------------===//
 // VOPC Instructions
@@ -11,81 +8,63 @@
 // Test forced e64 encoding
 
 v_cmp_lt_f32_e64 s[2:3], v4, -v6
-// SICI: v_cmp_lt_f32_e64 s[2:3], v4, -v6 ; encoding: [0x02,0x00,0x02,0xd0,0x04,0x0d,0x02,0x40]
-// VI:   v_cmp_lt_f32_e64 s[2:3], v4, -v6 ; encoding: [0x02,0x00,0x41,0xd0,0x04,0x0d,0x02,0x40]
-
+// CHECK: v_cmp_lt_f32_e64 s[2:3], v4, -v6 ; encoding: [0x02,0x00,0x02,0xd0,0x04,0x0d,0x02,0x40]
 
 //
 // Modifier tests:
 //
 
 v_cmp_lt_f32 s[2:3] -v4, v6
-// SICI: v_cmp_lt_f32_e64 s[2:3], -v4, v6 ; encoding: [0x02,0x00,0x02,0xd0,0x04,0x0d,0x02,0x20]
-// VI:   v_cmp_lt_f32_e64 s[2:3], -v4, v6 ; encoding: [0x02,0x00,0x41,0xd0,0x04,0x0d,0x02,0x20]
+// CHECK: v_cmp_lt_f32_e64 s[2:3], -v4, v6 ; encoding: [0x02,0x00,0x02,0xd0,0x04,0x0d,0x02,0x20] 
 
 v_cmp_lt_f32 s[2:3]  v4, -v6
-// SICI: v_cmp_lt_f32_e64 s[2:3], v4, -v6 ; encoding: [0x02,0x00,0x02,0xd0,0x04,0x0d,0x02,0x40]
-// VI:   v_cmp_lt_f32_e64 s[2:3], v4, -v6 ; encoding: [0x02,0x00,0x41,0xd0,0x04,0x0d,0x02,0x40]
+// CHECK: v_cmp_lt_f32_e64 s[2:3], v4, -v6 ; encoding: [0x02,0x00,0x02,0xd0,0x04,0x0d,0x02,0x40]
 
 v_cmp_lt_f32 s[2:3] -v4, -v6
-// SICI: v_cmp_lt_f32_e64 s[2:3], -v4, -v6 ; encoding: [0x02,0x00,0x02,0xd0,0x04,0x0d,0x02,0x60]
-// VI:   v_cmp_lt_f32_e64 s[2:3], -v4, -v6 ; encoding: [0x02,0x00,0x41,0xd0,0x04,0x0d,0x02,0x60]
+// CHECK: v_cmp_lt_f32_e64 s[2:3], -v4, -v6 ; encoding: [0x02,0x00,0x02,0xd0,0x04,0x0d,0x02,0x60]
 
 v_cmp_lt_f32 s[2:3] |v4|, v6
-// SICI: v_cmp_lt_f32_e64 s[2:3], |v4|, v6 ; encoding: [0x02,0x01,0x02,0xd0,0x04,0x0d,0x02,0x00]
-// VI:   v_cmp_lt_f32_e64 s[2:3], |v4|, v6 ; encoding: [0x02,0x01,0x41,0xd0,0x04,0x0d,0x02,0x00]
+// CHECK: v_cmp_lt_f32_e64 s[2:3], |v4|, v6 ; encoding: [0x02,0x01,0x02,0xd0,0x04,0x0d,0x02,0x00]
 
 v_cmp_lt_f32 s[2:3] v4, |v6|
-// SICI: v_cmp_lt_f32_e64 s[2:3], v4, |v6| ; encoding: [0x02,0x02,0x02,0xd0,0x04,0x0d,0x02,0x00]
-// VI:   v_cmp_lt_f32_e64 s[2:3], v4, |v6| ; encoding: [0x02,0x02,0x41,0xd0,0x04,0x0d,0x02,0x00]
+// CHECK: v_cmp_lt_f32_e64 s[2:3], v4, |v6| ; encoding: [0x02,0x02,0x02,0xd0,0x04,0x0d,0x02,0x00]
 
 v_cmp_lt_f32 s[2:3] |v4|, |v6|
-// SICI: v_cmp_lt_f32_e64 s[2:3], |v4|, |v6| ; encoding: [0x02,0x03,0x02,0xd0,0x04,0x0d,0x02,0x00]
-// VI:   v_cmp_lt_f32_e64 s[2:3], |v4|, |v6| ; encoding: [0x02,0x03,0x41,0xd0,0x04,0x0d,0x02,0x00]
+// CHECK: v_cmp_lt_f32_e64 s[2:3], |v4|, |v6| ; encoding: [0x02,0x03,0x02,0xd0,0x04,0x0d,0x02,0x00]
 
 v_cmp_lt_f32 s[2:3] -|v4|, v6
-// SICI: v_cmp_lt_f32_e64 s[2:3], -|v4|, v6 ; encoding: [0x02,0x01,0x02,0xd0,0x04,0x0d,0x02,0x20]
-// VI:   v_cmp_lt_f32_e64 s[2:3], -|v4|, v6 ; encoding: [0x02,0x01,0x41,0xd0,0x04,0x0d,0x02,0x20]
+// CHECK: v_cmp_lt_f32_e64 s[2:3], -|v4|, v6 ; encoding: [0x02,0x01,0x02,0xd0,0x04,0x0d,0x02,0x20]
 
 v_cmp_lt_f32 s[2:3] v4, -|v6|
-// SICI: v_cmp_lt_f32_e64 s[2:3], v4, -|v6| ; encoding: [0x02,0x02,0x02,0xd0,0x04,0x0d,0x02,0x40]
-// VI:   v_cmp_lt_f32_e64 s[2:3], v4, -|v6| ; encoding: [0x02,0x02,0x41,0xd0,0x04,0x0d,0x02,0x40]
+// CHECK: v_cmp_lt_f32_e64 s[2:3], v4, -|v6| ; encoding: [0x02,0x02,0x02,0xd0,0x04,0x0d,0x02,0x40]
 
 v_cmp_lt_f32 s[2:3] -|v4|, -|v6|
-// SICI: v_cmp_lt_f32_e64 s[2:3], -|v4|, -|v6| ; encoding: [0x02,0x03,0x02,0xd0,0x04,0x0d,0x02,0x60]
-// VI:   v_cmp_lt_f32_e64 s[2:3], -|v4|, -|v6| ; encoding: [0x02,0x03,0x41,0xd0,0x04,0x0d,0x02,0x60]
+// CHECK: v_cmp_lt_f32_e64 s[2:3], -|v4|, -|v6| ; encoding: [0x02,0x03,0x02,0xd0,0x04,0x0d,0x02,0x60]
 
 //
 // Instruction tests:
 //
 
 v_cmp_f_f32 s[2:3], v4, v6
-// SICI: v_cmp_f_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x00,0xd0,0x04,0x0d,0x02,0x00]
-// VI:   v_cmp_f_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x40,0xd0,0x04,0x0d,0x02,0x00]
+// CHECK: v_cmp_f_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x00,0xd0,0x04,0x0d,0x02,0x00]
 
 v_cmp_lt_f32 s[2:3], v4, v6
-// SICI: v_cmp_lt_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x02,0xd0,0x04,0x0d,0x02,0x00]
-// VI:   v_cmp_lt_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x41,0xd0,0x04,0x0d,0x02,0x00]
+// CHECK: v_cmp_lt_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x02,0xd0,0x04,0x0d,0x02,0x00]
 
 v_cmp_eq_f32 s[2:3], v4, v6
-// SICI: v_cmp_eq_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x04,0xd0,0x04,0x0d,0x02,0x00]
-// VI:   v_cmp_eq_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x42,0xd0,0x04,0x0d,0x02,0x00]
+// CHECK: v_cmp_eq_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x04,0xd0,0x04,0x0d,0x02,0x00]
 
 v_cmp_le_f32 s[2:3], v4, v6
-// SICI: v_cmp_le_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x06,0xd0,0x04,0x0d,0x02,0x00]
-// VI:   v_cmp_le_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x43,0xd0,0x04,0x0d,0x02,0x00]
+// CHECK: v_cmp_le_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x06,0xd0,0x04,0x0d,0x02,0x00]
 
 v_cmp_gt_f32 s[2:3], v4, v6
-// SICI: v_cmp_gt_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x08,0xd0,0x04,0x0d,0x02,0x00]
-// VI:   v_cmp_gt_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x44,0xd0,0x04,0x0d,0x02,0x00]
+// CHECK: v_cmp_gt_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x08,0xd0,0x04,0x0d,0x02,0x00]
 
 v_cmp_lg_f32 s[2:3], v4, v6
-// SICI: v_cmp_lg_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x0a,0xd0,0x04,0x0d,0x02,0x00]
-// VI:   v_cmp_lg_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x45,0xd0,0x04,0x0d,0x02,0x00]
+// CHECK: v_cmp_lg_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x0a,0xd0,0x04,0x0d,0x02,0x00]
 
 v_cmp_ge_f32 s[2:3], v4, v6
-// SICI: v_cmp_ge_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x0c,0xd0,0x04,0x0d,0x02,0x00]
-// VI:   v_cmp_ge_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x46,0xd0,0x04,0x0d,0x02,0x00]
+// CHECK: v_cmp_ge_f32_e64 s[2:3], v4, v6 ; encoding: [0x02,0x00,0x0c,0xd0,0x04,0x0d,0x02,0x00]
 
 // TODO: Finish VOPC
 
@@ -98,28 +77,22 @@ v_cmp_ge_f32 s[2:3], v4, v6
 // 
 
 v_fract_f32 v1, -v2
-// SICI: v_fract_f32_e64 v1, -v2 ; encoding: [0x01,0x00,0x40,0xd3,0x02,0x01,0x00,0x20]
-// VI:   v_fract_f32_e64 v1, -v2 ; encoding: [0x01,0x00,0x5b,0xd1,0x02,0x01,0x00,0x20]
+// CHECK: v_fract_f32_e64 v1, -v2 ; encoding: [0x01,0x00,0x40,0xd3,0x02,0x01,0x00,0x20]
 
 v_fract_f32 v1, |v2|
-// SICI: v_fract_f32_e64 v1, |v2| ; encoding: [0x01,0x01,0x40,0xd3,0x02,0x01,0x00,0x00]
-// VI:   v_fract_f32_e64 v1, |v2| ; encoding: [0x01,0x01,0x5b,0xd1,0x02,0x01,0x00,0x00]
+// CHECK: v_fract_f32_e64 v1, |v2| ; encoding: [0x01,0x01,0x40,0xd3,0x02,0x01,0x00,0x00]
 
 v_fract_f32 v1, -|v2|
-// SICI: v_fract_f32_e64 v1, -|v2| ; encoding: [0x01,0x01,0x40,0xd3,0x02,0x01,0x00,0x20]
-// VI:   v_fract_f32_e64 v1, -|v2| ; encoding: [0x01,0x01,0x5b,0xd1,0x02,0x01,0x00,0x20]
+// CHECK: v_fract_f32_e64 v1, -|v2| ; encoding: [0x01,0x01,0x40,0xd3,0x02,0x01,0x00,0x20]
 
 v_fract_f32 v1, v2 clamp
-// SICI: v_fract_f32_e64 v1, v2 clamp ; encoding: [0x01,0x08,0x40,0xd3,0x02,0x01,0x00,0x00]
-// VI:   v_fract_f32_e64 v1, v2 clamp ; encoding: [0x01,0x80,0x5b,0xd1,0x02,0x01,0x00,0x00]
+// CHECK: v_fract_f32_e64 v1, v2 clamp ; encoding: [0x01,0x08,0x40,0xd3,0x02,0x01,0x00,0x00]
 
 v_fract_f32 v1, v2 mul:2
-// SICI: v_fract_f32_e64 v1, v2 mul:2 ; encoding: [0x01,0x00,0x40,0xd3,0x02,0x01,0x00,0x08]
-// VI:   v_fract_f32_e64 v1, v2 mul:2 ; encoding: [0x01,0x00,0x5b,0xd1,0x02,0x01,0x00,0x08]
+// CHECK: v_fract_f32_e64 v1, v2 mul:2 ; encoding: [0x01,0x00,0x40,0xd3,0x02,0x01,0x00,0x08]
 
 v_fract_f32 v1, v2, div:2 clamp
-// SICI: v_fract_f32_e64 v1, v2 clamp div:2 ; encoding: [0x01,0x08,0x40,0xd3,0x02,0x01,0x00,0x18]
-// VI:   v_fract_f32_e64 v1, v2 clamp div:2 ; encoding: [0x01,0x80,0x5b,0xd1,0x02,0x01,0x00,0x18]
+// CHECK: v_fract_f32_e64 v1, v2 clamp div:2 ; encoding: [0x01,0x08,0x40,0xd3,0x02,0x01,0x00,0x18]
 
 // TODO: Finish VOP1
 
@@ -129,47 +102,37 @@ v_fract_f32 v1, v2, div:2 clamp
 
 // Test forced e64 encoding with e32 operands
 
-v_add_f32_e64 v1, v3, v5
-// SICI: v_add_f32_e64 v1, v3, v5 ; encoding: [0x01,0x00,0x06,0xd2,0x03,0x0b,0x02,0x00]
-// VI:   v_add_f32_e64 v1, v3, v5 ; encoding: [0x01,0x00,0x01,0xd1,0x03,0x0b,0x02,0x00]
+v_ldexp_f32_e64 v1, v3, v5
+// CHECK: v_ldexp_f32_e64 v1, v3, v5 ; encoding: [0x01,0x00,0x56,0xd2,0x03,0x0b,0x02,0x00]
 
 
 // TODO: Modifier tests
 
 v_cndmask_b32 v1, v3, v5, s[4:5]
-// SICI: v_cndmask_b32_e64 v1, v3, v5, s[4:5] ; encoding: [0x01,0x00,0x00,0xd2,0x03,0x0b,0x12,0x00]
-// VI:   v_cndmask_b32_e64 v1, v3, v5, s[4:5] ; encoding: [0x01,0x00,0x00,0xd1,0x03,0x0b,0x12,0x00]
+// CHECK: v_cndmask_b32_e64 v1, v3, v5, s[4:5] ; encoding: [0x01,0x00,0x00,0xd2,0x03,0x0b,0x12,0x00]
 
 //TODO: readlane, writelane
 
 v_add_f32 v1, v3, s5
-// SICI: v_add_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x06,0xd2,0x03,0x0b,0x00,0x00]
-// VI:   v_add_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x01,0xd1,0x03,0x0b,0x00,0x00]
+// CHECK: v_add_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x06,0xd2,0x03,0x0b,0x00,0x00]
 
 v_sub_f32 v1, v3, s5
-// SICI: v_sub_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x08,0xd2,0x03,0x0b,0x00,0x00]
-// VI:   v_sub_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x02,0xd1,0x03,0x0b,0x00,0x00]
+// CHECK: v_sub_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x08,0xd2,0x03,0x0b,0x00,0x00]
 
 v_subrev_f32 v1, v3, s5
-// SICI: v_subrev_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x0a,0xd2,0x03,0x0b,0x00,0x00]
-// VI:   v_subrev_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x03,0xd1,0x03,0x0b,0x00,0x00]
+// CHECK: v_subrev_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x0a,0xd2,0x03,0x0b,0x00,0x00]
 
 v_mac_legacy_f32 v1, v3, s5
-// SICI: v_mac_legacy_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x0c,0xd2,0x03,0x0b,0x00,0x00]
-// FIXME: The error message should be: error: instruction not supported on this GPU
-// NOVI: error: invalid operand for instruction
+// CHECK: v_mac_legacy_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x0c,0xd2,0x03,0x0b,0x00,0x00]
 
 v_mul_legacy_f32 v1, v3, s5
-// SICI: v_mul_legacy_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x0e,0xd2,0x03,0x0b,0x00,0x00]
-// VI:   v_mul_legacy_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x04,0xd1,0x03,0x0b,0x00,0x00]
+// CHECK: v_mul_legacy_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x0e,0xd2,0x03,0x0b,0x00,0x00]
 
 v_mul_f32 v1, v3, s5
-// SICI: v_mul_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x10,0xd2,0x03,0x0b,0x00,0x00]
-// VI:   v_mul_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x05,0xd1,0x03,0x0b,0x00,0x00]
+// CHECK: v_mul_f32_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x10,0xd2,0x03,0x0b,0x00,0x00]
 
 v_mul_i32_i24 v1, v3, s5
-// SICI: v_mul_i32_i24_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x12,0xd2,0x03,0x0b,0x00,0x00]
-// VI:   v_mul_i32_i24_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x06,0xd1,0x03,0x0b,0x00,0x00]
+// CHECK: v_mul_i32_i24_e64 v1, v3, s5 ; encoding: [0x01,0x00,0x12,0xd2,0x03,0x0b,0x00,0x00]
 
 ///===---------------------------------------------------------------------===//
 // VOP3 Instructions
@@ -178,8 +141,7 @@ v_mul_i32_i24 v1, v3, s5
 // TODO: Modifier tests
 
 v_mad_legacy_f32 v2, v4, v6, v8
-// SICI: v_mad_legacy_f32 v2, v4, v6, v8 ; encoding: [0x02,0x00,0x80,0xd2,0x04,0x0d,0x22,0x04]
-// VI:   v_mad_legacy_f32 v2, v4, v6, v8 ; encoding: [0x02,0x00,0xc0,0xd1,0x04,0x0d,0x22,0x04]
+// CHECK: v_mad_legacy_f32 v2, v4, v6, v8 ; encoding: [0x02,0x00,0x80,0xd2,0x04,0x0d,0x22,0x04]
 
 
 
diff --git a/test/MC/ARM/directive-arch-semantic-action.s b/test/MC/ARM/directive-arch-semantic-action.s
deleted file mode 100644
index b9c65d8..0000000
--- a/test/MC/ARM/directive-arch-semantic-action.s
+++ /dev/null
@@ -1,12 +0,0 @@
-@ RUN: not llvm-mc -triple arm-gnueabi-linux -filetype asm %s 2>&1 | FileCheck %s
-
-        .arch	armv6
-        dsb
-@ CHECK: error: instruction requires: data-barriers
-
-        .arch   armv7
-        dsb
-@ CHECK-NOT: error: instruction requires: data-barriers
-
-        .arch   invalid_architecture_name
-@ CHECK: error: Unknown arch name
diff --git a/test/MC/RISCV/EXT-A/valid-rv32imafd.s b/test/MC/RISCV/EXT-A/valid-rv32imafd.s
new file mode 100644
index 0000000..ad5fab5
--- /dev/null
+++ b/test/MC/RISCV/EXT-A/valid-rv32imafd.s
@@ -0,0 +1,20 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32IMAFD
+# XFAIL:
+
+lr.w		x5, x4
+sc.w		x6, x5
+amoswap.w	x8, x7, x6
+amoadd.w	x9, x8, x7
+amoxor.w	x10, x9, x8
+amoand.w	x11, x10, x9
+amoor.w		x12, x11, x10
+amomin.w	x13, x12, x11
+amomax.w	x14, x13, x12
+amominu.w	x15, x14, x13
+amomaxu.w	x16, x15, x14
+
+
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-A/valid-rv64imafd.s b/test/MC/RISCV/EXT-A/valid-rv64imafd.s
new file mode 100644
index 0000000..3de7256
--- /dev/null
+++ b/test/MC/RISCV/EXT-A/valid-rv64imafd.s
@@ -0,0 +1,33 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64IMAFD
+# XFAIL:
+
+lr.w		x5, x4
+sc.w		x6, x5
+amoswap.w	x8, x7, x6
+amoadd.w	x9, x8, x7
+amoxor.w	x10, x9, x8
+amoand.w	x11, x10, x9
+amoor.w		x12, x11, x10
+amomin.w	x13, x12, x11
+amomax.w	x14, x13, x12
+amominu.w	x15, x14, x13
+amomaxu.w	x16, x15, x14
+
+#-- rv64imafd only
+lr.d		x5, x4
+sc.d		x6, x5
+amoswap.d	x8, x7, x6
+amoadd.d	x9, x8, x7
+amoxor.d	x10, x9, x8
+amoand.d	x11, x10, x9
+amoor.d		x12, x11, x10
+amomin.d	x13, x12, x11
+amomax.d	x14, x13, x12
+amominu.d	x15, x14, x13
+amomaxu.d	x16, x15, x14
+
+
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-D/invalid-rv32d.s b/test/MC/RISCV/EXT-D/invalid-rv32d.s
new file mode 100644
index 0000000..5697e4a
--- /dev/null
+++ b/test/MC/RISCV/EXT-D/invalid-rv32d.s
@@ -0,0 +1,9 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32IMAFD | FileCheck --check-prefix=CHECK32 %s
+# XFAIL: *
+
+	fadd.s	x2, x1, x0
+		
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-D/invalid-rv64d.s b/test/MC/RISCV/EXT-D/invalid-rv64d.s
new file mode 100644
index 0000000..4507c5d
--- /dev/null
+++ b/test/MC/RISCV/EXT-D/invalid-rv64d.s
@@ -0,0 +1,9 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64IMAFD | FileCheck --check-prefix=CHECK64 %s
+# XFAIL: *
+
+	fadd.s	x2, x1, x0
+		
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-D/valid-rv32d-xfail.s b/test/MC/RISCV/EXT-D/valid-rv32d-xfail.s
new file mode 100644
index 0000000..42099ab
--- /dev/null
+++ b/test/MC/RISCV/EXT-D/valid-rv32d-xfail.s
@@ -0,0 +1,17 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64IMAFD | FileCheck --check-prefix=CHECK64 %s
+# XFAIL: *
+
+#-- rv64d instructions
+	fcvt.l.d f18, f19
+	fcvt.lu.d f1, f17
+	fmv.x.d	f9, f17
+	fcvt.d.l f22, f17
+	fcvt.d.lu f2, f31
+	fmv.d.x f29, f3
+		
+		
+	
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-D/valid-rv32d.s b/test/MC/RISCV/EXT-D/valid-rv32d.s
new file mode 100644
index 0000000..e9b176f
--- /dev/null
+++ b/test/MC/RISCV/EXT-D/valid-rv32d.s
@@ -0,0 +1,52 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32IMAFD | FileCheck --check-prefix=CHECK32 %s
+# XFAIL:
+
+#-- test register state f0-f31
+        fadd.s  f2, f1, f0
+        fadd.s  f5, f4, f3
+        fadd.s  f8, f7, f6
+        fadd.s  f11, f10, f9
+        fadd.s  f14, f13, f12
+        fadd.s  f17, f16, f15
+        fadd.s  f20, f19, f18
+        fadd.s  f23, f22, f21
+        fadd.s  f26, f25, f24
+        fadd.s  f29, f28, f27
+        fadd.s  f0, f31, f30
+
+#-- rv32d instructions
+	fld	f0, x1, 14
+	fld	f0, x1, -14
+	fsd	f5, x8, 36
+	fsd	f5, x8, -36
+	fmadd.d	f5, f6, f7, f0
+	fmsub.d	f10, f9, f13, f31
+	fnmsub.d f22, f7, f17, f11
+	fnmadd.d f12, f18, f22, f4
+	fadd.d	f13, f12, f2
+	fsub.d	f17, f15, f14
+	fmul.d	f9, f18, f30
+	fdiv.d	f3, f8, f9
+	fsqrt.d	f12, f19, f20
+	fsgnj.d	f12, f11, f10
+	fsgnjn.d f17, f18, f20
+	fsgnjx.d f27, f18, f20
+	fmin.d	f0, f1, f2
+	fmax.d	f8, f17, f22
+	fcvt.s.d f11, f18, f29
+	fcvt.d.s f9, f8, f7
+	feq.d	f21, f31, f27
+	flt.d	f18, f15, f10
+	fle.d	f9, f18, f0
+	fclass.d f31, f9
+	fcvt.w.d f22, f13, f8
+	fcvt.d.w f19, f7, f3
+	fcvt.d.wu f18, f3
+	
+		
+		
+	
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-D/valid-rv64d.s b/test/MC/RISCV/EXT-D/valid-rv64d.s
new file mode 100644
index 0000000..22a7697
--- /dev/null
+++ b/test/MC/RISCV/EXT-D/valid-rv64d.s
@@ -0,0 +1,59 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64IMAFD | FileCheck --check-prefix=CHECK64 %s
+# XFAIL:
+
+#-- test register state f0-f31
+        fadd.s  f2, f1, f0
+        fadd.s  f5, f4, f3
+        fadd.s  f8, f7, f6
+        fadd.s  f11, f10, f9
+        fadd.s  f14, f13, f12
+        fadd.s  f17, f16, f15
+        fadd.s  f20, f19, f18
+        fadd.s  f23, f22, f21
+        fadd.s  f26, f25, f24
+        fadd.s  f29, f28, f27
+        fadd.s  f0, f31, f30
+
+#-- rv32d instructions
+	fld	f0, x1, 14
+	fld	f0, x1, -14
+	fsd	f5, x8, 36
+	fsd	f5, x8, -36
+	fmadd.d	f5, f6, f7, f0
+	fmsub.d	f10, f9, f13, f31
+	fnmsub.d f22, f7, f17, f11
+	fnmadd.d f12, f18, f22, f4
+	fadd.d	f13, f12, f2
+	fsub.d	f17, f15, f14
+	fmul.d	f9, f18, f30
+	fdiv.d	f3, f8, f9
+	fsqrt.d	f12, f19, f20
+	fsgnj.d	f12, f11, f10
+	fsgnjn.d f17, f18, f20
+	fsgnjx.d f27, f18, f20
+	fmin.d	f0, f1, f2
+	fmax.d	f8, f17, f22
+	fcvt.s.d f11, f18, f29
+	fcvt.d.s f9, f8, f7
+	feq.d	f21, f31, f27
+	flt.d	f18, f15, f10
+	fle.d	f9, f18, f0
+	fclass.d f31, f9
+	fcvt.w.d f22, f13, f8
+	fcvt.d.w f19, f7, f3
+	fcvt.d.wu f18, f3
+	
+#-- rv64d instructions
+	fcvt.l.d f18, f19
+	fcvt.lu.d f1, f17
+	fmv.x.d	f9, f17
+	fcvt.d.l f22, f17
+	fcvt.d.lu f2, f31
+	fmv.d.x f29, f3
+		
+		
+	
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-F/invalid-rv32f.s b/test/MC/RISCV/EXT-F/invalid-rv32f.s
new file mode 100644
index 0000000..5697e4a
--- /dev/null
+++ b/test/MC/RISCV/EXT-F/invalid-rv32f.s
@@ -0,0 +1,9 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32IMAFD | FileCheck --check-prefix=CHECK32 %s
+# XFAIL: *
+
+	fadd.s	x2, x1, x0
+		
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-F/invalid-rv64f.s b/test/MC/RISCV/EXT-F/invalid-rv64f.s
new file mode 100644
index 0000000..4507c5d
--- /dev/null
+++ b/test/MC/RISCV/EXT-F/invalid-rv64f.s
@@ -0,0 +1,9 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64IMAFD | FileCheck --check-prefix=CHECK64 %s
+# XFAIL: *
+
+	fadd.s	x2, x1, x0
+		
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-F/valid-rv32f-xfail.s b/test/MC/RISCV/EXT-F/valid-rv32f-xfail.s
new file mode 100644
index 0000000..7b2de77
--- /dev/null
+++ b/test/MC/RISCV/EXT-F/valid-rv32f-xfail.s
@@ -0,0 +1,14 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32IMAFD | FileCheck --check-prefix=CHECK32 %s
+# XFAIL: *
+
+#-- RV64F only
+	fcvt.l.s	f9, f8
+	fcvt.lu.s	f10, f11
+	fcvt.s.l	f13, f14
+	fcvt.s.lu	f8, f9	
+	
+		
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-F/valid-rv32f.s b/test/MC/RISCV/EXT-F/valid-rv32f.s
new file mode 100644
index 0000000..c22cbcd
--- /dev/null
+++ b/test/MC/RISCV/EXT-F/valid-rv32f.s
@@ -0,0 +1,72 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32IMAFD | FileCheck --check-prefix=CHECK32 %s
+# XFAIL:
+
+# CHECK32:	fadd.s	f2, f1, f0              # encoding: [0x53,0xf1,0x00,0x00]
+# CHECK32:	fadd.s	f5, f4, f3              # encoding: [0xd3,0x72,0x32,0x00]
+# CHECK32:	fadd.s	f8, f7, f6              # encoding: [0x53,0xf4,0x63,0x00]
+# CHECK32:	fadd.s	f11, f10, f9            # encoding: [0xd3,0x75,0x95,0x00]
+# CHECK32:	fadd.s	f14, f13, f12           # encoding: [0x53,0xf7,0xc6,0x00]
+# CHECK32:	fadd.s	f17, f16, f15           # encoding: [0xd3,0x78,0xf8,0x00]
+# CHECK32:	fadd.s	f20, f19, f18           # encoding: [0x53,0xfa,0x29,0x01]
+# CHECK32:	fadd.s	f23, f22, f21           # encoding: [0xd3,0x7b,0x5b,0x01]
+# CHECK32:	fadd.s	f26, f25, f24           # encoding: [0x53,0xfd,0x8c,0x01]
+# CHECK32:	fadd.s	f29, f28, f27           # encoding: [0xd3,0x7e,0xbe,0x01]
+# CHECK32:	fadd.s	f0, f31, f30            # encoding: [0x53,0xf0,0xef,0x01]
+
+#-- test register state f0-f31	
+	fadd.s	f2, f1, f0
+	fadd.s	f5, f4, f3
+	fadd.s	f8, f7, f6
+	fadd.s	f11, f10, f9
+	fadd.s	f14, f13, f12
+	fadd.s	f17, f16, f15
+	fadd.s	f20, f19, f18
+	fadd.s	f23, f22, f21
+	fadd.s	f26, f25, f24
+	fadd.s	f29, f28, f27
+	fadd.s	f0, f31, f30
+
+#-- test rv32f instructions
+	flw		f1, x0, 12
+	flw		f3, x2, -12
+	fmadd.s		f5, f4, f3, f2	
+	fmsub.s		f5, f4, f3, f2	
+	fnmsub.s	f5, f4, f3, f2	
+	fnmadd.s	f8, f9, f10, f11
+	fadd.s		f12, f11, f10
+	fsub.s		f13, f14, f15
+	fmul.s		f16, f17, f18
+	fdiv.s		f19, f20, f21
+	fsqrt.s		f23, f20
+	fsgnj.s		f24, f25, f26
+	fsgnjn.s	f31, f20, f30	
+	fsgnjx.s	f31, f20, f30	
+	fmin.s		f14, f4, f1
+	fmax.s		f16, f1, f18
+	fcvt.w.s	f8, f31
+	fcvt.wu.s	f9, f10
+	fmv.x.s		f19, f18
+	feq.s		f0, f1, f2
+	flt.s		f22, f7, f9
+	fle.s		f4, f6, f7
+	fclass.s	f9, f22
+	fcvt.s.w	f4, f31
+	fcvt.s.wu	f19, f0, f1
+	fmv.s.x		f12, f29
+	fcsr		f10
+	frrm		f11
+	frflags		f12
+	fscsr		f13, f12
+	fsrm		f14, f15
+	fsflags		f16, f15
+	fsrmi		f12, 13
+	fsrmi		f12, -13
+	fsflagsi	f18, 17
+	fsflagsi	f18, -17
+
+	
+		
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-F/valid-rv64f.s b/test/MC/RISCV/EXT-F/valid-rv64f.s
new file mode 100644
index 0000000..95fdcbf
--- /dev/null
+++ b/test/MC/RISCV/EXT-F/valid-rv64f.s
@@ -0,0 +1,77 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64IMAFD | FileCheck --check-prefix=CHECK64 %s
+# XFAIL:
+
+# CHECK64:	fadd.s	f2, f1, f0              # encoding: [0x53,0xf1,0x00,0x00]
+# CHECK64:	fadd.s	f5, f4, f3              # encoding: [0xd3,0x72,0x32,0x00]
+# CHECK64:	fadd.s	f8, f7, f6              # encoding: [0x53,0xf4,0x63,0x00]
+# CHECK64:	fadd.s	f11, f10, f9            # encoding: [0xd3,0x75,0x95,0x00]
+# CHECK64:	fadd.s	f14, f13, f12           # encoding: [0x53,0xf7,0xc6,0x00]
+# CHECK64:	fadd.s	f17, f16, f15           # encoding: [0xd3,0x78,0xf8,0x00]
+# CHECK64:	fadd.s	f20, f19, f18           # encoding: [0x53,0xfa,0x29,0x01]
+# CHECK64:	fadd.s	f23, f22, f21           # encoding: [0xd3,0x7b,0x5b,0x01]
+# CHECK64:	fadd.s	f26, f25, f24           # encoding: [0x53,0xfd,0x8c,0x01]
+# CHECK64:	fadd.s	f29, f28, f27           # encoding: [0xd3,0x7e,0xbe,0x01]
+# CHECK64:	fadd.s	f0, f31, f30            # encoding: [0x53,0xf0,0xef,0x01]
+
+#-- test register state f0-f31	
+	fadd.s	f2, f1, f0
+	fadd.s	f5, f4, f3
+	fadd.s	f8, f7, f6
+	fadd.s	f11, f10, f9
+	fadd.s	f14, f13, f12
+	fadd.s	f17, f16, f15
+	fadd.s	f20, f19, f18
+	fadd.s	f23, f22, f21
+	fadd.s	f26, f25, f24
+	fadd.s	f29, f28, f27
+	fadd.s	f0, f31, f30
+
+#-- test rv32f instructions
+	flw		f1, x0, 12
+	flw		f3, x2, -12
+	fmadd.s		f5, f4, f3, f2	
+	fmsub.s		f5, f4, f3, f2	
+	fnmsub.s	f5, f4, f3, f2	
+	fnmadd.s	f8, f9, f10, f11
+	fadd.s		f12, f11, f10
+	fsub.s		f13, f14, f15
+	fmul.s		f16, f17, f18
+	fdiv.s		f19, f20, f21
+	fsqrt.s		f23, f20
+	fsgnj.s		f24, f25, f26
+	fsgnjn.s	f31, f20, f30	
+	fsgnjx.s	f31, f20, f30	
+	fmin.s		f14, f4, f1
+	fmax.s		f16, f1, f18
+	fcvt.w.s	f8, f31
+	fcvt.wu.s	f9, f10
+	fmv.x.s		f19, f18
+	feq.s		f0, f1, f2
+	flt.s		f22, f7, f9
+	fle.s		f4, f6, f7
+	fclass.s	f9, f22
+	fcvt.s.w	f4, f31
+	fcvt.s.wu	f19, f0, f1
+	fmv.s.x		f12, f29
+	fcsr		f10
+	frrm		f11
+	frflags		f12
+	fscsr		f13, f12
+	fsrm		f14, f15
+	fsflags		f16, f15
+	fsrmi		f12, 13
+	fsrmi		f12, -13
+	fsflagsi	f18, 17
+	fsflagsi	f18, -17
+
+#-- RV64F only
+	fcvt.l.s	f9, f8
+	fcvt.lu.s	f10, f11
+	fcvt.s.l	f13, f14
+	fcvt.s.lu	f8, f9	
+	
+		
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-M/valid-rv32imafd-div-xfail.s b/test/MC/RISCV/EXT-M/valid-rv32imafd-div-xfail.s
new file mode 100644
index 0000000..2883f5b
--- /dev/null
+++ b/test/MC/RISCV/EXT-M/valid-rv32imafd-div-xfail.s
@@ -0,0 +1,15 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32IMAFD | FileCheck %s
+# XFAIL: *
+
+#-- rq != {rs1|rs2}
+div	x10, x10, x8
+div	x10, x8, x10
+div	x10, x10, x10
+
+divu	x10, x10, x8
+divu	x10, x8, x10
+divu	x10, x10, x10
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-M/valid-rv32imafd.s b/test/MC/RISCV/EXT-M/valid-rv32imafd.s
new file mode 100644
index 0000000..7fc9621
--- /dev/null
+++ b/test/MC/RISCV/EXT-M/valid-rv32imafd.s
@@ -0,0 +1,26 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32IMAFD | FileCheck --check-prefix=CHECK32 %s
+
+# CHECK32:	mul     x5, x3, x4              # encoding: [0xb3,0x82,0x41,0x02]
+# CHECK32:	mulh    x6, x4, x5              # encoding: [0x33,0x13,0x52,0x02]
+# CHECK32:	mulhu   x7, x5, x6              # encoding: [0xb3,0xb3,0x62,0x02]
+# CHECK32:	div     x10, x9, x8             # encoding: [0x33,0xc5,0x84,0x02]
+# CHECK32:	rem     x11, x9, x8             # encoding: [0xb3,0xe5,0x84,0x02]
+# CHECK32:	divu    x10, x9, x8             # encoding: [0x33,0xd5,0x84,0x02]
+# CHECK32:	remu    x11, x9, x8             # encoding: [0xb3,0xf5,0x84,0x02]
+
+#-- Multiplication
+mul	x5, x3, x4
+mulh	x6, x4, x5
+mulhu	x7, x5, x6
+#mulhsu	x8, x6, x7	#-- currently unsupported
+
+#-- Division
+div	x10, x9, x8
+rem	x11, x9, x8
+divu	x10, x9, x8
+remu	x11, x9, x8
+
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-M/valid-rv64imafd-div-xfail.s b/test/MC/RISCV/EXT-M/valid-rv64imafd-div-xfail.s
new file mode 100644
index 0000000..83f42d4
--- /dev/null
+++ b/test/MC/RISCV/EXT-M/valid-rv64imafd-div-xfail.s
@@ -0,0 +1,23 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64IMAFD | FileCheck %s
+# XFAIL: *
+
+#-- rq != {rs1|rs2} 
+div	x10, x10, x8
+div	x10, x8, x10
+div	x10, x10, x10
+
+divu	x10, x10, x8
+divu	x10, x8, x10
+divu	x10, x10, x10
+
+divw	x10, x10, x8
+divw	x10, x8, x10
+divw	x10, x10, x10
+
+divuw	x10, x10, x8
+divuw	x10, x8, x10
+divuw	x10, x10, x10
+#-- EOF
+
diff --git a/test/MC/RISCV/EXT-M/valid-rv64imafd.s b/test/MC/RISCV/EXT-M/valid-rv64imafd.s
new file mode 100644
index 0000000..99cec39
--- /dev/null
+++ b/test/MC/RISCV/EXT-M/valid-rv64imafd.s
@@ -0,0 +1,39 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64IMAFD | FileCheck --check-prefix=CHECK64 %s
+
+# CHECK64:	mul     x5, x3, x4              # encoding: [0xb3,0x82,0x41,0x02]
+# CHECK64:	mulh    x6, x4, x5              # encoding: [0x33,0x13,0x52,0x02]
+# CHECK64:      mulhu   x7, x5, x6              # encoding: [0xb3,0xb3,0x62,0x02]
+# CHECK64:	mulw    x9, x7, x8              # encoding: [0xbb,0x84,0x83,0x02]
+
+# CHECK64:	div     x10, x9, x8             # encoding: [0x33,0xc5,0x84,0x02]
+# CHECK64:	rem     x11, x9, x8             # encoding: [0xb3,0xe5,0x84,0x02]
+# CHECK64:	divu    x10, x9, x8             # encoding: [0x33,0xd5,0x84,0x02]
+# CHECK64:	remu    x11, x9, x8             # encoding: [0xb3,0xf5,0x84,0x02]
+
+# CHECK64:	divw    x10, x9, x8             # encoding: [0x3b,0xc5,0x84,0x02]
+# CHECK64:	remw    x11, x9, x8             # encoding: [0xbb,0xe5,0x84,0x02]
+# CHECK64:	divuw   x10, x9, x8             # encoding: [0x3b,0xd5,0x84,0x02]
+# CHECK64:	remuw   x11, x9, x8             # encoding: [0xbb,0xf5,0x84,0x02]
+
+#-- Multiplication
+mul	x5, x3, x4
+mulh	x6, x4, x5
+mulhu	x7, x5, x6
+mulw	x9, x7, x8
+
+#-- Division
+div     x10, x9, x8
+rem     x11, x9, x8
+divu    x10, x9, x8
+remu	x11, x9, x8
+
+divw	x10, x9, x8
+remw	x11, x9, x8
+divuw	x10, x9, x8
+remuw	x11, x9, x8
+
+
+#-- EOF
+
diff --git a/test/MC/RISCV/RV32I/invalid-xfail-operand.s b/test/MC/RISCV/RV32I/invalid-xfail-operand.s
new file mode 100644
index 0000000..fb8ebcc
--- /dev/null
+++ b/test/MC/RISCV/RV32I/invalid-xfail-operand.s
@@ -0,0 +1,9 @@
+# Instructions that are expected to fail
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32I | FileCheck %s
+# XFAIL: *
+
+	add	x0, x1, x2, x3
+
+#-- EOF
+
diff --git a/test/MC/RISCV/RV32I/invalid-xfail-range.s b/test/MC/RISCV/RV32I/invalid-xfail-range.s
new file mode 100644
index 0000000..86730d0
--- /dev/null
+++ b/test/MC/RISCV/RV32I/invalid-xfail-range.s
@@ -0,0 +1,9 @@
+# Instructions that are expected to fail
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32I | FileCheck %s
+# XFAIL: *
+
+	add	x5, x4, x32
+
+#-- EOF
+
diff --git a/test/MC/RISCV/RV32I/valid-xfail-imm-overflow.s b/test/MC/RISCV/RV32I/valid-xfail-imm-overflow.s
new file mode 100644
index 0000000..2b41f2f
--- /dev/null
+++ b/test/MC/RISCV/RV32I/valid-xfail-imm-overflow.s
@@ -0,0 +1,10 @@
+# Instructions that are expected to fail
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32I | FileCheck %s
+# XFAIL: *
+
+	#-- overflow 32-immediate
+	addi x1, x0, 1200000000
+
+#-- EOF
+
diff --git a/test/MC/RISCV/RV32I/valid-xfail-nosymbol.s b/test/MC/RISCV/RV32I/valid-xfail-nosymbol.s
new file mode 100644
index 0000000..972b10c
--- /dev/null
+++ b/test/MC/RISCV/RV32I/valid-xfail-nosymbol.s
@@ -0,0 +1,10 @@
+# Instructions that are expected to fail
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32I | FileCheck %s
+# XFAIL: *
+
+	#-- branch to an unknown symbol 
+	beq     x0,x0, my_branch_target
+
+#-- EOF
+
diff --git a/test/MC/RISCV/RV32I/valid.s b/test/MC/RISCV/RV32I/valid.s
new file mode 100644
index 0000000..32b8892
--- /dev/null
+++ b/test/MC/RISCV/RV32I/valid.s
@@ -0,0 +1,217 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32I | FileCheck --check-prefix=CHECK32 %s
+# XFAIL:
+
+
+# CHECK32: addi    x0, x0, 0               # encoding: [0x13,0x00,0x00,0x00]
+# CHECK32: addi    x1, x0, 0               # encoding: [0x93,0x00,0x00,0x00]
+# CHECK32: addi    x2, x1, 0               # encoding: [0x13,0x81,0x00,0x00]
+# CHECK32: addi    x3, x2, 0               # encoding: [0x93,0x01,0x01,0x00]
+# CHECK32: addi    x4, x3, 0               # encoding: [0x13,0x82,0x01,0x00]
+# CHECK32: addi    x5, x4, 0               # encoding: [0x93,0x02,0x02,0x00]
+# CHECK32: addi    x6, x5, 0               # encoding: [0x13,0x83,0x02,0x00]
+# CHECK32: addi    x7, x6, 0               # encoding: [0x93,0x03,0x03,0x00]
+# CHECK32: addi    x8, x7, 0               # encoding: [0x13,0x84,0x03,0x00]
+# CHECK32: addi    x9, x8, 0               # encoding: [0x93,0x04,0x04,0x00]
+# CHECK32: addi    x10, x9, 0              # encoding: [0x13,0x85,0x04,0x00]
+# CHECK32: addi    x11, x10, 0             # encoding: [0x93,0x05,0x05,0x00]
+# CHECK32: addi    x12, x11, 0             # encoding: [0x13,0x86,0x05,0x00]
+# CHECK32: addi    x13, x12, 0             # encoding: [0x93,0x06,0x06,0x00]
+# CHECK32: addi    x14, x13, 0             # encoding: [0x13,0x87,0x06,0x00]
+# CHECK32: addi    x15, x14, 0             # encoding: [0x93,0x07,0x07,0x00]
+# CHECK32: addi    x16, x15, 0             # encoding: [0x13,0x88,0x07,0x00]
+# CHECK32: addi    x17, x16, 0             # encoding: [0x93,0x08,0x08,0x00]
+# CHECK32: addi    x18, x17, 0             # encoding: [0x13,0x89,0x08,0x00]
+# CHECK32: addi    x19, x18, 0             # encoding: [0x93,0x09,0x09,0x00]
+# CHECK32: addi    x20, x19, 0             # encoding: [0x13,0x8a,0x09,0x00]
+# CHECK32: addi    x21, x20, 0             # encoding: [0x93,0x0a,0x0a,0x00]
+# CHECK32: addi    x22, x21, 0             # encoding: [0x13,0x8b,0x0a,0x00]
+# CHECK32: addi    x23, x22, 0             # encoding: [0x93,0x0b,0x0b,0x00]
+# CHECK32: addi    x24, x23, 0             # encoding: [0x13,0x8c,0x0b,0x00]
+# CHECK32: addi    x25, x24, 0             # encoding: [0x93,0x0c,0x0c,0x00]
+# CHECK32: addi    x26, x25, 0             # encoding: [0x13,0x8d,0x0c,0x00]
+# CHECK32: addi    x27, x26, 0             # encoding: [0x93,0x0d,0x0d,0x00]
+# CHECK32: addi    x28, x27, 0             # encoding: [0x13,0x8e,0x0d,0x00]
+# CHECK32: addi    x29, x28, 0             # encoding: [0x93,0x0e,0x0e,0x00]
+# CHECK32: addi    x30, x29, 0             # encoding: [0x13,0x8f,0x0e,0x00]
+# CHECK32: addi    x31, x30, 0             # encoding: [0x93,0x0f,0x0f,0x00]
+
+# CHECK32: addi x0, x0, 0		   # encoding: [0x13,0x00,0x00,0x00]
+
+# CHECK32: addi	x3, x2, 1023               # encoding: [0x93,0x01,0xf1,0x3f]
+# CHECK32: addi	x4, x3, -1023              # encoding: [0x13,0x82,0x11,0xc0]
+# CHECK32: addi    x31, x0, 0              # encoding: [0x93,0x0f,0x00,0x00]
+# CHECK32: addi x31, x0, 0                 # encoding: [0x93,0x0f,0x00,0x00]
+
+# CHECK32: slti	x5, x4, 0                  # encoding: [0x93,0x22,0x02,0x00]
+# CHECK32: slti	x6, x5, 1023               # encoding: [0x13,0xa3,0xf2,0x3f]
+# CHECK32: sltiu x7, x0, 1                 # encoding: [0x93,0x33,0x10,0x00]
+# CHECK32: sltiu x7, x0, 1                 # encoding: [0x93,0x33,0x10,0x00]
+
+# CHECK32: andi	x8, x7, 12                 # encoding: [0x13,0xf4,0xc3,0x00]
+# CHECK32: ori	x9, x8, -24                # encoding: [0x93,0x64,0x84,0xfe]
+# CHECK32: xori	x10, x9, 17                # encoding: [0x13,0xc5,0x14,0x01]
+# CHECK32: xori	x11, x10, -1               # encoding: [0x93,0x45,0xf5,0xff]
+# CHECK32: xori x11, x10, -1               # encoding: [0x93,0x45,0xf5,0xff]
+
+# CHECK32: slli	x12, x10, 5                # encoding: [0x13,0x16,0x55,0x00]
+# CHECK32: srli	x13, x12, 31               # encoding: [0x93,0x56,0xf6,0x01]
+# CHECK32: srai	x14, x13, 2                # encoding: [0x13,0xd7,0x26,0x40]
+# CHECK32: lui	x15, 1048575               # encoding: [0xb7,0xf7,0xff,0xff]
+# CHECK32: auipc   x16, 4                  # encoding: [0x17,0x48,0x00,0x00]
+
+# CHECK32: add     x17, x16, x15           # encoding: [0xb3,0x08,0xf8,0x00]
+# CHECK32: sub     x18, x17, x16           # encoding: [0x33,0x89,0x08,0x41]
+# CHECK32: slt     x19, x18, x17           # encoding: [0xb3,0x29,0x19,0x01]
+# CHECK32: sltu    x20, x19, x18           # encoding: [0x33,0xba,0x29,0x01]
+# CHECK32: and     x21, x20, x19           # encoding: [0xb3,0x7a,0x3a,0x01]
+# CHECK32: or      x22, x21, x20           # encoding: [0x33,0xeb,0x4a,0x01]
+# CHECK32: xor     x23, x22, x21           # encoding: [0xb3,0x4b,0x5b,0x01]
+# CHECK32: sll     x24, x23, x22           # encoding: [0x33,0x9c,0x6b,0x01]
+# CHECK32: srl     x25, x24, x23           # encoding: [0xb3,0x5c,0x7c,0x01]
+# CHECK32: sra     x26, x25, x24           # encoding: [0x33,0xdd,0x8c,0x41]
+
+# CHECK32: addi    x2, x0, 0               # encoding: [0x13,0x01,0x00,0x00]
+
+# CHECK32: jalr    x28, x27, 8             # encoding: [0x67,0x8e,0x8d,0x00]
+
+# CHECK32: fence
+# CHECK32: fence.i                         # encoding: [0x0f,0x10,0x00,0x00]
+
+# CHECK32: scall                           # encoding: [0x73,0x00,0x00,0x00]
+# CHECK32: sbreak                          # encoding: [0x73,0x00,0x10,0x00]
+# CHECK32: rdcycle    x5                   # encoding: [0xf3,0x22,0x00,0xc0]
+# CHECK32: rdcycleh   x6                   # encoding: [0x73,0x23,0x00,0xc8]
+# CHECK32: rdtime     x7                   # encoding: [0xf3,0x23,0x10,0xc0]
+# CHECK32: rdtimeh    x8                   # encoding: [0x73,0x24,0x10,0xc8]
+# CHECK32: rdinstret  x9                   # encoding: [0xf3,0x24,0x20,0xc0]
+# CHECK32: rdinstreth x10                  # encoding: [0x73,0x25,0x20,0xc8]
+
+#-- register encodings: x0-x31
+#--                     x0 == $0
+	addi	x0, x0, 0
+	addi	x1, x0, 0
+	addi	x2, x1, 0
+	addi	x3, x2, 0
+	addi	x4, x3, 0
+	addi	x5, x4, 0
+	addi	x6, x5, 0
+	addi	x7, x6, 0
+	addi	x8, x7, 0
+	addi	x9, x8, 0
+	addi	x10, x9, 0
+	addi	x11, x10, 0
+	addi	x12, x11, 0
+	addi	x13, x12, 0
+	addi	x14, x13, 0
+	addi	x15, x14, 0
+	addi	x16, x15, 0
+	addi	x17, x16, 0
+	addi	x18, x17, 0
+	addi	x19, x18, 0
+	addi	x20, x19, 0
+	addi	x21, x20, 0
+	addi	x22, x21, 0
+	addi	x23, x22, 0
+	addi	x24, x23, 0
+	addi	x25, x24, 0
+	addi	x26, x25, 0
+	addi	x27, x26, 0
+	addi	x28, x27, 0
+	addi	x29, x28, 0
+	addi	x30, x29, 0
+	addi	x31, x30, 0
+
+#-- INTEGER COMPUTATIONAL INSTRUCTIONS
+#-- Integer Register-Immediate
+	nop				#-- pseudo-op for addi x0, x0, 0
+	addi	x3, x2, 1023
+	addi	x4, x3, -1023
+	addi	x31, x0, 0
+	mv	x31, x0			#-- pseudo-op for addi x31, x0, 0
+	slti	x5, x4, 0
+	slti	x6, x5, 1023
+	sltiu	x7, x0, 1
+	seqz	x7,x0			#-- pseudo-op for sltiu x7,x0,1
+	andi	x8, x7, 12
+	ori	x9, x8, -24
+	xori	x10, x9, 17
+	xori	x11, x10, -1
+	not	x11, x10		#-- pseudo-op for xori x11, x10, -1
+	slli	x12, x10, 5
+	srli	x13, x12, 31
+	srai	x14, x13, 2
+	lui	x15, 1048575
+	auipc	x16, 4
+
+#-- Integer Register-Register
+	add	x17, x16, x15
+	sub	x18, x17, x16
+	slt	x19, x18, x17
+	sltu	x20, x19, x18
+	and	x21, x20, x19
+	or	x22, x21, x20
+	xor	x23, x22, x21
+	sll	x24, x23, x22
+	srl	x25, x24, x23
+	sra	x26, x25, x24
+
+#-- CONTROL TRANSFER INSTRUCTIONS
+	mv 	x2, x0
+	jal	x27, 8
+	jal	x0, 8			#-- unconditional jump
+	jalr	x28, x27, 8
+	beq	x28,x27, target_beq
+target_beq:
+	bne	x28,x27, target_bne
+target_bne:
+	blt	x29, x28, target_blt
+target_blt:
+	bltu	x30, x29, target_blt	
+target_bltu:
+	bge	x31, x30, target_bge
+target_bge:
+	bgeu	x2, x31, target_bgeu
+target_bgeu:
+	nop
+
+#-- LOAD AND STORE INSTRUCTIONS	
+
+	lw x5, 0(x3)
+	sw x5, 0(x3)
+	sh x6, 0(x3)
+	sb x7, 0(x3)
+	sw x5, -4(x3)
+	sh x6, -4(x3)
+	sb x7, -4(x3)
+	sw x5, 4(x3)
+	sh x6, 4(x3)
+	sb x7, 4(x3)
+	lw x5, 0(x3)
+	lh x6, 0(x3)
+	lb x7, 0(x3)
+	lw x5, -4(x3)
+	lh x6, -4(x3)
+	lb x7, -4(x3)
+	lw x5, 4(x3)
+	lh x6, 4(x3)
+	lb x7, 4(x3)
+
+#-- MEMORY MODEL
+	fence
+	fence.i
+
+#-- SYSTEM INSTRUCTIONS
+	scall
+	sbreak
+	rdcycle	x5
+	rdcycleh x6
+	rdtime	x7
+	rdtimeh	x8
+	rdinstret x9 
+	rdinstreth x10 
+		
+
+#-- EOF
+
diff --git a/test/MC/RISCV/RV64I/invalid-rv64i-xfail.s b/test/MC/RISCV/RV64I/invalid-rv64i-xfail.s
new file mode 100644
index 0000000..dfc9c5e
--- /dev/null
+++ b/test/MC/RISCV/RV64I/invalid-rv64i-xfail.s
@@ -0,0 +1,12 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64I | FileCheck --check-prefix=CHECK64 %s
+# XFAIL: *
+
+	rdcycleh x6
+	rdtimeh	x8
+	rdinstreth x10 
+		
+
+#-- EOF
+
diff --git a/test/MC/RISCV/RV64I/valid-xfail-nosymbol.s b/test/MC/RISCV/RV64I/valid-xfail-nosymbol.s
new file mode 100644
index 0000000..0914c96
--- /dev/null
+++ b/test/MC/RISCV/RV64I/valid-xfail-nosymbol.s
@@ -0,0 +1,10 @@
+# Instructions that are expected to fail
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64I | FileCheck %s
+# XFAIL: *
+
+	#-- branch to an unknown symbol 
+	beq     x0,x0, my_branch_target
+
+#-- EOF
+
diff --git a/test/MC/RISCV/RV64I/valid.s b/test/MC/RISCV/RV64I/valid.s
new file mode 100644
index 0000000..83fcef5
--- /dev/null
+++ b/test/MC/RISCV/RV64I/valid.s
@@ -0,0 +1,253 @@
+# Instructions that are valid
+#
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64I | FileCheck --check-prefix=CHECK64 %s
+# XFAIL:
+
+
+# CHECK64: addi    x0, x0, 0               # encoding: [0x13,0x00,0x00,0x00]
+# CHECK64: addi    x1, x0, 0               # encoding: [0x93,0x00,0x00,0x00]
+# CHECK64: addi    x2, x1, 0               # encoding: [0x13,0x81,0x00,0x00]
+# CHECK64: addi    x3, x2, 0               # encoding: [0x93,0x01,0x01,0x00]
+# CHECK64: addi    x4, x3, 0               # encoding: [0x13,0x82,0x01,0x00]
+# CHECK64: addi    x5, x4, 0               # encoding: [0x93,0x02,0x02,0x00]
+# CHECK64: addi    x6, x5, 0               # encoding: [0x13,0x83,0x02,0x00]
+# CHECK64: addi    x7, x6, 0               # encoding: [0x93,0x03,0x03,0x00]
+# CHECK64: addi    x8, x7, 0               # encoding: [0x13,0x84,0x03,0x00]
+# CHECK64: addi    x9, x8, 0               # encoding: [0x93,0x04,0x04,0x00]
+# CHECK64: addi    x10, x9, 0              # encoding: [0x13,0x85,0x04,0x00]
+# CHECK64: addi    x11, x10, 0             # encoding: [0x93,0x05,0x05,0x00]
+# CHECK64: addi    x12, x11, 0             # encoding: [0x13,0x86,0x05,0x00]
+# CHECK64: addi    x13, x12, 0             # encoding: [0x93,0x06,0x06,0x00]
+# CHECK64: addi    x14, x13, 0             # encoding: [0x13,0x87,0x06,0x00]
+# CHECK64: addi    x15, x14, 0             # encoding: [0x93,0x07,0x07,0x00]
+# CHECK64: addi    x16, x15, 0             # encoding: [0x13,0x88,0x07,0x00]
+# CHECK64: addi    x17, x16, 0             # encoding: [0x93,0x08,0x08,0x00]
+# CHECK64: addi    x18, x17, 0             # encoding: [0x13,0x89,0x08,0x00]
+# CHECK64: addi    x19, x18, 0             # encoding: [0x93,0x09,0x09,0x00]
+# CHECK64: addi    x20, x19, 0             # encoding: [0x13,0x8a,0x09,0x00]
+# CHECK64: addi    x21, x20, 0             # encoding: [0x93,0x0a,0x0a,0x00]
+# CHECK64: addi    x22, x21, 0             # encoding: [0x13,0x8b,0x0a,0x00]
+# CHECK64: addi    x23, x22, 0             # encoding: [0x93,0x0b,0x0b,0x00]
+# CHECK64: addi    x24, x23, 0             # encoding: [0x13,0x8c,0x0b,0x00]
+# CHECK64: addi    x25, x24, 0             # encoding: [0x93,0x0c,0x0c,0x00]
+# CHECK64: addi    x26, x25, 0             # encoding: [0x13,0x8d,0x0c,0x00]
+# CHECK64: addi    x27, x26, 0             # encoding: [0x93,0x0d,0x0d,0x00]
+# CHECK64: addi    x28, x27, 0             # encoding: [0x13,0x8e,0x0d,0x00]
+# CHECK64: addi    x29, x28, 0             # encoding: [0x93,0x0e,0x0e,0x00]
+# CHECK64: addi    x30, x29, 0             # encoding: [0x13,0x8f,0x0e,0x00]
+# CHECK64: addi    x31, x30, 0             # encoding: [0x93,0x0f,0x0f,0x00]
+
+# CHECK64: addi    x0, x0, 0               # encoding: [0x13,0x00,0x00,0x00]
+
+# CHECK64: addi	x3, x2, 1023               # encoding: [0x93,0x01,0xf1,0x3f]
+# CHECK64: addi	x4, x3, -1023              # encoding: [0x13,0x82,0x11,0xc0]
+# CHECK64: addi x31, x0, 0                 # encoding: [0x93,0x0f,0x00,0x00]
+# CHECK64: addi x31, x0, 0                 # encoding: [0x93,0x0f,0x00,0x00]
+# CHECK64: slti	x5, x4, 0                  # encoding: [0x93,0x22,0x02,0x00]
+# CHECK64: slti	x6, x5, 1023               # encoding: [0x13,0xa3,0xf2,0x3f]
+# CHECK64: sltiu x7, x0, 1                 # encoding: [0x93,0x33,0x10,0x00]
+# CHECK64: sltiu x7, x0, 1                 # encoding: [0x93,0x33,0x10,0x00]
+# CHECK64: andi	x8, x7, 12                 # encoding: [0x13,0xf4,0xc3,0x00]
+# CHECK64: ori	x9, x8, -24                # encoding: [0x93,0x64,0x84,0xfe]
+# CHECK64: xori	x10, x9, 17                # encoding: [0x13,0xc5,0x14,0x01]
+# CHECK64: xori	x11, x10, -1               # encoding: [0x93,0x45,0xf5,0xff]
+# CHECK64: xori x11, x10, -1               # encoding: [0x93,0x45,0xf5,0xff]
+# CHECK64: slli	x12, x10, 5                # encoding: [0x13,0x16,0x55,0x00]
+# CHECK64: srli	x13, x12, 31               # encoding: [0x93,0x56,0xf6,0x01]
+# CHECK64: srai	x14, x13, 2                # encoding: [0x13,0xd7,0x26,0x40]
+# CHECK64: lui	x15, 1048575               # encoding: [0xb7,0xf7,0xff,0xff]
+# CHECK64: auipc   x16, 4                  # encoding: [0x17,0x48,0x00,0x00]
+
+# CHECK64: addiw	x5, x4, 1023       # encoding: [0x9b,0x02,0xf2,0x3f]
+# CHECK64: addiw	x6, x7, 0          # encoding: [0x1b,0x83,0x03,0x00]
+# CHECK64: addiw	x6, x7, 0          # encoding: [0x1b,0x83,0x03,0x00]
+# CHECK64: slliw	x12, x10, 2        # encoding: [0x1b,0x16,0x25,0x00]
+# CHECK64: srliw	x13, x12, 3        # encoding: [0x9b,0x56,0x36,0x00]
+# CHECK64: sraiw	x14, x13, 4        # encoding: [0x1b,0xd7,0x46,0x40]
+
+# CHECK64: add     x17, x16, x15           # encoding: [0xb3,0x08,0xf8,0x00]
+# CHECK64: sub     x18, x17, x16           # encoding: [0x33,0x89,0x08,0x41]
+# CHECK64: slt     x19, x18, x17           # encoding: [0xb3,0x29,0x19,0x01]
+# CHECK64: sltu    x20, x19, x18           # encoding: [0x33,0xba,0x29,0x01]
+# CHECK64: and     x21, x20, x19           # encoding: [0xb3,0x7a,0x3a,0x01]
+# CHECK64: or      x22, x21, x20           # encoding: [0x33,0xeb,0x4a,0x01]
+# CHECK64: xor     x23, x22, x21           # encoding: [0xb3,0x4b,0x5b,0x01]
+# CHECK64: sll     x24, x23, x22           # encoding: [0x33,0x9c,0x6b,0x01]
+# CHECK64: srl     x25, x24, x23           # encoding: [0xb3,0x5c,0x7c,0x01]
+# CHECK64: sra     x26, x25, x24           # encoding: [0x33,0xdd,0x8c,0x41]
+
+# CHECK64: addw	x10, x11, x12              # encoding: [0x3b,0x85,0xc5,0x00]
+# CHECK64: subw	x9, x8, x7                 # encoding: [0xbb,0x04,0x74,0x40]
+# CHECK64: sllw	x4, x5, x6                 # encoding: [0x3b,0x92,0x62,0x00]
+# CHECK64: srlw	x6, x7, x8                 # encoding: [0x3b,0xd3,0x83,0x00]
+# CHECK64: sraw	x9, x10, x11               # encoding: [0xbb,0x54,0xb5,0x40]
+
+# CHECK64: addi      x2, x0, 0             # encoding: [0x13,0x01,0x00,0x00]
+
+# CHECK64: jalr      x28, x27, 8           # encoding: [0x67,0x8e,0x8d,0x00]
+
+# CHECK64: fence
+# CHECK64: fence.i                         # encoding: [0x0f,0x10,0x00,0x00]
+
+# CHECK64: scall                           # encoding: [0x73,0x00,0x00,0x00]
+# CHECK64: sbreak                          # encoding: [0x73,0x00,0x10,0x00]
+# CHECK64: rdcycle   x5                    # encoding: [0xf3,0x22,0x00,0xc0]
+# CHECK64: rdtime    x7                    # encoding: [0xf3,0x23,0x10,0xc0]
+# CHECK64: rdinstret x9                    # encoding: [0xf3,0x24,0x20,0xc0]
+
+#-- register encodings: x0-x31
+#--                     x0 == $0
+	addi	x0, x0, 0
+	addi	x1, x0, 0
+	addi	x2, x1, 0
+	addi	x3, x2, 0
+	addi	x4, x3, 0
+	addi	x5, x4, 0
+	addi	x6, x5, 0
+	addi	x7, x6, 0
+	addi	x8, x7, 0
+	addi	x9, x8, 0
+	addi	x10, x9, 0
+	addi	x11, x10, 0
+	addi	x12, x11, 0
+	addi	x13, x12, 0
+	addi	x14, x13, 0
+	addi	x15, x14, 0
+	addi	x16, x15, 0
+	addi	x17, x16, 0
+	addi	x18, x17, 0
+	addi	x19, x18, 0
+	addi	x20, x19, 0
+	addi	x21, x20, 0
+	addi	x22, x21, 0
+	addi	x23, x22, 0
+	addi	x24, x23, 0
+	addi	x25, x24, 0
+	addi	x26, x25, 0
+	addi	x27, x26, 0
+	addi	x28, x27, 0
+	addi	x29, x28, 0
+	addi	x30, x29, 0
+	addi	x31, x30, 0
+
+#-- INTEGER COMPUTATIONAL INSTRUCTIONS
+#-- Integer Register-Immediate
+	nop				#-- pseudo-op for addi x0, x0, 0
+	addi	x3, x2, 1023
+	addi	x4, x3, -1023
+	addi	x31, x0, 0
+	mv	x31, x0			#-- pseudo-op for addi x31, x0, 0
+	slti	x5, x4, 0
+	slti	x6, x5, 1023
+	sltiu	x7, x0, 1
+	seqz	x7,x0			#-- pseudo-op for sltiu x7,x0,1
+	andi	x8, x7, 12
+	ori	x9, x8, -24
+	xori	x10, x9, 17
+	xori	x11, x10, -1
+	not	x11, x10		#-- pseudo-op for xori x11, x10, -1
+	slli	x12, x10, 5	
+	srli	x13, x12, 31
+	srai	x14, x13, 2
+	lui	x15, 1048575
+	auipc	x16, 4
+
+#-- RV64I only
+	addiw	x5, x4, 1023
+	addiw	x6, x7, 0
+	sext.w	x6, x7			#-- pseudo-op for addiw x6, x7, 0
+	slliw	x12, x10, 2
+	srliw	x13, x12, 3
+	sraiw	x14, x13, 4
+	
+
+#-- Integer Register-Register
+	add	x17, x16, x15
+	sub	x18, x17, x16
+	slt	x19, x18, x17
+	sltu	x20, x19, x18
+	and	x21, x20, x19
+	or	x22, x21, x20
+	xor	x23, x22, x21
+	sll	x24, x23, x22
+	srl	x25, x24, x23
+	sra	x26, x25, x24
+
+#-- RV64I only
+	addw	x10, x11, x12
+	subw	x9, x8, x7
+	sllw	x4, x5, x6
+	srlw	x6, x7, x8
+	sraw	x9, x10, x11
+	
+
+#-- CONTROL TRANSFER INSTRUCTIONS
+	mv 	x2, x0
+	jal	x27, 8
+	jal	x0, 8			#-- unconditional jump
+	jalr	x28, x27, 8	
+	beq	x28,x27, target_beq
+target_beq:
+	bne	x28,x27, target_bne
+target_bne:
+	blt	x29, x28, target_blt
+target_blt:
+	bltu	x30, x29, target_bltu	
+target_bltu:
+	bge	x31, x30, target_bge
+target_bge:
+	bgeu	x2, x31, target_bgeu
+target_bgeu:
+	nop
+
+#-- LOAD AND STORE INSTRUCTIONS	
+
+	sw x5, 0(x3)
+	sh x6, 0(x3)
+	sb x7, 0(x3)
+	sw x5, -4(x3)
+	sh x6, -4(x3)
+	sb x7, -4(x3)
+	sw x5, 4(x3)
+	sh x6, 4(x3)
+	sb x7, 4(x3)
+	lw x5, 0(x3)
+	lh x6, 0(x3)
+	lb x7, 0(x3)
+	lw x5, -4(x3)
+	lh x6, -4(x3)
+	lb x7, -4(x3)
+	lw x5, 4(x3)
+	lh x6, 4(x3)
+	lb x7, 4(x3)
+
+#-- RV64I only
+	ld x5, 0(x3)
+	ld x6, -4(x3)
+	ld x7, 4(x3)
+	lwu x5, 0(x3)
+	lhu x6, 0(x3)
+	lbu x7, 0(x3)
+	lwu x5, -4(x3)
+	lhu x6, -4(x3)
+	lbu x7, -4(x3)
+	lwu x5, 4(x3)
+	lhu x6, 4(x3)
+	lbu x7, 4(x3)
+
+
+#-- MEMORY MODEL
+	fence
+	fence.i
+
+#-- SYSTEM INSTRUCTIONS
+#-- *h encodings not supported in RV64I
+	scall
+	sbreak
+	rdcycle	x5
+	rdtime	x7
+	rdinstret x9 
+		
+
+#-- EOF
+
diff --git a/test/MC/RISCV/invalid-subtargets-xfail.s b/test/MC/RISCV/invalid-subtargets-xfail.s
new file mode 100644
index 0000000..df35460
--- /dev/null
+++ b/test/MC/RISCV/invalid-subtargets-xfail.s
@@ -0,0 +1,7 @@
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32IJ | FileCheck %s
+# XFAIL: *
+
+	addi	x0, x0, 0
+	
+#-- EOF
+
diff --git a/test/MC/RISCV/lit.local.cfg b/test/MC/RISCV/lit.local.cfg
new file mode 100644
index 0000000..d0b081e
--- /dev/null
+++ b/test/MC/RISCV/lit.local.cfg
@@ -0,0 +1,3 @@
+if not 'RISCV' in config.root.targets:
+    config.unsupported = True
+
diff --git a/test/MC/RISCV/valid-subtargets.s b/test/MC/RISCV/valid-subtargets.s
new file mode 100644
index 0000000..6f1cf3c
--- /dev/null
+++ b/test/MC/RISCV/valid-subtargets.s
@@ -0,0 +1,11 @@
+#-- Subtarget Tests
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32I | FileCheck %s
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV32IMAFD | FileCheck %s
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64I | FileCheck %s
+# RUN: llvm-mc %s -triple=riscv-unknown-linux -show-encoding -mcpu=RV64IAMFD | FileCheck %s
+
+# CHECK: addi    x0, x0, 0               # encoding: [0x13,0x00,0x00,0x00]
+	addi	x0, x0, 0
+	
+#-- EOF
+
diff --git a/test/MC/Sparc/sparc-directive-xword.s b/test/MC/Sparc/sparc-directive-xword.s
index 736f99f..0c9e249 100644
--- a/test/MC/Sparc/sparc-directive-xword.s
+++ b/test/MC/Sparc/sparc-directive-xword.s
@@ -1,6 +1,5 @@
 ! RUN: not llvm-mc %s -arch=sparc   -show-encoding 2>&1 | FileCheck %s --check-prefix=SPARC32
-! RUN: llvm-mc %s -triple sparc64 -show-encoding | FileCheck %s --check-prefix=SPARC64
-! RUN: llvm-mc %s -triple sparcv9 -show-encoding | FileCheck %s --check-prefix=SPARCV9
+! RUN: llvm-mc %s -arch=sparcv9 -show-encoding | FileCheck %s --check-prefix=SPARC64
 
         ! SPARC32:       error: unknown directive
         ! SPARC32-NEXT:  .xword 65536
@@ -9,5 +8,3 @@
         ! SPARC64:  .xword 65536
         .xword 65536
 
-        ! SPARCV9:  .xword 65536
-        .xword 65536
diff --git a/test/Transforms/InstCombine/vector-casts.ll b/test/Transforms/InstCombine/vector-casts.ll
index af18b4c..727eb4e 100644
--- a/test/Transforms/InstCombine/vector-casts.ll
+++ b/test/Transforms/InstCombine/vector-casts.ll
@@ -150,14 +150,3 @@ entry:
   ret <4 x float> undef
 }
 
-define <8 x i32> @pr24458(<8 x float> %n) {
-; CHECK-LABEL: @pr24458
-  %notequal_b_load_.i = fcmp une <8 x float> %n, zeroinitializer
-  %equal_a_load72_.i = fcmp ueq <8 x float> %n, zeroinitializer
-  %notequal_b_load__to_boolvec.i = sext <8 x i1> %notequal_b_load_.i to <8 x i32>
-  %equal_a_load72__to_boolvec.i = sext <8 x i1> %equal_a_load72_.i to <8 x i32>
-  %wrong = or <8 x i32> %notequal_b_load__to_boolvec.i, %equal_a_load72__to_boolvec.i
-  ret <8 x i32> %wrong
-; CHECK-NEXT: ret <8 x i32> <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
-}
-
diff --git a/test/Transforms/InstSimplify/2011-09-05-InsertExtractValue.ll b/test/Transforms/InstSimplify/2011-09-05-InsertExtractValue.ll
index 441bc1a..7e391ab 100644
--- a/test/Transforms/InstSimplify/2011-09-05-InsertExtractValue.ll
+++ b/test/Transforms/InstSimplify/2011-09-05-InsertExtractValue.ll
@@ -36,13 +36,3 @@ define i32 @test3(i32 %a, float %b) {
 ; CHECK-LABEL: @test3(
 ; CHECK: ret i32 %a
 }
-
-define i8 @test4(<8 x i8> %V) {
-  %add     = add <8 x i8> %V, bitcast (double 0x319BEB8FD172E36 to <8 x i8>)
-  %extract = extractelement <8 x i8> %add, i32 6
-  ret i8 %extract
-; CHECK-LABEL: @test4(
-; CHECK: %[[add:.*]] = add <8 x i8> %V, bitcast (<1 x double> <double 0x319BEB8FD172E36> to <8 x i8>)
-; CHECK-NEXT: %[[extract:.*]] = extractelement <8 x i8> %[[add]], i32 6
-; CHECK-NEXT: ret i8 %[[extract]]
-}
diff --git a/test/Transforms/Scalarizer/cache-bug.ll b/test/Transforms/Scalarizer/cache-bug.ll
deleted file mode 100644
index f8c2d10..0000000
--- a/test/Transforms/Scalarizer/cache-bug.ll
+++ /dev/null
@@ -1,30 +0,0 @@
-; RUN: opt -scalarizer -S < %s | FileCheck %s
-target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
-
-
-; Check that vector element 1 is scalarized correctly from a chain of
-; insertelement instructions
-define void @func(i32 %x) {
-; CHECK-LABEL: @func(
-; CHECK-NOT: phi i32 [ %x, %entry ], [ %inc.pos.y, %loop ]
-; CHECK:     phi i32 [ %inc, %entry ], [ %inc.pos.y, %loop ]
-; CHECK:   ret void
-entry:
-  %vecinit = insertelement <2 x i32> <i32 0, i32 0>, i32 %x, i32 1
-  %inc = add i32 %x, 1
-  %0 = insertelement <2 x i32> %vecinit, i32 %inc, i32 1
-  br label %loop
-
-loop:
-  %pos = phi <2 x i32> [ %0, %entry ], [ %new.pos.y, %loop ]
-  %i = phi i32 [ 0, %entry ], [ %new.i, %loop ]
-  %pos.y = extractelement <2 x i32> %pos, i32 1
-  %inc.pos.y = add i32 %pos.y, 1
-  %new.pos.y = insertelement <2 x i32> %pos, i32 %inc.pos.y, i32 1
-  %new.i = add i32 %i, 1
-  %cmp2 = icmp slt i32 %new.i, 1
-  br i1 %cmp2, label %loop, label %exit
-
-exit:
-  ret void
-}
diff --git a/tools/llvm-lto/llvm-lto.cpp b/tools/llvm-lto/llvm-lto.cpp
index 0821898..9678c83 100644
--- a/tools/llvm-lto/llvm-lto.cpp
+++ b/tools/llvm-lto/llvm-lto.cpp
@@ -214,11 +214,8 @@ int main(int argc, char **argv) {
     if (SetMergedModule && i == BaseArg) {
       // Transfer ownership to the code generator.
       CodeGen.setModule(Module.release());
-    } else if (!CodeGen.addModule(Module.get())) {
-      // Print a message here so that we know addModule() did not abort.
-      errs() << argv[0] << ": error adding file '" << InputFilenames[i] << "'\n";
+    } else if (!CodeGen.addModule(Module.get()))
       return 1;
-    }
 
     unsigned NumSyms = LTOMod->getSymbolCount();
     for (unsigned I = 0; I < NumSyms; ++I) {
diff --git a/tools/llvm-shlib/Makefile b/tools/llvm-shlib/Makefile
index 2bc81da..19077a3 100644
--- a/tools/llvm-shlib/Makefile
+++ b/tools/llvm-shlib/Makefile
@@ -61,7 +61,7 @@ endif
 
 ifeq ($(HOST_OS), $(filter $(HOST_OS), DragonFly Linux FreeBSD GNU/kFreeBSD GNU))
     # Add soname to the library.
-    LLVMLibsOptions += -Wl,--soname,lib$(LIBRARYNAME).1$(SHLIBEXT)
+    LLVMLibsOptions += -Wl,--soname,lib$(LIBRARYNAME)$(SHLIBEXT)
 endif
 
 ifeq ($(HOST_OS), $(filter $(HOST_OS), Linux GNU GNU/kFreeBSD))
diff --git a/unittests/Transforms/Utils/Local.cpp b/unittests/Transforms/Utils/Local.cpp
index 2ff5604..f0c3ecf 100644
--- a/unittests/Transforms/Utils/Local.cpp
+++ b/unittests/Transforms/Utils/Local.cpp
@@ -58,40 +58,3 @@ TEST(Local, RecursivelyDeleteDeadPHINodes) {
   delete bb0;
   delete bb1;
 }
-
-TEST(Local, RemoveDuplicatePHINodes) {
-  LLVMContext &C(getGlobalContext());
-  IRBuilder<> B(C);
-
-  std::unique_ptr<Function> F(
-      Function::Create(FunctionType::get(B.getVoidTy(), false),
-                       GlobalValue::ExternalLinkage, "F"));
-  BasicBlock *Entry(BasicBlock::Create(C, "", F.get()));
-  BasicBlock *BB(BasicBlock::Create(C, "", F.get()));
-  BranchInst::Create(BB, Entry);
-
-  B.SetInsertPoint(BB);
-
-  AssertingVH<PHINode> P1 = B.CreatePHI(Type::getInt32Ty(C), 2);
-  P1->addIncoming(B.getInt32(42), Entry);
-
-  PHINode *P2 = B.CreatePHI(Type::getInt32Ty(C), 2);
-  P2->addIncoming(B.getInt32(42), Entry);
-
-  AssertingVH<PHINode> P3 = B.CreatePHI(Type::getInt32Ty(C), 2);
-  P3->addIncoming(B.getInt32(42), Entry);
-  P3->addIncoming(B.getInt32(23), BB);
-
-  PHINode *P4 = B.CreatePHI(Type::getInt32Ty(C), 2);
-  P4->addIncoming(B.getInt32(42), Entry);
-  P4->addIncoming(B.getInt32(23), BB);
-
-  P1->addIncoming(P3, BB);
-  P2->addIncoming(P4, BB);
-  BranchInst::Create(BB, BB);
-
-  // Verify that we can eliminate PHIs that become duplicates after chaning PHIs
-  // downstream.
-  EXPECT_TRUE(EliminateDuplicatePHINodes(BB));
-  EXPECT_EQ(3U, BB->size());
-}
